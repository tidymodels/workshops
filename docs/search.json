[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, we‚Äôll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "index.html#is-this-workshop-for-me",
    "href": "index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %&gt;% and/or native pipe |&gt;\nYou are familiar with functions from dplyr, tidyr, and ggplot2\nYou can read data into R, transform and reshape data, and make a wide variety of graphs\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning."
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download\nThe following R packages, which you can install from the R console:\n\n\ninstall.packages(c(\"Cubist\", \"DALEXtra\", \"doParallel\", \"earth\", \"embed\", \n                   \"forcats\", \"lme4\", \"parallelly\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"rules\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@hockeyR\")"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participants‚Äô convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nIntroduction to tidymodels\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n\n\n\nAdvanced tidymodels\n\n01: Tuning hyperparameters\n\nThere‚Äôs also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files (version 1.3.433) for working along are available on GitHub. (Don‚Äôt worry if you haven‚Äôt used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "index.html#past-workshops",
    "href": "index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\n25-26 July 2022 at rstudio::conf()"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "index.html#reuse-and-licensing",
    "href": "index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.¬†not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "slides/01-introduction.html#who-are-you",
    "href": "slides/01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %>% or base R |> pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "slides/01-introduction.html#who-are-tidymodels",
    "href": "slides/01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and Desir√©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "slides/01-introduction.html#asking-for-help",
    "href": "slides/01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nüü™ ‚ÄúI‚Äôm stuck and need help!‚Äù\n\n\nüü© ‚ÄúI finished the exercise‚Äù"
  },
  {
    "objectID": "slides/01-introduction.html#section",
    "href": "slides/01-introduction.html#section",
    "title": "1 - Introduction",
    "section": "üëÄ",
    "text": "üëÄ"
  },
  {
    "objectID": "slides/01-introduction.html#tentative-plan-for-this-workshop",
    "href": "slides/01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\nToday:\n\nYour data budget\nWhat makes a model\nEvaluating models\n\nTomorrow:\n\nFeature engineering\nTuning hyperparameters\nTransportation case study\nWrapping up!"
  },
  {
    "objectID": "slides/01-introduction.html#section-1",
    "href": "slides/01-introduction.html#section-1",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors üëã\n\n Log in to RStudio Cloud here (free):\nbit.ly/tidymodels-iceland-2022"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-machine-learning",
    "href": "slides/01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-machine-learning-1",
    "href": "slides/01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-machine-learning-2",
    "href": "slides/01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn",
    "href": "slides/01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\n03:00\n\n\n\n\nthe ‚Äútwo cultures‚Äù\nmodel first vs.¬†data first\ninference vs.¬†prediction"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-tidymodels",
    "href": "slides/01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#> ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.0.0 ‚îÄ‚îÄ\n#> ‚úî broom        1.0.0     ‚úî rsample      1.1.0\n#> ‚úî dials        1.0.0     ‚úî tibble       3.1.8\n#> ‚úî dplyr        1.0.9     ‚úî tidyr        1.2.0\n#> ‚úî infer        1.0.2     ‚úî tune         1.0.0\n#> ‚úî modeldata    1.0.0     ‚úî workflows    1.0.0\n#> ‚úî parsnip      1.0.1     ‚úî workflowsets 1.0.0\n#> ‚úî purrr        0.3.4     ‚úî yardstick    1.0.0\n#> ‚úî recipes      1.0.1\n#> ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n#> ‚úñ purrr::discard() masks scales::discard()\n#> ‚úñ dplyr::filter()  masks stats::filter()\n#> ‚úñ dplyr::lag()     masks stats::lag()\n#> ‚úñ recipes::step()  masks stats::step()\n#> ‚Ä¢ Use tidymodels_prefer() to resolve common conflicts."
  },
  {
    "objectID": "slides/01-introduction.html#the-whole-game",
    "href": "slides/01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nTomorrow we will walk through a case study in detail to illustrate feature engineering and model tuning.\nToday we will walk through the analysis at a higher level to show the model development process as a whole and give you an introduction to the data set.\nThe data are from the NHL where we want to predict whether a shot was on-goal or not! üèí\nIt‚Äôs a good example to show how model development works."
  },
  {
    "objectID": "slides/01-introduction.html#shots-on-goal",
    "href": "slides/01-introduction.html#shots-on-goal",
    "title": "1 - Introduction",
    "section": "Shots on goal",
    "text": "Shots on goal"
  },
  {
    "objectID": "slides/01-introduction.html#data-spending",
    "href": "slides/01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data spending",
    "text": "Data spending"
  },
  {
    "objectID": "slides/01-introduction.html#a-first-model",
    "href": "slides/01-introduction.html#a-first-model",
    "title": "1 - Introduction",
    "section": "A first model",
    "text": "A first model"
  },
  {
    "objectID": "slides/01-introduction.html#starting-point-logistic-regression",
    "href": "slides/01-introduction.html#starting-point-logistic-regression",
    "title": "1 - Introduction",
    "section": "Starting point: logistic regression",
    "text": "Starting point: logistic regression\n\nWe‚Äôll start by using basic logistic regression to predict our binary outcome.\nOur first model will have 12 simple predictor columns.\nOne initial question: there are 622 players taking shots.\nFor logistic regression, do we convert these to binary indicators (a.k.a. ‚Äúdummies‚Äù)?"
  },
  {
    "objectID": "slides/01-introduction.html#basic-features-inc-dummy-variables",
    "href": "slides/01-introduction.html#basic-features-inc-dummy-variables",
    "title": "1 - Introduction",
    "section": "Basic features (inc dummy variables)",
    "text": "Basic features (inc dummy variables)"
  },
  {
    "objectID": "slides/01-introduction.html#different-player-encoding",
    "href": "slides/01-introduction.html#different-player-encoding",
    "title": "1 - Introduction",
    "section": "Different player encoding",
    "text": "Different player encoding"
  },
  {
    "objectID": "slides/01-introduction.html#what-about-location",
    "href": "slides/01-introduction.html#what-about-location",
    "title": "1 - Introduction",
    "section": "What about location",
    "text": "What about location\nThe previous models used the x/y coordinates.\nAre there better ways to represent shot location?\nHow can we make location more usable for the model?"
  },
  {
    "objectID": "slides/01-introduction.html#add-shot-angle",
    "href": "slides/01-introduction.html#add-shot-angle",
    "title": "1 - Introduction",
    "section": "Add shot angle?",
    "text": "Add shot angle?"
  },
  {
    "objectID": "slides/01-introduction.html#add-shot-from-defensive-zone",
    "href": "slides/01-introduction.html#add-shot-from-defensive-zone",
    "title": "1 - Introduction",
    "section": "Add shot from defensive zone?",
    "text": "Add shot from defensive zone?"
  },
  {
    "objectID": "slides/01-introduction.html#add-shot-behind-goal-line",
    "href": "slides/01-introduction.html#add-shot-behind-goal-line",
    "title": "1 - Introduction",
    "section": "Add shot behind goal line?",
    "text": "Add shot behind goal line?"
  },
  {
    "objectID": "slides/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "href": "slides/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "title": "1 - Introduction",
    "section": "Nonlinear terms for angle and distance",
    "text": "Nonlinear terms for angle and distance"
  },
  {
    "objectID": "slides/01-introduction.html#try-another-model",
    "href": "slides/01-introduction.html#try-another-model",
    "title": "1 - Introduction",
    "section": "Try another model",
    "text": "Try another model"
  },
  {
    "objectID": "slides/01-introduction.html#switch-to-boosting-and-basic-features",
    "href": "slides/01-introduction.html#switch-to-boosting-and-basic-features",
    "title": "1 - Introduction",
    "section": "Switch to boosting and basic features",
    "text": "Switch to boosting and basic features"
  },
  {
    "objectID": "slides/01-introduction.html#boosting-with-location-features",
    "href": "slides/01-introduction.html#boosting-with-location-features",
    "title": "1 - Introduction",
    "section": "Boosting with location features",
    "text": "Boosting with location features"
  },
  {
    "objectID": "slides/01-introduction.html#choose-wisely",
    "href": "slides/01-introduction.html#choose-wisely",
    "title": "1 - Introduction",
    "section": "Choose wisely‚Ä¶",
    "text": "Choose wisely‚Ä¶"
  },
  {
    "objectID": "slides/01-introduction.html#finalize-and-verify",
    "href": "slides/01-introduction.html#finalize-and-verify",
    "title": "1 - Introduction",
    "section": "Finalize and verify",
    "text": "Finalize and verify"
  },
  {
    "objectID": "slides/01-introduction.html#and-so-on",
    "href": "slides/01-introduction.html#and-so-on",
    "title": "1 - Introduction",
    "section": "‚Ä¶ and so on",
    "text": "‚Ä¶ and so on\nOnce we find an acceptable model and feature set, the process is to\n\nConfirm our results on the test set.\nDocument the data and model development process.\nDeploy, monitor, etc."
  },
  {
    "objectID": "slides/01-introduction.html#lets-install-some-packages",
    "href": "slides/01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Let‚Äôs install some packages",
    "text": "Let‚Äôs install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\ninstall.packages(c(\"Cubist\", \"DALEXtra\", \"doParallel\", \"earth\", \"embed\", \n                   \"forcats\", \"lme4\", \"parallelly\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"rules\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@hockeyR\")\n\n\n\n Or log in to RStudio Cloud:\nbit.ly/tidymodels-iceland-2022"
  },
  {
    "objectID": "slides/01-introduction.html#our-versions",
    "href": "slides/01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\n\n\n\nbroom (1.0.0, CRAN), DALEX (2.4.2, local), DALEXtra (2.2.1, CRAN), dials (1.0.0, CRAN), doParallel (1.0.17, CRAN), dplyr (1.0.9, CRAN), embed (1.0.0, CRAN), ggplot2 (3.3.6, CRAN), modeldata (1.0.0, CRAN), ongoal (0.0.3, Github (topepo/ongoal@68e6466bb), parsnip (1.0.1, CRAN), purrr (0.3.4, CRAN), ranger (0.14.1, CRAN), recipes (1.0.1, CRAN), rpart (4.1.16, CRAN), rpart.plot (3.1.1, CRAN), rsample (1.1.0, CRAN), scales (1.2.0, CRAN), stacks (1.0.0, CRAN), tibble (3.1.8, CRAN), tidymodels (1.0.0, CRAN), tidyr (1.2.0, CRAN), tune (1.0.0, CRAN), vetiver (0.1.7, CRAN), workflows (1.0.0, CRAN), workflowsets (1.0.0, CRAN), xgboost (1.6.0.1, CRAN), and yardstick (1.0.0, CRAN)\nQuarto: 1.0.38\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching-1",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching-1",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\nRed-eyed tree frog embryos can hatch earlier than their normal ~7 days if they detect potential predator threat!\nType ?stacks::tree_frogs to learn more about this dataset, including references.\nWe are using a slightly modified version from stacks.\n\n\nlibrary(tidymodels)\n\ndata(\"tree_frogs\", package = \"stacks\")\ntree_frogs <- tree_frogs %>%\n  mutate(t_o_d = factor(t_o_d),\n         age = age / 86400) %>%\n  filter(!is.na(latency)) %>%\n  select(-c(clutch, hatched))"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching-2",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching-2",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\n\n\nN = 572\nA numeric outcome, latency\n4 other variables\n\ntreatment, reflex, and t_o_d are nominal predictors\nage is a numeric predictor\n\n\n\n\n\n\n\n\nlatency: How long it took the frog to hatch after being stimulated - i.e.¬†after being poked by a blunt probe (in seconds).\ntreatment: Whether or not they got gentamicin, a compound that knocks out the embryo‚Äôs lateral line (a sensory organ).\nreflex: A measure of ear function (low, mid, full)\nt_o_d: Time that the stimulus was applied (morning, afternoon, night)\nage: Age at the time it was stimulated (in days)"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching-3",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching-3",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\ntree_frogs\n#> # A tibble: 572 √ó 5\n#>    treatment  reflex   age t_o_d     latency\n#>    <chr>      <fct>  <dbl> <fct>       <dbl>\n#>  1 control    full    5.40 morning        22\n#>  2 control    low     4.18 night         360\n#>  3 control    full    4.65 afternoon     106\n#>  4 control    mid     4.14 night         180\n#>  5 control    full    4.6  afternoon      60\n#>  6 gentamicin full    5.36 morning        39\n#>  7 control    full    4.56 afternoon     214\n#>  8 control    full    5.43 morning        50\n#>  9 control    full    4.63 afternoon     224\n#> 10 control    full    5.40 morning        63\n#> # ‚Ä¶ with 562 more rows"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending",
    "href": "slides/02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not üö´ use the test set during training."
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-1",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-2",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "slides/02-data-budget.html#your-turn",
    "href": "slides/02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-3",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs)\nfrog_split\n#> <Training/Testing/Total>\n#> <429/143/572>\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "slides/02-data-budget.html#accessing-the-data",
    "href": "slides/02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nfrog_train <- training(frog_split)\nfrog_test <- testing(frog_split)"
  },
  {
    "objectID": "slides/02-data-budget.html#the-training-set",
    "href": "slides/02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nfrog_train\n#> # A tibble: 429 √ó 5\n#>    treatment  reflex   age t_o_d     latency\n#>    <chr>      <fct>  <dbl> <fct>       <dbl>\n#>  1 control    full    5.36 morning        36\n#>  2 gentamicin full    5.37 morning        72\n#>  3 gentamicin full    4.65 afternoon     141\n#>  4 control    full    5.42 morning        27\n#>  5 control    full    5.43 morning        27\n#>  6 gentamicin full    5.38 morning        73\n#>  7 gentamicin full    5.42 morning        68\n#>  8 gentamicin full    4.75 afternoon     124\n#>  9 control    full    5.00 night          62\n#> 10 control    full    5.39 morning        25\n#> # ‚Ä¶ with 419 more rows"
  },
  {
    "objectID": "slides/02-data-budget.html#the-test-set",
    "href": "slides/02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \n\nfrog_test\n#> # A tibble: 143 √ó 5\n#>    treatment  reflex   age t_o_d     latency\n#>    <chr>      <fct>  <dbl> <fct>       <dbl>\n#>  1 control    full    5.40 morning        22\n#>  2 control    low     4.18 night         360\n#>  3 control    full    4.63 afternoon     224\n#>  4 gentamicin full    4.75 afternoon     158\n#>  5 control    mid     4.22 night          91\n#>  6 gentamicin full    4.89 night         301\n#>  7 control    full    5.38 morning         2\n#>  8 control    full    4.80 afternoon      56\n#>  9 control    full    5.36 morning        11\n#> 10 control    full    5.40 morning        64\n#> # ‚Ä¶ with 133 more rows"
  },
  {
    "objectID": "slides/02-data-budget.html#your-turn-1",
    "href": "slides/02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\n05:00"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-4",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-4",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs, prop = 0.8)\nfrog_train <- training(frog_split)\nfrog_test <- testing(frog_split)\n\nnrow(frog_train)\n#> [1] 457\nnrow(frog_test)\n#> [1] 115"
  },
  {
    "objectID": "slides/02-data-budget.html#section-1",
    "href": "slides/02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "We will use this tomorrow"
  },
  {
    "objectID": "slides/02-data-budget.html#your-turn-2",
    "href": "slides/02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the frog_train data on your own!\n\nWhat‚Äôs the distribution of the outcome, latency?\nWhat‚Äôs the distribution of numeric variables like age?\nHow does latency differ across the categorical variables?\n\n\n\n\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "slides/02-data-budget.html#section-3",
    "href": "slides/02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\n\nThis histogram brings up a concern. What if in our training set we get unlucky and sample few or none of these large values? That could mean that our model wouldn‚Äôt be able to predict such values. Let‚Äôs come back to that!"
  },
  {
    "objectID": "slides/02-data-budget.html#section-4",
    "href": "slides/02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency, treatment, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE)"
  },
  {
    "objectID": "slides/02-data-budget.html#section-5",
    "href": "slides/02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "frog_train %>%\n  ggplot(aes(latency, reflex, fill = reflex)) +\n  geom_boxplot(alpha = 0.3, show.legend = FALSE)"
  },
  {
    "objectID": "slides/02-data-budget.html#section-6",
    "href": "slides/02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(age, latency, color = reflex)) +\n  geom_point(alpha = .8, size = 2)"
  },
  {
    "objectID": "slides/02-data-budget.html#section-7",
    "href": "slides/02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "Stratified sampling would split within each quartile\n\nBased on our exploration, we realized that stratifying by latency might help get a consistent distribution. For instance, we‚Äôd include high and low latency in both the test and training"
  },
  {
    "objectID": "slides/02-data-budget.html#stratification",
    "href": "slides/02-data-budget.html#stratification",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nUse strata = latency\n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)\nfrog_split\n#> <Training/Testing/Total>\n#> <456/116/572>\n\n\nStratification often helps, with very little downside\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn",
    "href": "slides/03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\n03:00\n\n\n\n\n\nlm for linear model\nglm for generalized linear model (e.g.¬†logistic regression)\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nlinear_reg()\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm\n\n\nModels have default engines"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %>%\n  set_engine(\"glmnet\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: glmnet"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %>%\n  set_engine(\"stan\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: stan"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#> Decision Tree Model Specification (unknown)\n#> \n#> Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %>% \n  set_mode(\"regression\")\n#> Decision Tree Model Specification (regression)\n#> \n#> Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-1",
    "href": "slides/03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code so it creates a different model.\n\n\n\n05:00\n\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#models-well-be-using-today",
    "href": "slides/03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models we‚Äôll be using today",
    "text": "Models we‚Äôll be using today\n\nLinear regression\nDecision trees"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#linear-regression",
    "href": "slides/03-what-makes-a-model.html#linear-regression",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#linear-regression-1",
    "href": "slides/03-what-makes-a-model.html#linear-regression-1",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#linear-regression-2",
    "href": "slides/03-what-makes-a-model.html#linear-regression-2",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\n\n\n\n\n\n\nOutcome modeled as linear combination of predictors:\n\n\\(\\mbox{latency} = \\beta_0 + \\beta_1\\cdot\\mbox{age} + \\epsilon\\)\n\nFind a line that minimizes the mean squared error (MSE)"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#decision-trees",
    "href": "slides/03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#decision-trees-1",
    "href": "slides/03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#decision-trees-2",
    "href": "slides/03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLinear regression\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "slides/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\n\n\n\n\n\n\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "slides/03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#why-a-workflow",
    "href": "slides/03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering tomorrow!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your ‚Äúnew‚Äù data just doesn‚Äôt have an instance of that level)"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-1",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\ntree_spec %>% \n  fit(latency ~ ., data = frog_train) \n#> parsnip model object\n#> \n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=4.947975 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 4.947975 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 4.664439 30  102190.20 147.83330  \n#>         28) age>=4.566638 22   53953.86 129.77270 *\n#>         29) age< 4.566638 8   21326.00 197.50000 *\n#>       15) age>=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-2",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow() %>%\n  add_formula(latency ~ .) %>%\n  add_model(tree_spec) %>%\n  fit(data = frog_train) \n#> ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=4.947975 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 4.947975 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 4.664439 30  102190.20 147.83330  \n#>         28) age>=4.566638 22   53953.86 129.77270 *\n#>         29) age< 4.566638 8   21326.00 197.50000 *\n#>       15) age>=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-3",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow(latency ~ ., tree_spec) %>% \n  fit(data = frog_train) \n#> ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=4.947975 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 4.947975 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 4.664439 30  102190.20 147.83330  \n#>         28) age>=4.566638 22   53953.86 129.77270 *\n#>         29) age< 4.566638 8   21326.00 197.50000 *\n#>       15) age>=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-2",
    "href": "slides/03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code so it uses a linear model.\n\n\n\n05:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#predict-with-your-model",
    "href": "slides/03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\ntree_fit <-\n  workflow(latency ~ ., tree_spec) %>% \n  fit(data = frog_train)"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-3",
    "href": "slides/03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-4",
    "href": "slides/03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#understand-your-model",
    "href": "slides/03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#understand-your-model-1",
    "href": "slides/03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#understand-your-model-2",
    "href": "slides/03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-5",
    "href": "slides/03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nExtract the model engine object from your fitted linear workflow.\n‚ö†Ô∏è Never predict() with any extracted components!\n\n\n\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.¬†give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#deploying-a-model",
    "href": "slides/03-what-makes-a-model.html#deploying-a-model",
    "title": "3 - What makes a model?",
    "section": "Deploying a model ",
    "text": "Deploying a model \nHow do you use your new tree_fit model in production?\n\nlibrary(vetiver)\nv <- vetiver_model(tree_fit, \"frog_hatching\")\nv\n#> \n#> ‚îÄ‚îÄ frog_hatching ‚îÄ <butchered_workflow> model for deployment \n#> A rpart regression modeling workflow using 4 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#deploy-your-model-1",
    "href": "slides/03-what-makes-a-model.html#deploy-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new model tree_fit in production?\n\nlibrary(plumber)\npr() %>%\n  vetiver_api(v)\n#> # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#> # Use `pr_run()` on this object to start the API.\n#> ‚îú‚îÄ‚îÄ[queryString]\n#> ‚îú‚îÄ‚îÄ[body]\n#> ‚îú‚îÄ‚îÄ[cookieParser]\n#> ‚îú‚îÄ‚îÄ[sharedSecret]\n#> ‚îú‚îÄ‚îÄ/logo\n#> ‚îÇ  ‚îÇ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2/Resources/library/vetiver\n#> ‚îú‚îÄ‚îÄ/ping (GET)\n#> ‚îî‚îÄ‚îÄ/predict (POST)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-6",
    "href": "slides/03-what-makes-a-model.html#your-turn-6",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\n05:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\n\naugment(tree_fit, new_data = frog_test) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 rsq     standard       0.380\n#> 3 mae     standard      40.2\n\n\n\nRMSE: difference between the predicted and observed values ‚¨áÔ∏è\n\\(R^2\\): squared correlation between the predicted and observed values ‚¨ÜÔ∏è\nMAE: similar to RMSE, but mean absolute error ‚¨áÔ∏è"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        59.2"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %>%\n  group_by(reflex) %>%\n  rmse(latency, .pred)\n#> # A tibble: 3 √ó 4\n#>   reflex .metric .estimator .estimate\n#>   <fct>  <chr>   <chr>          <dbl>\n#> 1 low    rmse    standard        94.3\n#> 2 mid    rmse    standard       101. \n#> 3 full   rmse    standard        51.2"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\nfrog_metrics <- metric_set(rmse, msd)\naugment(tree_fit, new_data = frog_test) %>%\n  frog_metrics(latency, .pred)\n#> # A tibble: 2 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 msd     standard      -0.908"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è",
    "text": "Dangers of overfitting ‚ö†Ô∏è"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è",
    "text": "Dangers of overfitting ‚ö†Ô∏è"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\ntree_fit %>%\n  augment(frog_train)\n#> # A tibble: 456 √ó 6\n#>    treatment  reflex   age t_o_d     latency .pred\n#>    <chr>      <fct>  <dbl> <fct>       <dbl> <dbl>\n#>  1 control    full    5.42 morning        33  39.8\n#>  2 control    full    5.38 morning        19  66.7\n#>  3 control    full    5.38 morning         2  66.7\n#>  4 control    full    5.44 morning        39  39.8\n#>  5 control    full    5.41 morning        42  39.8\n#>  6 control    full    4.75 afternoon      20  59.8\n#>  7 control    full    4.95 night          31  83.1\n#>  8 control    full    5.42 morning        21  39.8\n#>  9 gentamicin full    5.39 morning        30  64.6\n#> 10 control    full    4.55 afternoon      43 174. \n#> # ‚Ä¶ with 446 more rows\n\nWe call this ‚Äúresubstitution‚Äù or ‚Äúrepredicting the training set‚Äù"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4\n\nWe call this a ‚Äúresubstitution estimate‚Äù"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4\n\n\n\ntree_fit %>%\n  augment(frog_test) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        59.2\n\n\n\n\n‚ö†Ô∏è Remember that we‚Äôre demonstrating overfitting\n\n\n‚ö†Ô∏è Don‚Äôt use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn",
    "href": "slides/04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and metrics() to compute a regression metric like mae().\nCompute the metrics for both training and testing data.\nNotice the evidence of overfitting! ‚ö†Ô∏è\n\n\n\n05:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      49.4  \n#> 2 rsq     standard       0.494\n#> 3 mae     standard      33.4\n\n\n\ntree_fit %>%\n  augment(frog_test) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 rsq     standard       0.380\n#> 3 mae     standard      40.2\n\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation",
    "href": "slides/04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-1",
    "href": "slides/04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-1",
    "href": "slides/04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-2",
    "href": "slides/04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train) # v = 10 is default\n#> #  10-fold cross-validation \n#> # A tibble: 10 √ó 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [410/46]> Fold01\n#>  2 <split [410/46]> Fold02\n#>  3 <split [410/46]> Fold03\n#>  4 <split [410/46]> Fold04\n#>  5 <split [410/46]> Fold05\n#>  6 <split [410/46]> Fold06\n#>  7 <split [411/45]> Fold07\n#>  8 <split [411/45]> Fold08\n#>  9 <split [411/45]> Fold09\n#> 10 <split [411/45]> Fold10"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-3",
    "href": "slides/04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nfrog_folds <- vfold_cv(frog_train)\nfrog_folds$splits[1:3]\n#> [[1]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n#> \n#> [[2]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n#> \n#> [[3]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-4",
    "href": "slides/04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, v = 5)\n#> #  5-fold cross-validation \n#> # A tibble: 5 √ó 2\n#>   splits           id   \n#>   <list>           <chr>\n#> 1 <split [364/92]> Fold1\n#> 2 <split [365/91]> Fold2\n#> 3 <split [365/91]> Fold3\n#> 4 <split [365/91]> Fold4\n#> 5 <split [365/91]> Fold5"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-5",
    "href": "slides/04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, strata = latency)\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [408/48]> Fold01\n#>  2 <split [408/48]> Fold02\n#>  3 <split [408/48]> Fold03\n#>  4 <split [409/47]> Fold04\n#>  5 <split [411/45]> Fold05\n#>  6 <split [412/44]> Fold06\n#>  7 <split [412/44]> Fold07\n#>  8 <split [412/44]> Fold08\n#>  9 <split [412/44]> Fold09\n#> 10 <split [412/44]> Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-6",
    "href": "slides/04-evaluating-models.html#cross-validation-6",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWe‚Äôll use this setup:\n\nset.seed(123)\nfrog_folds <- vfold_cv(frog_train, v = 10, strata = latency)\nfrog_folds\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [408/48]> Fold01\n#>  2 <split [408/48]> Fold02\n#>  3 <split [408/48]> Fold03\n#>  4 <split [409/47]> Fold04\n#>  5 <split [411/45]> Fold05\n#>  6 <split [412/44]> Fold06\n#>  7 <split [412/44]> Fold07\n#>  8 <split [412/44]> Fold08\n#>  9 <split [412/44]> Fold09\n#> 10 <split [412/44]> Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "slides/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "slides/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\ntree_res <- fit_resamples(tree_wflow, frog_folds)\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 4\n#>    splits           id     .metrics         .notes          \n#>    <list>           <chr>  <list>           <list>          \n#>  1 <split [408/48]> Fold01 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  2 <split [408/48]> Fold02 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  3 <split [408/48]> Fold03 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  4 <split [409/47]> Fold04 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  5 <split [411/45]> Fold05 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  6 <split [412/44]> Fold06 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  7 <split [412/44]> Fold07 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  8 <split [412/44]> Fold08 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  9 <split [412/44]> Fold09 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#> 10 <split [412/44]> Fold10 <tibble [2 √ó 4]> <tibble [0 √ó 3]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res %>%\n  collect_metrics()\n#> # A tibble: 2 √ó 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   59.6      10  2.31   Preprocessor1_Model1\n#> 2 rsq     standard    0.305    10  0.0342 Preprocessor1_Model1\n\n\nWe can reliably measure performance using only the training data üéâ"
  },
  {
    "objectID": "slides/04-evaluating-models.html#comparing-metrics",
    "href": "slides/04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\n\n\n\ntree_res %>%\n  collect_metrics() %>% \n  select(.metric, mean, n)\n#> # A tibble: 2 √ó 3\n#>   .metric   mean     n\n#>   <chr>    <dbl> <int>\n#> 1 rmse    59.6      10\n#> 2 rsq      0.305    10\n\n\nThe RMSE previously was\n\n49.36 for the training set\n59.16 for test set\n\n\n\n\nRemember that:\n‚ö†Ô∏è the training set gives you overly optimistic metrics\n‚ö†Ô∏è the test set is precious"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-1",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_frog <- control_resamples(save_pred = TRUE)\ntree_res <- fit_resamples(tree_wflow, frog_folds, control = ctrl_frog)\n\ntree_preds <- collect_predictions(tree_res)\ntree_preds\n#> # A tibble: 456 √ó 5\n#>    id     .pred  .row latency .config             \n#>    <chr>  <dbl> <int>   <dbl> <chr>               \n#>  1 Fold01  39.6     1      33 Preprocessor1_Model1\n#>  2 Fold01  72.1     3       2 Preprocessor1_Model1\n#>  3 Fold01  63.8     9      30 Preprocessor1_Model1\n#>  4 Fold01  72.1    13      46 Preprocessor1_Model1\n#>  5 Fold01  43.3    28      11 Preprocessor1_Model1\n#>  6 Fold01  61.7    35      41 Preprocessor1_Model1\n#>  7 Fold01  39.6    51      43 Preprocessor1_Model1\n#>  8 Fold01 134.     70      20 Preprocessor1_Model1\n#>  9 Fold01  70.6    74      21 Preprocessor1_Model1\n#> 10 Fold01  39.6   106      14 Preprocessor1_Model1\n#> # ‚Ä¶ with 446 more rows"
  },
  {
    "objectID": "slides/04-evaluating-models.html#section-3",
    "href": "slides/04-evaluating-models.html#section-3",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "tree_preds %>% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "slides/04-evaluating-models.html#where-are-the-fitted-models",
    "href": "slides/04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 5\n#>    splits           id     .metrics         .notes           .predictions     \n#>    <list>           <chr>  <list>           <list>           <list>           \n#>  1 <split [408/48]> Fold01 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  2 <split [408/48]> Fold02 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  3 <split [408/48]> Fold03 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  4 <split [409/47]> Fold04 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [47 √ó 4]>\n#>  5 <split [411/45]> Fold05 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [45 √ó 4]>\n#>  6 <split [412/44]> Fold06 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  7 <split [412/44]> Fold07 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  8 <split [412/44]> Fold08 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  9 <split [412/44]> Fold09 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#> 10 <split [412/44]> Fold10 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n\n\nüóëÔ∏è"
  },
  {
    "objectID": "slides/04-evaluating-models.html#bootstrapping",
    "href": "slides/04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/04-evaluating-models.html#bootstrapping-1",
    "href": "slides/04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(frog_train)\n#> # Bootstrap sampling \n#> # A tibble: 25 √ó 2\n#>    splits            id         \n#>    <list>            <chr>      \n#>  1 <split [456/163]> Bootstrap01\n#>  2 <split [456/166]> Bootstrap02\n#>  3 <split [456/173]> Bootstrap03\n#>  4 <split [456/177]> Bootstrap04\n#>  5 <split [456/166]> Bootstrap05\n#>  6 <split [456/163]> Bootstrap06\n#>  7 <split [456/164]> Bootstrap07\n#>  8 <split [456/165]> Bootstrap08\n#>  9 <split [456/170]> Bootstrap09\n#> 10 <split [456/177]> Bootstrap10\n#> # ‚Ä¶ with 15 more rows"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-2",
    "href": "slides/04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nbootstrap folds (change times from the default)\nvalidation set (use the reference guide to find the function)\n\nDon‚Äôt forget to set a seed when you resample!\n\n\n\n05:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#bootstrapping-2",
    "href": "slides/04-evaluating-models.html#bootstrapping-2",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(322)\nbootstraps(frog_train, times = 10)\n#> # Bootstrap sampling \n#> # A tibble: 10 √ó 2\n#>    splits            id         \n#>    <list>            <chr>      \n#>  1 <split [456/173]> Bootstrap01\n#>  2 <split [456/168]> Bootstrap02\n#>  3 <split [456/170]> Bootstrap03\n#>  4 <split [456/164]> Bootstrap04\n#>  5 <split [456/176]> Bootstrap05\n#>  6 <split [456/156]> Bootstrap06\n#>  7 <split [456/166]> Bootstrap07\n#>  8 <split [456/168]> Bootstrap08\n#>  9 <split [456/167]> Bootstrap09\n#> 10 <split [456/170]> Bootstrap10"
  },
  {
    "objectID": "slides/04-evaluating-models.html#validation-set",
    "href": "slides/04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nvalidation_split(frog_train, strata = latency)\n#> # Validation Set Split (0.75/0.25)  using stratification \n#> # A tibble: 1 √ó 2\n#>   splits            id        \n#>   <list>            <chr>     \n#> 1 <split [340/116]> validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "slides/04-evaluating-models.html#random-forest-1",
    "href": "slides/04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ",
    "text": "Random forest üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\nEnsemble many decision tree models\nAll the trees vote! üó≥Ô∏è\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this tomorrow!), as long as there are enough trees"
  },
  {
    "objectID": "slides/04-evaluating-models.html#create-a-random-forest-model",
    "href": "slides/04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec <- rand_forest(trees = 1000, mode = \"regression\")\nrf_spec\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger"
  },
  {
    "objectID": "slides/04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "slides/04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow <- workflow(latency ~ ., rf_spec)\nrf_wflow\n#> ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-3",
    "href": "slides/04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\nplot true vs.¬†predicted values\n\n\n\n\n08:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-2",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_frog <- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res <- fit_resamples(rf_wflow, frog_folds, control = ctrl_frog)\ncollect_metrics(rf_res)\n#> # A tibble: 2 √ó 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   55.9      10  1.71   Preprocessor1_Model1\n#> 2 rsq     standard    0.370    10  0.0306 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/04-evaluating-models.html#section-5",
    "href": "slides/04-evaluating-models.html#section-5",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(rf_res) %>% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "slides/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "slides/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "4 - Evaluating models",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluate-a-workflow-set",
    "href": "slides/04-evaluating-models.html#evaluate-a-workflow-set",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec))\n#> # A workflow set/tibble: 2 √ó 4\n#>   wflow_id              info             option    result    \n#>   <chr>                 <list>           <list>    <list>    \n#> 1 formula_decision_tree <tibble [1 √ó 4]> <opts[0]> <list [0]>\n#> 2 formula_rand_forest   <tibble [1 √ó 4]> <opts[0]> <list [0]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "href": "slides/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %>%\n  workflow_map(\"fit_resamples\", resamples = frog_folds)\n#> # A workflow set/tibble: 2 √ó 4\n#>   wflow_id              info             option    result   \n#>   <chr>                 <list>           <list>    <list>   \n#> 1 formula_decision_tree <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n#> 2 formula_rand_forest   <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "href": "slides/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %>%\n  workflow_map(\"fit_resamples\", resamples = frog_folds) %>%\n  rank_results()\n#> # A tibble: 4 √ó 9\n#>   wflow_id         .config .metric   mean std_err     n preprocessor model  rank\n#>   <chr>            <chr>   <chr>    <dbl>   <dbl> <int> <chr>        <chr> <int>\n#> 1 formula_rand_fo‚Ä¶ Prepro‚Ä¶ rmse    55.8    1.71      10 formula      rand‚Ä¶     1\n#> 2 formula_rand_fo‚Ä¶ Prepro‚Ä¶ rsq      0.371  0.0301    10 formula      rand‚Ä¶     1\n#> 3 formula_decisio‚Ä¶ Prepro‚Ä¶ rmse    59.6    2.31      10 formula      deci‚Ä¶     2\n#> 4 formula_decisio‚Ä¶ Prepro‚Ä¶ rsq      0.305  0.0342    10 formula      deci‚Ä¶     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-4",
    "href": "slides/04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#the-final-fit",
    "href": "slides/04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLet‚Äôs fit the model on the training set and verify our performance using the test set.\n\nWe‚Äôve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# frog_split has train + test info\nfinal_fit <- last_fit(rf_wflow, frog_split) \n\nfinal_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 √ó 6\n#>   splits            id               .metrics .notes   .predictions .workflow \n#>   <list>            <chr>            <list>   <list>   <list>       <list>    \n#> 1 <split [456/116]> train/test split <tibble> <tibble> <tibble>     <workflow>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#what-is-in-final_fit",
    "href": "slides/04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#> # A tibble: 2 √ó 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 rmse    standard      57.1   Preprocessor1_Model1\n#> 2 rsq     standard       0.420 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "slides/04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "slides/04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#> # A tibble: 116 √ó 5\n#>    id               .pred  .row latency .config             \n#>    <chr>            <dbl> <int>   <dbl> <chr>               \n#>  1 train/test split  43.5     1      22 Preprocessor1_Model1\n#>  2 train/test split 104.      3     106 Preprocessor1_Model1\n#>  3 train/test split  76.2     6      39 Preprocessor1_Model1\n#>  4 train/test split  42.4     8      50 Preprocessor1_Model1\n#>  5 train/test split  43.5    10      63 Preprocessor1_Model1\n#>  6 train/test split  43.1    14      25 Preprocessor1_Model1\n#>  7 train/test split  51.5    16      48 Preprocessor1_Model1\n#>  8 train/test split 160.     17      91 Preprocessor1_Model1\n#>  9 train/test split  50.9    32      11 Preprocessor1_Model1\n#> 10 train/test split 171.     33     109 Preprocessor1_Model1\n#> # ‚Ä¶ with 106 more rows\n\n\nThese are predictions for the test set"
  },
  {
    "objectID": "slides/04-evaluating-models.html#section-6",
    "href": "slides/04-evaluating-models.html#section-6",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(final_fit) %>%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, col = \"deeppink4\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "slides/04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "slides/04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#> ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  1000 \n#> Sample size:                      456 \n#> Number of independent variables:  4 \n#> Mtry:                             2 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       3124.583 \n#> R squared (OOB):                  0.3531813\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-5",
    "href": "slides/04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nEnd of the day discussion!\nWhich model do you think you would decide to use?\nWhat surprised you the most?\nWhat is one thing you are looking forward to for tomorrow?\n\n\n\n05:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/05-feature-engineering.html#working-with-our-predictors",
    "href": "slides/05-feature-engineering.html#working-with-our-predictors",
    "title": "5 - Feature engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\n\nThe model requires them in a different format (e.g.¬†dummy variables for lm()).\nThe model needs certain data qualities (e.g.¬†same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a ‚Äúfeature engineering‚Äù).\n\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "slides/05-feature-engineering.html#what-is-feature-engineering",
    "href": "slides/05-feature-engineering.html#what-is-feature-engineering",
    "title": "5 - Feature engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPCA feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection."
  },
  {
    "objectID": "slides/05-feature-engineering.html#example-dates",
    "href": "slides/05-feature-engineering.html#example-dates",
    "title": "5 - Feature engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n\nIt can be re-engineered as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "slides/05-feature-engineering.html#general-definitions",
    "href": "slides/05-feature-engineering.html#general-definitions",
    "title": "5 - Feature engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\nIn a little bit, we‚Äôll see successful (and unsuccessful) feature engineering methods for our example data.\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "slides/05-feature-engineering.html#the-nhl-data",
    "href": "slides/05-feature-engineering.html#the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "The NHL data üèí",
    "text": "The NHL data üèí\n\nFrom Pittsburgh Penguins games, 8,915 shots\nData from the 2015-2016 season\n\n\nLet‚Äôs predict whether a shot is on-goal (a goal or blocked by goaltender) or not."
  },
  {
    "objectID": "slides/05-feature-engineering.html#case-study",
    "href": "slides/05-feature-engineering.html#case-study",
    "title": "5 - Feature engineering",
    "section": "Case study",
    "text": "Case study\n\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nglimpse(season_2015)\n#&gt; Rows: 8,915\n#&gt; Columns: 13\n#&gt; $ on_goal         &lt;fct&gt; no, no, yes, no, yes, no, no, yes, no, no, no, yes, no‚Ä¶\n#&gt; $ period          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n#&gt; $ game_seconds    &lt;dbl&gt; 13, 36, 47, 92, 99, 125, 179, 220, 252, 309, 413, 427,‚Ä¶\n#&gt; $ strength        &lt;fct&gt; even, even, even, even, even, even, even, even, even, ‚Ä¶\n#&gt; $ home_skaters    &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ‚Ä¶\n#&gt; $ away_skaters    &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ‚Ä¶\n#&gt; $ goaltender      &lt;fct&gt; marc_andre_fleury, antti_niemi, antti_niemi, antti_nie‚Ä¶\n#&gt; $ goal_difference &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1,‚Ä¶\n#&gt; $ shooter         &lt;fct&gt; evgeni_malkin, valeri_nichushkin, phil_kessel, beau_be‚Ä¶\n#&gt; $ shooter_type    &lt;fct&gt; center, right_wing, center, right_wing, center, defens‚Ä¶\n#&gt; $ coord_x         &lt;dbl&gt; -66, -49, 64, 65, 80, 42, -55, 62, -67, -58, 76, 56, -‚Ä¶\n#&gt; $ coord_y         &lt;dbl&gt; -11, -21, -31, -21, 13, 31, -19, 15, -9, -16, -8, 25, ‚Ä¶\n#&gt; $ extra_attacker  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶"
  },
  {
    "objectID": "slides/05-feature-engineering.html#data-splitting-strategy",
    "href": "slides/05-feature-engineering.html#data-splitting-strategy",
    "title": "5 - Feature engineering",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "slides/05-feature-engineering.html#why-a-validation-set",
    "href": "slides/05-feature-engineering.html#why-a-validation-set",
    "title": "5 - Feature engineering",
    "section": "Why a validation set?",
    "text": "Why a validation set?\nRecall that resampling gives us performance measures without using the test set.\nIt‚Äôs important to get good resampling statistics (e.g.¬†\\(R^2\\)).\n\nThat usually means having enough data to estimate performance.\n\nWhen you have ‚Äúa lot‚Äù of data, a validation set can be an efficient way to do this."
  },
  {
    "objectID": "slides/05-feature-engineering.html#splitting-the-nhl-data",
    "href": "slides/05-feature-engineering.html#splitting-the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "Splitting the NHL data ",
    "text": "Splitting the NHL data \n\nset.seed(23)\nnhl_split &lt;- initial_split(season_2015, prop = 3/4)\nnhl_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;6686/2229/8915&gt;\n\nnhl_train_and_val &lt;- training(nhl_split)\nnhl_test  &lt;- testing(nhl_split)\n\n## not testing\nnrow(nhl_train_and_val)\n#&gt; [1] 6686\n \n## testing\nnrow(nhl_test)\n#&gt; [1] 2229"
  },
  {
    "objectID": "slides/05-feature-engineering.html#validation-split",
    "href": "slides/05-feature-engineering.html#validation-split",
    "title": "5 - Feature engineering",
    "section": "Validation split ",
    "text": "Validation split \nSince there are a lot of observations, we‚Äôll use a validation set:\n\nset.seed(234)\nnhl_val &lt;- validation_split(nhl_train_and_val, prop = 0.80)\nnhl_val\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 √ó 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [5348/1338]&gt; validation\n\n\nRemember that a validation split is a type of resample."
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn",
    "href": "slides/05-feature-engineering.html#your-turn",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nLet‚Äôs explore the training set data.\nUse the function plot_nhl_shots() for nice spatial plots of the data.\n\n\n\nnhl_train &lt;- analysis(nhl_val$splits[[1]])\n\nset.seed(100)\nnhl_train %&gt;% \n  sample_n(200) %&gt;%\n  plot_nhl_shots(emphasis = shooter_type)\n\n\n\n\n\n\n\n\n‚àí+\n08:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "slides/05-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "5 - Feature engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "slides/05-feature-engineering.html#a-first-recipe",
    "href": "slides/05-feature-engineering.html#a-first-recipe",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train)\n\n\n\nThe recipe() function assigns columns to roles of ‚Äúoutcome‚Äù or ‚Äúpredictor‚Äù using the formula"
  },
  {
    "objectID": "slides/05-feature-engineering.html#a-first-recipe-1",
    "href": "slides/05-feature-engineering.html#a-first-recipe-1",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(nhl_rec)\n#&gt; # A tibble: 13 √ó 4\n#&gt;    variable        type      role      source  \n#&gt;    &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 period          &lt;chr [2]&gt; predictor original\n#&gt;  2 game_seconds    &lt;chr [2]&gt; predictor original\n#&gt;  3 strength        &lt;chr [3]&gt; predictor original\n#&gt;  4 home_skaters    &lt;chr [2]&gt; predictor original\n#&gt;  5 away_skaters    &lt;chr [2]&gt; predictor original\n#&gt;  6 goaltender      &lt;chr [3]&gt; predictor original\n#&gt;  7 goal_difference &lt;chr [2]&gt; predictor original\n#&gt;  8 shooter         &lt;chr [3]&gt; predictor original\n#&gt;  9 shooter_type    &lt;chr [3]&gt; predictor original\n#&gt; 10 coord_x         &lt;chr [2]&gt; predictor original\n#&gt; 11 coord_y         &lt;chr [2]&gt; predictor original\n#&gt; 12 extra_attacker  &lt;chr [2]&gt; predictor original\n#&gt; 13 on_goal         &lt;chr [3]&gt; outcome   original"
  },
  {
    "objectID": "slides/05-feature-engineering.html#create-indicator-variables",
    "href": "slides/05-feature-engineering.html#create-indicator-variables",
    "title": "5 - Feature engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns."
  },
  {
    "objectID": "slides/05-feature-engineering.html#filter-out-constant-columns",
    "href": "slides/05-feature-engineering.html#filter-out-constant-columns",
    "title": "5 - Feature engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of ‚Äúpredictor‚Äù"
  },
  {
    "objectID": "slides/05-feature-engineering.html#normalization",
    "href": "slides/05-feature-engineering.html#normalization",
    "title": "5 - Feature engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "slides/05-feature-engineering.html#reduce-correlation",
    "href": "slides/05-feature-engineering.html#reduce-correlation",
    "title": "5 - Feature engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "slides/05-feature-engineering.html#other-possible-steps",
    "href": "slides/05-feature-engineering.html#other-possible-steps",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extraction‚Ä¶"
  },
  {
    "objectID": "slides/05-feature-engineering.html#other-possible-steps-1",
    "href": "slides/05-feature-engineering.html#other-possible-steps-1",
    "title": "5 - Feature engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = on_goal)\n\n\nA fancy machine learning supervised dimension reduction technique‚Ä¶\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "slides/05-feature-engineering.html#other-possible-steps-2",
    "href": "slides/05-feature-engineering.html#other-possible-steps-2",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_ns(coord_y, coord_x, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-1",
    "href": "slides/05-feature-engineering.html#your-turn-1",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the on-goal data to :\n\ncreate one-hot indicator variables\nremove zero-variance variables\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#minimal-recipe",
    "href": "slides/05-feature-engineering.html#minimal-recipe",
    "title": "5 - Feature engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nnhl_indicators &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/05-feature-engineering.html#using-a-workflow",
    "href": "slides/05-feature-engineering.html#using-a-workflow",
    "title": "5 - Feature engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nnhl_glm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(nhl_indicators) %&gt;%\n  add_model(logistic_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nnhl_glm_res &lt;-\n  nhl_glm_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.756     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.753     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-2",
    "href": "slides/05-feature-engineering.html#your-turn-2",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#holdout-predictions",
    "href": "slides/05-feature-engineering.html#holdout-predictions",
    "title": "5 - Feature engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nglm_val_pred &lt;- collect_predictions(nhl_glm_res)\nglm_val_pred %&gt;% slice(1:7)\n#&gt; # A tibble: 7 √ó 7\n#&gt;   id             .pred_yes .pred_no  .row .pred_class on_goal .config           \n#&gt;   &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;chr&gt;             \n#&gt; 1 validation 0.00844          0.992     2 no          no      Preprocessor1_Mod‚Ä¶\n#&gt; 2 validation 0.0864           0.914    10 no          no      Preprocessor1_Mod‚Ä¶\n#&gt; 3 validation 0.799            0.201    22 yes         yes     Preprocessor1_Mod‚Ä¶\n#&gt; 4 validation 0.615            0.385    24 yes         yes     Preprocessor1_Mod‚Ä¶\n#&gt; 5 validation 0.787            0.213    31 yes         yes     Preprocessor1_Mod‚Ä¶\n#&gt; 6 validation 0.872            0.128    39 yes         yes     Preprocessor1_Mod‚Ä¶\n#&gt; 7 validation 0.00000000903    1.00     40 no          no      Preprocessor1_Mod‚Ä¶"
  },
  {
    "objectID": "slides/05-feature-engineering.html#two-class-data-1",
    "href": "slides/05-feature-engineering.html#two-class-data-1",
    "title": "5 - Feature engineering",
    "section": "Two class data",
    "text": "Two class data\nThese definitions assume that we know the threshold for converting ‚Äúsoft‚Äù probability predictions into ‚Äúhard‚Äù class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity ‚¨áÔ∏è, specificity ‚¨ÜÔ∏è\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity ‚¨ÜÔ∏è, specificity ‚¨áÔ∏è"
  },
  {
    "objectID": "slides/05-feature-engineering.html#varying-the-threshold",
    "href": "slides/05-feature-engineering.html#varying-the-threshold",
    "title": "5 - Feature engineering",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "slides/05-feature-engineering.html#roc-curves",
    "href": "slides/05-feature-engineering.html#roc-curves",
    "title": "5 - Feature engineering",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 üíØ\nROC AUC = 1/2 üò¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "slides/05-feature-engineering.html#roc-curves-1",
    "href": "slides/05-feature-engineering.html#roc-curves-1",
    "title": "5 - Feature engineering",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\nroc_curve_points &lt;- glm_val_pred %&gt;% roc_curve(truth = on_goal, .pred_yes)\nroc_curve_points %&gt;% slice(1, 50, 100)\n#&gt; # A tibble: 3 √ó 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 -Inf            0            1    \n#&gt; 2    0.00143      0.0412       0.968\n#&gt; 3    0.00988      0.127        0.968\n\nglm_val_pred %&gt;% roc_auc(truth = on_goal, .pred_yes)\n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.753"
  },
  {
    "objectID": "slides/05-feature-engineering.html#roc-curve-plot",
    "href": "slides/05-feature-engineering.html#roc-curve-plot",
    "title": "5 - Feature engineering",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\nautoplot(roc_curve_points)"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-3",
    "href": "slides/05-feature-engineering.html#your-turn-3",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "href": "slides/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "title": "5 - Feature engineering",
    "section": "What do we do with the player data? üèí",
    "text": "What do we do with the player data? üèí\nThere are 598 unique player values in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables üò≥\nlump players who rarely shoot into an ‚Äúother‚Äù group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the shooter column with the estimated effect of that predictor\n\n\n\nLet‚Äôs look at othering then effect encodings."
  },
  {
    "objectID": "slides/05-feature-engineering.html#per-player-statistics",
    "href": "slides/05-feature-engineering.html#per-player-statistics",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics",
    "text": "Per-player statistics"
  },
  {
    "objectID": "slides/05-feature-engineering.html#collapsing-factor-levels",
    "href": "slides/05-feature-engineering.html#collapsing-factor-levels",
    "title": "5 - Feature engineering",
    "section": "Collapsing factor levels",
    "text": "Collapsing factor levels\nThere is a recipe step that will redefine factor levels based on the their frequency in the training set:\n\nnhl_other_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  # Any player with &lt;= 0.01% of shots is set to \"other\"\n  step_other(shooter, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nUsing this code, 402 players (out of 598) were collapsed into ‚Äúother‚Äù based on the training set.\nWe could try to optimize the threshold for collapsing (see the next set of slides on model tuning)."
  },
  {
    "objectID": "slides/05-feature-engineering.html#does-othering-help",
    "href": "slides/05-feature-engineering.html#does-othering-help",
    "title": "5 - Feature engineering",
    "section": "Does othering help?",
    "text": "Does othering help?\n\nnhl_other_wflow &lt;-\n  nhl_glm_wflow %&gt;%\n  update_recipe(nhl_other_rec)\n\nnhl_other_res &lt;-\n  nhl_other_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_other_res)\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.778     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.804     1      NA Preprocessor1_Model1\n\nA little better ROC AUC and much faster to complete.\nNow let‚Äôs look at a more sophisticated tool called effect encodings."
  },
  {
    "objectID": "slides/05-feature-engineering.html#what-is-an-effect-encoding",
    "href": "slides/05-feature-engineering.html#what-is-an-effect-encoding",
    "title": "5 - Feature engineering",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitative‚Äôs predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 √ó 3\n#&gt;   on_goal shooter        .row\n#&gt;   &lt;fct&gt;   &lt;fct&gt;         &lt;int&gt;\n#&gt; 1 yes     sidney_crosby     1\n#&gt; 2 yes     mike_hoffman      2\n#&gt; 3 yes     ian_cole          3\n#&gt; 4 yes     matt_cullen       4\n#&gt; 5 yes     kris_letang       5\n#&gt; 6 no      nazem_kadri       6\n#&gt; 7 yes     david_perron      7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 √ó 3\n#&gt;   on_goal shooter  .row\n#&gt;   &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 yes       0.220     1\n#&gt; 2 yes       0.302     2\n#&gt; 3 yes       0.261     3\n#&gt; 4 yes       0.463     4\n#&gt; 5 yes       0.197     5\n#&gt; 6 no        0.302     6\n#&gt; 7 yes       0.400     7\n\n\n\nThe shooter column is replaced with the log-odds of being on goal.\n\nAs a reminder:\n\\[\\text{log-odds} = log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)\\]\nwhere \\(\\hat{p}\\) is the on goal rate estimate.\nFor logistic regression, this is what the predictors are modeling. The log-odds are more likely to be linear with the outcome."
  },
  {
    "objectID": "slides/05-feature-engineering.html#per-player-statistics-again",
    "href": "slides/05-feature-engineering.html#per-player-statistics-again",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics again",
    "text": "Per-player statistics again\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these rates use partial pooling.\nPooling borrows strength across players and shrinks extreme values (e.g.¬†zero or one) towards the mean for players with very few shots.\nThe embed package has recipe steps for effect encodings.\n\n\n\n\nPartial pooling gives better estimates for players with fewer shots by shrinking the estimate to the overall on-goal rate (56.5%)"
  },
  {
    "objectID": "slides/05-feature-engineering.html#partial-pooling",
    "href": "slides/05-feature-engineering.html#partial-pooling",
    "title": "5 - Feature engineering",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "slides/05-feature-engineering.html#player-effects",
    "href": "slides/05-feature-engineering.html#player-effects",
    "title": "5 - Feature engineering",
    "section": "Player effects  ",
    "text": "Player effects  \n\nlibrary(embed)\n\nnhl_effect_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "slides/05-feature-engineering.html#recipes-are-estimated",
    "href": "slides/05-feature-engineering.html#recipes-are-estimated",
    "title": "5 - Feature engineering",
    "section": "Recipes are estimated ",
    "text": "Recipes are estimated \nPreprocessing steps in a recipe use the training set to compute quantities.\n\nWhat kind of quantities are computed for preprocessing?\n\nLevels of a factor\nWhether a column has zero variance\nNormalization\nFeature extraction\nEffect encodings\n\n\n\nWhen a recipe is part of a workflow, this estimation occurs when fit() is called."
  },
  {
    "objectID": "slides/05-feature-engineering.html#effect-encoding-results",
    "href": "slides/05-feature-engineering.html#effect-encoding-results",
    "title": "5 - Feature engineering",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nnhl_effect_wflow &lt;-\n  nhl_glm_wflow %&gt;%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res &lt;-\n  nhl_effect_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_effect_res)\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.791     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.805     1      NA Preprocessor1_Model1\n\nBetter and it can handle new players (if they occur)."
  },
  {
    "objectID": "slides/05-feature-engineering.html#angle",
    "href": "slides/05-feature-engineering.html#angle",
    "title": "5 - Feature engineering",
    "section": "Angle",
    "text": "Angle\n\nnhl_angle_rec &lt;-\n  nhl_effect_rec %&gt;%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) )\n  )\n\n\n\nNote the danger of using step_mutate() ‚Äì easy to have data leakage"
  },
  {
    "objectID": "slides/05-feature-engineering.html#shot-from-the-defensive-zone",
    "href": "slides/05-feature-engineering.html#shot-from-the-defensive-zone",
    "title": "5 - Feature engineering",
    "section": "Shot from the defensive zone",
    "text": "Shot from the defensive zone\n\nnhl_zone_rec &lt;-\n  nhl_angle_rec %&gt;%\n  step_mutate(\n    defensive_zone = ifelse(coord_x &lt;= -25.5, 1, 0)\n  )"
  },
  {
    "objectID": "slides/05-feature-engineering.html#behind-goal-line",
    "href": "slides/05-feature-engineering.html#behind-goal-line",
    "title": "5 - Feature engineering",
    "section": "Behind goal line",
    "text": "Behind goal line\n\nnhl_behind_rec &lt;-\n  nhl_zone_rec %&gt;%\n  step_mutate(\n    behind_goal_line = ifelse(coord_x &gt;= 89, 1, 0)\n  )"
  },
  {
    "objectID": "slides/05-feature-engineering.html#fit-different-recipes",
    "href": "slides/05-feature-engineering.html#fit-different-recipes",
    "title": "5 - Feature engineering",
    "section": "Fit different recipes    ",
    "text": "Fit different recipes    \nA workflow set can cross models and/or preprocessors and then resample them en masse.\n\nno_coord_rec &lt;- \n  nhl_indicators %&gt;% \n  step_rm(starts_with(\"coord\"))\n\nset.seed(9)\n\nnhl_glm_set_res &lt;-\n  workflow_set(\n    list(`1_no_coord` = no_coord_rec,   `2_other` = nhl_other_rec, \n         `3_effects`  = nhl_effect_rec, `4_angle` = nhl_angle_rec, \n         `5_zone`     = nhl_zone_rec,   `6_bgl`   = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %&gt;%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val, verbose = TRUE, control = ctrl)"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-4",
    "href": "slides/05-feature-engineering.html#your-turn-4",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a workflow set with 2 or 3 recipes.\n(Consider using recipes we‚Äôve already created.)\nUse workflow_map() to resample the workflow set.\n\n\n\n‚àí+\n08:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#compare-recipes",
    "href": "slides/05-feature-engineering.html#compare-recipes",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %&gt;%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point(size = 3) +\n  labs(y = NULL, x = \"ROC AUC (validation set)\")"
  },
  {
    "objectID": "slides/05-feature-engineering.html#compare-recipes-1",
    "href": "slides/05-feature-engineering.html#compare-recipes-1",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes"
  },
  {
    "objectID": "slides/05-feature-engineering.html#debugging-a-recipe",
    "href": "slides/05-feature-engineering.html#debugging-a-recipe",
    "title": "5 - Feature engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.¬†encoded_players) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back.\n\n\n\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "slides/05-feature-engineering.html#example",
    "href": "slides/05-feature-engineering.html#example",
    "title": "5 - Feature engineering",
    "section": "Example",
    "text": "Example\n\nnhl_angle_fit &lt;- prep(nhl_angle_rec)\n\ntidy(nhl_angle_fit, number = 1) %&gt;% slice(1:4)\n#&gt; # A tibble: 4 √ó 4\n#&gt;   level           value terms   id                 \n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              \n#&gt; 1 aaron_ekblad    0.250 shooter lencode_mixed_qBdjK\n#&gt; 2 adam_clendening 0.202 shooter lencode_mixed_qBdjK\n#&gt; 3 adam_cracknell  0.288 shooter lencode_mixed_qBdjK\n#&gt; 4 adam_henrique   0.221 shooter lencode_mixed_qBdjK\n\nbake(nhl_angle_fit, nhl_train %&gt;% slice(1:3), starts_with(\"coord\"), angle, shooter)\n#&gt; # A tibble: 3 √ó 4\n#&gt;   coord_x coord_y angle shooter\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1      67     -37  59.3   0.220\n#&gt; 2      55      -7  11.6   0.302\n#&gt; 3      42     -18  21.0   0.261"
  },
  {
    "objectID": "slides/05-feature-engineering.html#more-on-recipes",
    "href": "slides/05-feature-engineering.html#more-on-recipes",
    "title": "5 - Feature engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters.\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search üí† which tests a pre-defined set of candidate values\nIterative search üåÄ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet‚Äôs take our previous recipe and add a few changes:\n\nglm_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) ),\n    defensive_zone = ifelse(coord_x <= -25.5, 1, 0),\n    behind_goal_line = ifelse(coord_x >= 89, 1, 0)\n  ) %>%\n  step_zv(all_predictors()) %>%\n  step_ns(angle, deg_free = tune(\"angle\")) %>%\n  step_ns(coord_x, deg_free = tune(\"coord_x\")) %>%\n  step_normalize(all_numeric_predictors())\n\n\nLet‚Äôs tune() our spline terms!"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "href": "slides/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet‚Äôs take our previous recipe and add a few changes:\n\nglm_spline_wflow <-\n  workflow() %>%\n  add_model(logistic_reg()) %>%\n  add_recipe(glm_rec)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#section",
    "href": "slides/06-tuning-hyperparameters.html#section",
    "title": "6 - Tuning Hyperparameters",
    "section": "",
    "text": "Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.\nMore spline terms = more ‚Äúwiggly‚Äù, i.e.¬†flexibly model a nonlinear relationship\nHow many spline terms? This is called degrees of freedom\n2 and 5 look like they underfit; 20 and 100 look like they overfit"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "href": "slides/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "title": "6 - Tuning Hyperparameters",
    "section": "Splines and nonlinear relationships",
    "text": "Splines and nonlinear relationships\n\n\n\n\n\n\n\n\n\n\nOur hockey data exhibits nonlinear relationships\nWe can model nonlinearity like this via a model (later this afternoon) or feature engineering\nHow do we decide how ‚Äúwiggly‚Äù or flexible to make our spline features? TUNING"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#grid-search",
    "href": "slides/06-tuning-hyperparameters.html#grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nParameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#create-a-grid",
    "href": "slides/06-tuning-hyperparameters.html#create-a-grid",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nglm_spline_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>       angle deg_free nparam[+]\n#>     coord_x deg_free nparam[+]\n\n\nA parameter set can be updated (e.g.¬†to change the ranges)."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#create-a-grid-1",
    "href": "slides/06-tuning-hyperparameters.html#create-a-grid-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(12)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 2\n#>    angle coord_x\n#>    <int>   <int>\n#>  1     5       8\n#>  2     1      13\n#>  3     8       6\n#>  4    11       5\n#>  5     4       1\n#>  6     6      10\n#>  7    10      15\n#>  8    12      13\n#>  9     8      13\n#> 10    10       6\n#> # ‚Ä¶ with 15 more rows\n\n\n\n\nA space-filling design like this tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn",
    "href": "slides/06-tuning-hyperparameters.html#your-turn",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#create-a-grid-2",
    "href": "slides/06-tuning-hyperparameters.html#create-a-grid-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(12)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 25)\n\ngrid\n#> # A tibble: 225 √ó 2\n#>    angle coord_x\n#>    <int>   <int>\n#>  1     1       1\n#>  2     2       1\n#>  3     3       1\n#>  4     4       1\n#>  5     5       1\n#>  6     6       1\n#>  7     7       1\n#>  8     8       1\n#>  9     9       1\n#> 10    10       1\n#> # ‚Ä¶ with 215 more rows\n\n\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the deg_free parameters only have a range of 1->15."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "slides/06-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "6 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nset.seed(12)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(angle = spline_degree(c(2L, 50L)),\n         coord_x = spline_degree(c(2L, 50L))) %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 2\n#>    angle coord_x\n#>    <int>   <int>\n#>  1    14      27\n#>  2     4      42\n#>  3    26      20\n#>  4    36      16\n#>  5    13       3\n#>  6    20      33\n#>  7    31      49\n#>  8    40      44\n#>  9    24      45\n#> 10    34      18\n#> # ‚Ä¶ with 15 more rows\n\n\nEven though angle is a deg_free parameter in step_ns(), we don‚Äôt use the dials deg_free() object here. We have a special spline_degree() function that has better defaults for splines."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#the-results",
    "href": "slides/06-tuning-hyperparameters.html#the-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %>% \n  ggplot(aes(angle, coord_x)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#spline-grid-search",
    "href": "slides/06-tuning-hyperparameters.html#spline-grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Spline grid search   ",
    "text": "Spline grid search   \n\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglm_spline_res <-\n  glm_spline_wflow %>%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\n\nglm_spline_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes           .predictions         \n#>   <list>              <chr>      <list>            <list>           <list>               \n#> 1 <split [4482/1121]> validation <tibble [50 √ó 6]> <tibble [3 √ó 3]> <tibble [28,025 √ó 8]>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x3: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-1",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTune our glm_wflow.\nWhat happens if you don‚Äôt supply a grid argument to tune_grid()?\n\n\n\n05:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#grid-results",
    "href": "slides/06-tuning-hyperparameters.html#grid-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(glm_spline_res)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res)\n#> # A tibble: 50 √ó 8\n#>    angle coord_x .metric  .estimator  mean     n std_err .config              \n#>    <int>   <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1    14      27 accuracy binary     0.858     1      NA Preprocessor01_Model1\n#>  2    14      27 roc_auc  binary     0.878     1      NA Preprocessor01_Model1\n#>  3     4      42 accuracy binary     0.855     1      NA Preprocessor02_Model1\n#>  4     4      42 roc_auc  binary     0.879     1      NA Preprocessor02_Model1\n#>  5    26      20 accuracy binary     0.858     1      NA Preprocessor03_Model1\n#>  6    26      20 roc_auc  binary     0.876     1      NA Preprocessor03_Model1\n#>  7    36      16 accuracy binary     0.857     1      NA Preprocessor04_Model1\n#>  8    36      16 roc_auc  binary     0.875     1      NA Preprocessor04_Model1\n#>  9    13       3 accuracy binary     0.859     1      NA Preprocessor05_Model1\n#> 10    13       3 roc_auc  binary     0.867     1      NA Preprocessor05_Model1\n#> # ‚Ä¶ with 40 more rows"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results-1",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#> # A tibble: 50 √ó 7\n#>    id         angle coord_x .metric  .estimator .estimate .config              \n#>    <chr>      <int>   <int> <chr>    <chr>          <dbl> <chr>                \n#>  1 validation    14      27 accuracy binary         0.858 Preprocessor01_Model1\n#>  2 validation    14      27 roc_auc  binary         0.878 Preprocessor01_Model1\n#>  3 validation     4      42 accuracy binary         0.855 Preprocessor02_Model1\n#>  4 validation     4      42 roc_auc  binary         0.879 Preprocessor02_Model1\n#>  5 validation    26      20 accuracy binary         0.858 Preprocessor03_Model1\n#>  6 validation    26      20 roc_auc  binary         0.876 Preprocessor03_Model1\n#>  7 validation    36      16 accuracy binary         0.857 Preprocessor04_Model1\n#>  8 validation    36      16 roc_auc  binary         0.875 Preprocessor04_Model1\n#>  9 validation    13       3 accuracy binary         0.859 Preprocessor05_Model1\n#> 10 validation    13       3 roc_auc  binary         0.867 Preprocessor05_Model1\n#> # ‚Ä¶ with 40 more rows"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 8\n#>   angle coord_x .metric .estimator  mean     n std_err .config              \n#>   <int>   <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    10      15 roc_auc binary     0.879     1      NA Preprocessor13_Model1\n#> 2    10      32 roc_auc binary     0.879     1      NA Preprocessor20_Model1\n#> 3     4      42 roc_auc binary     0.879     1      NA Preprocessor02_Model1\n#> 4    14      27 roc_auc binary     0.878     1      NA Preprocessor01_Model1\n#> 5    18      25 roc_auc binary     0.878     1      NA Preprocessor18_Model1"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 3\n#>   angle coord_x .config              \n#>   <int>   <int> <chr>                \n#> 1    10      15 Preprocessor13_Model1\n\n\nThis best result has:\n\nlow-degree spline for angle (less ‚Äúwiggly‚Äù, less complex)\nhigher-degree spline for coord_x (more ‚Äúwiggly‚Äù, more complex)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-trees-1",
    "href": "slides/06-tuning-hyperparameters.html#boosted-trees-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ",
    "text": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\nEnsemble many decision tree models\n\n\n\nReview how a decision tree model works:\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#single-decision-tree",
    "href": "slides/06-tuning-hyperparameters.html#single-decision-tree",
    "title": "6 - Tuning Hyperparameters",
    "section": "Single decision tree",
    "text": "Single decision tree"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-trees-2",
    "href": "slides/06-tuning-hyperparameters.html#boosted-trees-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ",
    "text": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\nBoosting methods fit a sequence of tree-based models.\n\n\nEach tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\nThis is like gradient-based steepest ascent methods from calculus."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nMost modern boosting methods have a lot of tuning parameters!\n\n\nFor tree growth and pruning (min_n, max_depth, etc)\nFor boosting (trees, stop_iter, learn_rate)\n\n\n\nWe‚Äôll use early stopping to stop boosting when a few iterations produce consecutively worse results."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "href": "slides/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "title": "6 - Tuning Hyperparameters",
    "section": "Comparing tree ensembles",
    "text": "Comparing tree ensembles\n\n\nRandom forest\n\nIndependent trees\nBootstrapped data\nNo pruning\n1000‚Äôs of trees\n\n\nBoosting\n\nDependent trees\nDifferent case weights\nTune tree parameters\nFar fewer trees\n\n\n\nThe general consensus for tree-based models is, in terms of performance: boosting > random forest > bagging > single trees."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-tree-code",
    "href": "slides/06-tuning-hyperparameters.html#boosted-tree-code",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree code",
    "text": "Boosted tree code\n\nxgb_spec <-\n  boost_tree(\n    trees = tune(), min_n = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %>%\n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n\nxgb_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nxgb_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(xgb_rec)\n\n\nvalidation is an argument to parsnip::xgb_train(), not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to xgb.train(watchlist = list(validation = data)).\nSee translate(xgb_spec) to see where it is passed to parsnip::xgb_train()."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-2",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate your boosted tree workflow.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#running-in-parallel",
    "href": "slides/06-tuning-hyperparameters.html#running-in-parallel",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models don‚Äôt depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores <- parallelly::availableCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "slides/06-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\n\n\n\n\n\n\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning",
    "href": "slides/06-tuning-hyperparameters.html#tuning",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning ",
    "text": "Tuning \nThis will take some time to run ‚è≥\n\nset.seed(9)\n\nxgb_res <-\n  xgb_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 30, control = ctrl) # automatic grid now!"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-3",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nStart tuning the boosted tree model!\nWe won‚Äôt wait for everyone‚Äôs tuning to finish, but take this time to get it started before we move on.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results-2",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nxgb_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes           .predictions          \n#>   <list>              <chr>      <list>            <list>           <list>                \n#> 1 <split [4482/1121]> validation <tibble [60 √ó 9]> <tibble [0 √ó 3]> <tibble [33,630 √ó 11]>"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results-3",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nautoplot(xgb_res)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#again-with-the-location-features",
    "href": "slides/06-tuning-hyperparameters.html#again-with-the-location-features",
    "title": "6 - Tuning Hyperparameters",
    "section": "Again with the location features",
    "text": "Again with the location features\n\ncoord_rec <- \n  xgb_rec %>%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) ),\n    defensive_zone = ifelse(coord_x <= -25.5, 1, 0),\n    behind_goal_line = ifelse(coord_x >= 89, 1, 0)\n  )\n\nxgb_coord_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res <-\n  xgb_coord_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 30, control = ctrl)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "href": "slides/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "title": "6 - Tuning Hyperparameters",
    "section": "Did the machine figure it out?",
    "text": "Did the machine figure it out?\nNo extra features:\n\nshow_best(xgb_res, metric = \"roc_auc\", n = 3)\n#> # A tibble: 3 √ó 11\n#>   trees min_n tree_depth learn_rate loss_reduction .metric .estimator  mean     n std_err .config              \n#>   <int> <int>      <int>      <dbl>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1   822    10          7    0.00527       0.00529  roc_auc binary     0.883     1      NA Preprocessor1_Model07\n#> 2  1697     3          3    0.0116        0.674    roc_auc binary     0.882     1      NA Preprocessor1_Model02\n#> 3  1264    16          2    0.00732       0.000340 roc_auc binary     0.881     1      NA Preprocessor1_Model12\n\nWith additional coordinate features:\n\nshow_best(xgb_coord_res, metric = \"roc_auc\", n = 3)\n#> # A tibble: 3 √ó 11\n#>   trees min_n tree_depth learn_rate loss_reduction .metric .estimator  mean     n std_err .config              \n#>   <int> <int>      <int>      <dbl>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1  1753    15          1    0.00563       0.0663   roc_auc binary     0.881     1      NA Preprocessor1_Model11\n#> 2  1697     3          3    0.0116        0.674    roc_auc binary     0.881     1      NA Preprocessor1_Model02\n#> 3  1264    16          2    0.00732       0.000340 roc_auc binary     0.879     1      NA Preprocessor1_Model12"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#compare-models",
    "href": "slides/06-tuning-hyperparameters.html#compare-models",
    "title": "6 - Tuning Hyperparameters",
    "section": "Compare models",
    "text": "Compare models\nBest logistic regression results:\n\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.879     1      NA Preprocessor13_Model1\n\n\nBest boosting results:\n\nxgb_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.883     1      NA Preprocessor1_Model07"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#updating-the-workflow",
    "href": "slides/06-tuning-hyperparameters.html#updating-the-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Updating the workflow  ",
    "text": "Updating the workflow  \n\nbest_auc <- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#> # A tibble: 1 √ó 3\n#>   angle coord_x .config              \n#>   <int>   <int> <chr>                \n#> 1    10      15 Preprocessor13_Model1\n\nglm_spline_wflow <-\n  glm_spline_wflow %>% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#> ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> 7 Recipe Steps\n#> \n#> ‚Ä¢ step_lencode_mixed()\n#> ‚Ä¢ step_dummy()\n#> ‚Ä¢ step_mutate()\n#> ‚Ä¢ step_zv()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_normalize()\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "href": "slides/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "title": "6 - Tuning Hyperparameters",
    "section": "The final fit to the NHL data  ",
    "text": "The final fit to the NHL data  \n\ntest_res <- \n  glm_spline_wflow %>% \n  last_fit(split = nhl_split)\n\ntest_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 √ó 6\n#>   splits              id               .metrics         .notes           .predictions         .workflow \n#>   <list>              <chr>            <list>           <list>           <list>               <list>    \n#> 1 <split [5603/1868]> train/test split <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [1,868 √ó 6]> <workflow>\n\n\nRemember that last_fit() fits one time with the combined training and validation set, then evaluates one time with the testing set."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-4",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-4",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nFinalize your workflow with the best parameters.\nCreate a final fit.\n\n\n\n08:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "href": "slides/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "title": "6 - Tuning Hyperparameters",
    "section": "Estimates of ROC AUC ",
    "text": "Estimates of ROC AUC \nValidation results from tuning:\n\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, mean, n, std_err)\n#> # A tibble: 1 √ó 4\n#>   .metric  mean     n std_err\n#>   <chr>   <dbl> <int>   <dbl>\n#> 1 roc_auc 0.879     1      NA\n\n\nTest set results:\n\ntest_res %>% collect_metrics()\n#> # A tibble: 2 √ó 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.843 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.866 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#final-fitted-workflow",
    "href": "slides/06-tuning-hyperparameters.html#final-fitted-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Final fitted workflow",
    "text": "Final fitted workflow\nExtract the final fitted workflow, fit using the training set:\n\nfinal_glm_spline_wflow <- \n  test_res %>% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#> # A tibble: 3 √ó 1\n#>   .pred_class\n#>   <fct>      \n#> 1 no         \n#> 2 yes        \n#> 3 yes"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#next-steps",
    "href": "slides/06-tuning-hyperparameters.html#next-steps",
    "title": "6 - Tuning Hyperparameters",
    "section": "Next steps",
    "text": "Next steps\n\nDocument the model.\n\n\n\nDeploy the model.\n\n\n\n\nCreate an applicability domain model to help monitor our data over time.\n\n\n\n\nUse explainers to characterize the model and the predictions."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#explain-yourself",
    "href": "slides/06-tuning-hyperparameters.html#explain-yourself",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain yourself",
    "text": "Explain yourself\nThere are two categories of model explanations, global and local.\n\n\nGlobal model explanations provide an overall understanding aggregated over a whole set of observations.\nLocal model explanations provide information about a prediction for a single observation.\n\n\n\nYou can also build global model explanations by aggregating local model explanations."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "href": "slides/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "title": "6 - Tuning Hyperparameters",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nWe can build explainers using:\n\noriginal, basic predictors\nderived features\n\n\nlibrary(DALEXtra)\n\nglm_explainer <- explain_tidymodels(\n  final_glm_spline_wflow,\n  data = dplyr::select(nhl_train, -on_goal),\n  # DALEX required an integer for factors:\n  y = as.integer(nhl_train$on_goal) - 1,\n  verbose = FALSE\n)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "href": "slides/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates\nWith our explainer, let‚Äôs create partial dependence profiles:\n\nset.seed(123)\npdp_coord_x <- model_profile(\n  glm_explainer,\n  variables = \"coord_x\",\n  N = 500,\n  groups = \"strength\"\n)\n\n\nYou can use the default plot() method or create your own visualization."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "href": "slides/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "href": "slides/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-5",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-5",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate an explainer for our glm model.\nTry using other variables, like extra_attacker or game_seconds.\n\n\n\n05:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/07-transit-case-study.html#chicago-l-train-data",
    "href": "slides/07-transit-case-study.html#chicago-l-train-data",
    "title": "7 - Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (‚ÄúL‚Äù) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "slides/07-transit-case-study.html#predictors",
    "href": "slides/07-transit-case-study.html#predictors",
    "title": "7 - Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "slides/07-transit-case-study.html#l-train-locations",
    "href": "slides/07-transit-case-study.html#l-train-locations",
    "title": "7 - Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "slides/07-transit-case-study.html#your-turn-explore-the-data",
    "href": "slides/07-transit-case-study.html#your-turn-explore-the-data",
    "title": "7 - Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#> [1] 5698   50\nstations\n#>  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#>  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#>  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#> [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#> [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/07-transit-case-study.html#splitting-with-chicago-data",
    "href": "slides/07-transit-case-study.html#splitting-with-chicago-data",
    "title": "7 - Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLet‚Äôs put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split <- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#> <Training/Testing/Total>\n#> <5684/14/5698>\n\nchi_train <- training(chi_split)\nchi_test  <- testing(chi_split)\n\n## training\nnrow(chi_train)\n#> [1] 5684\n \n## testing\nnrow(chi_test)\n#> [1] 14"
  },
  {
    "objectID": "slides/07-transit-case-study.html#time-series-resampling",
    "href": "slides/07-transit-case-study.html#time-series-resampling",
    "title": "7 - Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "slides/07-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "slides/07-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "7 - Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\n\n\n\n\n\n\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "slides/07-transit-case-study.html#times-series-resampling",
    "href": "slides/07-transit-case-study.html#times-series-resampling",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs <-\n  chi_train %>%\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "slides/07-transit-case-study.html#times-series-resampling-1",
    "href": "slides/07-transit-case-study.html#times-series-resampling-1",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs <-\n  chi_train %>%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "slides/07-transit-case-study.html#times-series-resampling-2",
    "href": "slides/07-transit-case-study.html#times-series-resampling-2",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs <-\n  chi_train %>%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "slides/07-transit-case-study.html#times-series-resampling-3",
    "href": "slides/07-transit-case-study.html#times-series-resampling-3",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs <-\n  chi_train %>%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "slides/07-transit-case-study.html#times-series-resampling-4",
    "href": "slides/07-transit-case-study.html#times-series-resampling-4",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs <-\n  chi_train %>%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] %>% assessment() %>% pluck(\"date\") %>% range()\n#> [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] %>% assessment() %>% pluck(\"date\") %>% range()\n#> [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "slides/07-transit-case-study.html#our-resampling-object",
    "href": "slides/07-transit-case-study.html#our-resampling-object",
    "title": "7 - Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#> # Sliding period resampling \n#> # A tibble: 16 √ó 2\n#>    splits            id     \n#>    <list>            <chr>  \n#>  1 <split [5463/14]> Slice01\n#>  2 <split [5467/14]> Slice02\n#>  3 <split [5467/14]> Slice03\n#>  4 <split [5467/14]> Slice04\n#>  5 <split [5467/14]> Slice05\n#>  6 <split [5467/14]> Slice06\n#>  7 <split [5467/14]> Slice07\n#>  8 <split [5467/14]> Slice08\n#>  9 <split [5467/14]> Slice09\n#> 10 <split [5467/14]> Slice10\n#> 11 <split [5467/14]> Slice11\n#> 12 <split [5467/14]> Slice12\n#> 13 <split [5467/14]> Slice13\n#> 14 <split [5467/14]> Slice14\n#> 15 <split [5467/14]> Slice15\n#> 16 <split [5467/11]> Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "slides/07-transit-case-study.html#feature-engineering-with-recipes",
    "href": "slides/07-transit-case-study.html#feature-engineering-with-recipes",
    "title": "7 - Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec <- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of ‚Äúoutcome‚Äù or ‚Äúpredictor‚Äù"
  },
  {
    "objectID": "slides/07-transit-case-study.html#a-recipe",
    "href": "slides/07-transit-case-study.html#a-recipe",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#> # A tibble: 50 √ó 4\n#>    variable         type    role      source  \n#>    <chr>            <chr>   <chr>     <chr>   \n#>  1 Austin           numeric predictor original\n#>  2 Quincy_Wells     numeric predictor original\n#>  3 Belmont          numeric predictor original\n#>  4 Archer_35th      numeric predictor original\n#>  5 Oak_Park         numeric predictor original\n#>  6 Western          numeric predictor original\n#>  7 Clark_Lake       numeric predictor original\n#>  8 Clinton          numeric predictor original\n#>  9 Merchandise_Mart numeric predictor original\n#> 10 Irving_Park      numeric predictor original\n#> # ‚Ä¶ with 40 more rows"
  },
  {
    "objectID": "slides/07-transit-case-study.html#a-recipe---work-with-dates",
    "href": "slides/07-transit-case-study.html#a-recipe---work-with-dates",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec <- \n  recipe(ridership ~ ., data = chi_train) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "slides/07-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "slides/07-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec <- \n  recipe(ridership ~ ., data = chi_train) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we don‚Äôt need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "slides/07-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "slides/07-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec <- \n  recipe(ridership ~ ., data = chi_train) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>%\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points."
  },
  {
    "objectID": "slides/07-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "slides/07-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec <- \n  recipe(ridership ~ ., data = chi_train) %>% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %>% \n  step_holiday(date) %>% \n  update_role(date, new_role = \"id\") %>%\n  update_role_requirements(role = \"id\", bake = TRUE) %>% \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "slides/07-transit-case-study.html#a-recipe---handle-correlations",
    "href": "slides/07-transit-case-study.html#a-recipe---handle-correlations",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec <- \n  chi_rec %>% \n  step_normalize(all_of(!!stations)) %>% \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWe‚Äôll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "slides/07-transit-case-study.html#make-some-models",
    "href": "slides/07-transit-case-study.html#make-some-models",
    "title": "7 - Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLet‚Äôs try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec <- cubist_rules(committees = 25, neighbors = tune())\nmars_spec <- mars(prod_degree = tune()) %>% set_mode(\"regression\")\nlm_spec <- linear_reg()\n\nchi_set <- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) %>% \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "slides/07-transit-case-study.html#process-them-on-the-resamples",
    "href": "slides/07-transit-case-study.html#process-them-on-the-resamples",
    "title": "7 - Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res <- \n  chi_set %>% \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "slides/07-transit-case-study.html#how-do-the-results-look",
    "href": "slides/07-transit-case-study.html#how-do-the-results-look",
    "title": "7 - Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#> # A tibble: 31 √ó 9\n#>    wflow_id     .config              .metric  mean std_err     n preprocessor model   rank\n#>    <chr>        <chr>                <chr>   <dbl>   <dbl> <int> <chr>        <chr>  <int>\n#>  1 pca_cubist   Preprocessor1_Model1 mae     0.798   0.104    16 recipe       cubis‚Ä¶     1\n#>  2 pca_cubist   Preprocessor3_Model3 mae     0.978   0.110    16 recipe       cubis‚Ä¶     2\n#>  3 pca_cubist   Preprocessor4_Model2 mae     0.983   0.122    16 recipe       cubis‚Ä¶     3\n#>  4 pca_cubist   Preprocessor4_Model1 mae     0.991   0.127    16 recipe       cubis‚Ä¶     4\n#>  5 pca_cubist   Preprocessor3_Model2 mae     0.991   0.113    16 recipe       cubis‚Ä¶     5\n#>  6 pca_cubist   Preprocessor2_Model2 mae     1.02    0.118    16 recipe       cubis‚Ä¶     6\n#>  7 pca_cubist   Preprocessor1_Model3 mae     1.05    0.134    16 recipe       cubis‚Ä¶     7\n#>  8 basic_cubist Preprocessor1_Model8 mae     1.07    0.115    16 recipe       cubis‚Ä¶     8\n#>  9 basic_cubist Preprocessor1_Model7 mae     1.07    0.112    16 recipe       cubis‚Ä¶     9\n#> 10 basic_cubist Preprocessor1_Model6 mae     1.07    0.114    16 recipe       cubis‚Ä¶    10\n#> # ‚Ä¶ with 21 more rows"
  },
  {
    "objectID": "slides/07-transit-case-study.html#plot-the-results",
    "href": "slides/07-transit-case-study.html#plot-the-results",
    "title": "7 - Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "slides/07-transit-case-study.html#pull-out-specific-results",
    "href": "slides/07-transit-case-study.html#pull-out-specific-results",
    "title": "7 - Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res %>% \n  extract_workflow_set_result(\"pca_cubist\") %>% \n  autoplot()"
  },
  {
    "objectID": "slides/07-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "slides/07-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "slides/07-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/07-transit-case-study.html#building-a-model-stack",
    "href": "slides/07-transit-case-study.html#building-a-model-stack",
    "title": "7 - Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "slides/07-transit-case-study.html#start-the-stack-and-add-members",
    "href": "slides/07-transit-case-study.html#start-the-stack-and-add-members",
    "title": "7 - Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack <- \n  stacks() %>% \n  add_candidates(chi_res)"
  },
  {
    "objectID": "slides/07-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "slides/07-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "7 - Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res <- blend_predictions(chi_stack)\n\nchi_stack_res\n#> # A tibble: 5 √ó 3\n#>   member           type         weight\n#>   <chr>            <chr>         <dbl>\n#> 1 pca_cubist_1_1   cubist_rules  0.343\n#> 2 pca_cubist_3_2   cubist_rules  0.236\n#> 3 basic_cubist_1_4 cubist_rules  0.189\n#> 4 pca_lm_4_1       linear_reg    0.163\n#> 5 pca_cubist_3_3   cubist_rules  0.109"
  },
  {
    "objectID": "slides/07-transit-case-study.html#how-did-it-do",
    "href": "slides/07-transit-case-study.html#how-did-it-do",
    "title": "7 - Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "slides/07-transit-case-study.html#what-does-it-use",
    "href": "slides/07-transit-case-study.html#what-does-it-use",
    "title": "7 - Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "slides/07-transit-case-study.html#fit-the-required-candidate-models",
    "href": "slides/07-transit-case-study.html#fit-the-required-candidate-models",
    "title": "7 - Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res <- fit_members(chi_stack_res)"
  },
  {
    "objectID": "slides/07-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "slides/07-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "7 - Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist <- \n  chi_res %>% \n  extract_workflow_set_result(\"pca_cubist\") %>% \n  select_best()\n\ncubist_res <- \n  chi_res %>% \n  extract_workflow(\"pca_cubist\") %>% \n  finalize_workflow(best_cubist) %>% \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "slides/07-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "slides/07-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "7 - Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe don‚Äôt have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred <- \n  predict(chi_stack_res, chi_test) %>% \n  bind_cols(chi_test)"
  },
  {
    "objectID": "slides/07-transit-case-study.html#compare-the-results",
    "href": "slides/07-transit-case-study.html#compare-the-results",
    "title": "7 - Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#> # A tibble: 1 √ó 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 mae     standard       0.670 Preprocessor1_Model1\n\nstack_pred %>% mae(ridership, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard       0.689"
  },
  {
    "objectID": "slides/07-transit-case-study.html#plot-the-test-set",
    "href": "slides/07-transit-case-study.html#plot-the-test-set",
    "title": "7 - Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\ncubist_res %>% \n  collect_predictions() %>% \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/08-wrapping-up.html#your-turn",
    "href": "slides/08-wrapping-up.html#your-turn",
    "title": "8 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\n05:00"
  },
  {
    "objectID": "slides/08-wrapping-up.html#resources-to-keep-learning",
    "href": "slides/08-wrapping-up.html#resources-to-keep-learning",
    "title": "8 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Twitter and at the tidyverse blog for updates!\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/annotations.html",
    "href": "slides/annotations.html",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThere‚Äôs a lot that we want to tell you. We don‚Äôt want people to have to frantically scribble down things that we say that are not on the slides.\nWe‚Äôve added sections to this document with longer explanations and links to other resources.\n\n\n\nThis is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you don‚Äôt have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single ‚Äúresample‚Äù.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just ‚Äútesting‚Äù and ‚Äútraining‚Äù. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "slides/annotations.html#data-splitting-and-spending",
    "href": "slides/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nWhat does set.seed() do?\nWe‚Äôll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the ‚Äúseed‚Äù) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our ‚Äúrandom‚Äù numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n\nIf we don‚Äôt set the seed, R uses the clock time and the process ID to create a seed. This isn‚Äôt reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we don‚Äôt get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to ‚Äúspread the randomness around‚Äù. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "slides/annotations.html#what-is-wrong-with-this",
    "href": "slides/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.¬†you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesn‚Äôt work.\n\nThe big issue here is that you won‚Äôt be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in ‚ÄòSelection bias in gene extraction on the basis of microarray gene-expression data‚Äô. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "slides/annotations.html#where-are-the-fitted-models",
    "href": "slides/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we don‚Äôt keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "slides/annotations.html#the-final-fit",
    "href": "slides/annotations.html#the-final-fit",
    "title": "Annotations",
    "section": "The final fit",
    "text": "The final fit\nSince our data spending scheme created the resamples from the training set, last_fit() will use all of the training data to fit the final workflow.\nAs shown in the Whole Game slides, there is a slightly different scheme used when we have a validation set (instead of multiple resamples like 10-fold CV)."
  },
  {
    "objectID": "slides/annotations.html#using-a-workflow",
    "href": "slides/annotations.html#using-a-workflow",
    "title": "Annotations",
    "section": "Using a workflow",
    "text": "Using a workflow\nWhat‚Äôs going on with the\n\nprediction from a rank-deficient fit may be misleading\n\nwarnings?\nFor linear regression, a computation is used called matrix inversion. The matrix in question is called the ‚Äúmodel matrix‚Äù and it contains the predictor set for the training data.\nMatrix inversion can fail if two or more columns:\n\nare identical, or\nadd up to some other column.\n\nThese situations are called linear dependencies.\nWhen this happens, lm() is pretty tolerant. It does not fail but does not compute regression coefficients for a minimal number of predictors involved in the dependency (and issues the warning above).\nFor these data, there is this dependency:\nshooter_type_{level} = shooter_{level}\nHere is what is happening: since the player shooting only ever plays a single position, their indicators sum up (row-wide) to the same data as the sum of the shooter type indicators.\nIn other words, if you know the player‚Äôs name, you know their position too. This is a perfect redundency in the data.\nThe way to avoid this problem is to use step_lincomb(all_numeric_predictors()) in the recipe. This step removes the minimum number of columns to avoid the issue.\nJust in case you ever want to figure out what the specific issues are, this code might help:\n\n# Get the exact data set used to fit the model. For a recipe, that is:\nprocessed_data &lt;- \n  nhl_indicators %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, all_predictors())\n\n# Let's capture their names:\npred_names &lt;- names(processed_data)\n\n# caret has a function to determine the variables involved in each dependency\nissues &lt;- caret::findLinearCombos(processed_data)\n# Convert the column index to names:\nissues_vars &lt;- purrr::map(issues$linearCombos, ~ pred_names[.x])\n\n# caret proposes removing these columns to get rid of the issue. The choice\n# is pretty arbitrary: \nremoved_names &lt;- pred_names[issues$remove]\n\ntl;dr\nLinear regression detects some redundancies in the predictor set. We can ignore the warnings since lm() can deal with it or use step_lincomb() to avoid the warnings."
  },
  {
    "objectID": "slides/annotations.html#per-player-statistics",
    "href": "slides/annotations.html#per-player-statistics",
    "title": "Annotations",
    "section": "Per-player statistics",
    "text": "Per-player statistics\nThe effect encoding method essentially takes the effect of a variable, like player, and makes a data column for that effect. In our example, the ability of a player to have an on-goal shot is quantified by a model and then added as a data column to be used in the model.\nSuppose NHL rookie Max has a single shot in the data and it was on goal. If we used a naive estimate for Max‚Äôs effect, the model is being told that Max should have a 100% chance of being on goal. That‚Äôs a very poor estimate since it is from a single data point.\nContrast this with seasoned player Davis, who has taken 250 shots and 75% of these were on goal. Davis‚Äôs proportion is more predictive because it is estimated with better data (i.e., more total shots). Partial pooling leverages the entire data set and can borrow strength from all of the players. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a player‚Äôs data is of good quality, the partial pooling effect estimate is closer to the raw proportion. Max‚Äôs data is not great and is ‚Äúshrunk‚Äù towards the center of the overall on goal proportion. Since there is so little known about Max‚Äôs shot history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nIf the outcome were numeric, the effect would be the mean of the outcome per player. In this case, partial pooling is very similar to the James‚ÄìStein estimator: https://en.wikipedia.org/wiki/James‚ÄìStein_estimator"
  },
  {
    "objectID": "slides/annotations.html#player-effects",
    "href": "slides/annotations.html#player-effects",
    "title": "Annotations",
    "section": "Player effects",
    "text": "Player effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we aren‚Äôt overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for players with small sample sizes. It can‚Äôt correct for improper data usage or data leakage though."
  },
  {
    "objectID": "slides/annotations.html#angle",
    "href": "slides/annotations.html#angle",
    "title": "Annotations",
    "section": "Angle",
    "text": "Angle\nAbout geometry‚Ä¶\nThe coordinates for the rink are centered at (0, 0) and the goal lines are both 89 ft from center. The center of the goal on the left is at (-89, 0) and the right-hand goal is centered at (89, 0).\nFor angle to center of the goal mouth, the formula is\n\\[a = \\tan^{-1}\\left(\\frac{y}{x}\\right)\\] This is in radian units and we can convert to degrees using\n\\[a = \\frac{180}{\\pi}\\tan^{-1}\\left(\\frac{y}{x}\\right)\\] For the angle to the goal, we need to alter \\(x\\) and use x* = (89 - abs(coord_x)) instead."
  },
  {
    "objectID": "slides/annotations.html#update-parameter-ranges",
    "href": "slides/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %&gt;% \n  update(mtry = mtry(c(1, 100)))\n\nIn our case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) %&gt;% \n  step_ns(dis, deg_free = tune()) %&gt;% \n  tunable()\n\nns_tunable\n\n#&gt; # A tibble: 1 √ó 5\n#&gt;   name     call_info        source component component_id\n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;       \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_ns   ns_P1Tjg\n\nns_tunable$call_info\n\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  1 15"
  },
  {
    "objectID": "slides/annotations.html#boosted-tree-tuning-parameters",
    "href": "slides/annotations.html#boosted-tree-tuning-parameters",
    "title": "Annotations",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesn‚Äôt make any improvements for stop_iter iterations, training stops.\nHere‚Äôs an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. That‚Äôs not an argument to any function in the xgboost package.\nparsnip has its own wrapper around (xgboost::xgb.train()) called xgb_train(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the xgboost entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for xgboost)."
  },
  {
    "objectID": "slides/annotations.html#the-final-fit-to-the-nhl-data",
    "href": "slides/annotations.html#the-final-fit-to-the-nhl-data",
    "title": "Annotations",
    "section": "The final fit to the NHL data",
    "text": "The final fit to the NHL data\nRecall that last_fit() uses the objects produced by initial_split() to determine what data are used for the final model fit and which are used as the test set.\nFor the validation set, last_fit() will use the non-testing data to create the final model fit. This includes the training and validation set.\nThere is no agreement in the community on whether this is the best approach or if we should just use the training set. There are good arguments either way.\nIf you only want to use the training set for the final model, you can do this via:\n\ntraining_data &lt;- nhl_val$splits[[1]] %&gt;% analysis()\n\n# Use `fit()` to train the model on just the training set\nfinal_glm_spline_wflow &lt;- \n  glm_spline_wflow %&gt;% \n  fit(data = training_data)\n\n# Create test set predictions\ntest_set_pred &lt;- augment(final_glm_spline_wflow, nhl_test)\n\n# Setup and compute the test set metrics\ncls_metrics &lt;- metric_set(roc_auc, accuracy)\n\ntest_res &lt;- \n  test_set_pred %&gt;% \n  cls_metrics(on_goal, estimate = .pred_class, .pred_yes)\ntest_res"
  },
  {
    "objectID": "slides/annotations.html#explain-yourself",
    "href": "slides/annotations.html#explain-yourself",
    "title": "Annotations",
    "section": "Explain yourself",
    "text": "Explain yourself\nSome other resources:\n\nTMwR chapter Explaining Models and Predictions\nExplanatory Model Analysis book\nInterpretable Machine Learning book\nDefinitions, methods, and applications in interpretable machine learning"
  },
  {
    "objectID": "slides/annotations.html#a-tidymodels-explainer",
    "href": "slides/annotations.html#a-tidymodels-explainer",
    "title": "Annotations",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nFor our example, the angle was an original predictor. Recall that we made spline terms from this predictor, so there are derived features such as angle_ns_1 and so on.\nOriginal versus derived doesn‚Äôt affect local explainers since we are focused on a single prediction.\nFor global explainers, we should decide between:\n\nexplaining the overall affect of angle (lumping all its features into one importance score), or\nexplaining the effect of each term in the model (including angle_ns_1 and so on).\n\nThe choice depends on what you want. For example, if we have an original date predictor and make features for month and year, is it more informative to know if date is important (overall) or exactly how the date is important? You might want to look at it both ways."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease do not photograph people wearing red lanyards\nThere are gender neutral bathrooms near National Harbor rooms\nA meditation room is located at National Harbor 9 (8am - 5pm, Mon - Thurs)\nA lactation room is located at Potomac Dressing Room (8am - 5pm, Mon - Thurs)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies-1",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies-1",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the rstudio::conf code of conduct, which applies to all workshops: https://www.rstudio.com/conference/2022/2022-conf-code-of-conduct/\nCoC site has info on how to report a problem (in person, email, phone)\nYou are required to wear a mask that fully covers your mouth and nose at all times in all public spaces"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-you",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %>% or base R |> pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we",
    "title": "1 - Introduction",
    "section": "Who are we?",
    "text": "Who are we?\n\n\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\n\nJulia Silge\nDavid Robinson\nDavis Vaughan"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we-1",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we-1",
    "title": "1 - Introduction",
    "section": "Who are we?",
    "text": "Who are we?\n\n\n\nKelly Bodwin\nMichael Chow\nPritam Dalal\nMatt Dancho\nJon Harmon\n\n\n\nMike Mahoney\nEdgar Ruiz\nAsmae Toumi\nQiushi Yan\n\n\n\n\nMany thanks to Julie Jung, Alison Hill, and Desir√©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#asking-for-help",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nüü™ ‚ÄúI‚Äôm stuck and need help!‚Äù\n\n\nüü© ‚ÄúI finished the exercise‚Äù"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#section-2",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "üëÄ",
    "text": "üëÄ"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#plan-for-this-workshop",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nToday:\n\nYour data budget\nWhat makes a model\nEvaluating models\n\nTomorrow:\n\nFeature engineering\nTuning hyperparameters\nWrapping up!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#section-3",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors üëã\n\n Log in to RStudio Cloud here (free):\nbit.ly/tidymodels-rstudioconf-2022"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\n03:00\n\n\n\n\nthe ‚Äútwo cultures‚Äù\nmodel first vs.¬†data first\ninference vs.¬†prediction"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-tidymodels",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#> ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.0.0 ‚îÄ‚îÄ\n#> ‚úî broom        1.0.0     ‚úî rsample      1.0.0\n#> ‚úî dials        1.0.0     ‚úî tibble       3.1.8\n#> ‚úî dplyr        1.0.9     ‚úî tidyr        1.2.0\n#> ‚úî infer        1.0.2     ‚úî tune         1.0.0\n#> ‚úî modeldata    1.0.0     ‚úî workflows    1.0.0\n#> ‚úî parsnip      1.0.0     ‚úî workflowsets 1.0.0\n#> ‚úî purrr        0.3.4     ‚úî yardstick    1.0.0\n#> ‚úî recipes      1.0.1\n#> ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n#> ‚úñ purrr::discard() masks scales::discard()\n#> ‚úñ dplyr::filter()  masks stats::filter()\n#> ‚úñ dplyr::lag()     masks stats::lag()\n#> ‚úñ recipes::step()  masks stats::step()\n#> ‚Ä¢ Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#the-whole-game",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nTomorrow we will walk through a case study in detail to illustrate feature engineering and model tuning.\nToday we will walk through the analysis at a higher level to show the model development process as a whole and give you an introduction to the data set.\nThe data are from the NHL where we want to predict whether a shot was on-goal or not! üèí\nIt‚Äôs a good example to show how model development works."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#shots-on-goal",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#shots-on-goal",
    "title": "1 - Introduction",
    "section": "Shots on goal",
    "text": "Shots on goal"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#data-spending",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data spending",
    "text": "Data spending"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#a-first-model",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#a-first-model",
    "title": "1 - Introduction",
    "section": "A first model",
    "text": "A first model"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#starting-point-logistic-regression",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#starting-point-logistic-regression",
    "title": "1 - Introduction",
    "section": "Starting point: logistic regression",
    "text": "Starting point: logistic regression\n\nWe‚Äôll start by using basic logistic regression to predict our binary outcome.\nOur first model will have 16 simple predictor columns.\nOne initial question: there are 640 players taking shots.\nFor logistic regression, do we convert these to binary indicators (a.k.a. ‚Äúdummies‚Äù)?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#basic-features-inc-dummy-variables",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#basic-features-inc-dummy-variables",
    "title": "1 - Introduction",
    "section": "Basic features (inc dummy variables)",
    "text": "Basic features (inc dummy variables)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#different-player-encoding",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#different-player-encoding",
    "title": "1 - Introduction",
    "section": "Different player encoding",
    "text": "Different player encoding"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-about-location",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-about-location",
    "title": "1 - Introduction",
    "section": "What about location",
    "text": "What about location\nThe previous models used the x/y coordinates.\nAre there better ways to represent shot location?\nHow can we make location more usable for the model?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-angle",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-angle",
    "title": "1 - Introduction",
    "section": "Add shot angle?",
    "text": "Add shot angle?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-distance",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-distance",
    "title": "1 - Introduction",
    "section": "Add shot distance?",
    "text": "Add shot distance?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-behind-goal-line",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-behind-goal-line",
    "title": "1 - Introduction",
    "section": "Add shot behind goal line?",
    "text": "Add shot behind goal line?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "title": "1 - Introduction",
    "section": "Nonlinear terms for angle and distance",
    "text": "Nonlinear terms for angle and distance"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#try-another-model",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#try-another-model",
    "title": "1 - Introduction",
    "section": "Try another model",
    "text": "Try another model"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#switch-to-boosting-and-basic-features",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#switch-to-boosting-and-basic-features",
    "title": "1 - Introduction",
    "section": "Switch to boosting and basic features",
    "text": "Switch to boosting and basic features"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#boosting-with-location-features",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#boosting-with-location-features",
    "title": "1 - Introduction",
    "section": "Boosting with location features",
    "text": "Boosting with location features"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#choose-wisely",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#choose-wisely",
    "title": "1 - Introduction",
    "section": "Choose wisely‚Ä¶",
    "text": "Choose wisely‚Ä¶"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#finalize-and-verify",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#finalize-and-verify",
    "title": "1 - Introduction",
    "section": "Finalize and verify",
    "text": "Finalize and verify"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#and-so-on",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#and-so-on",
    "title": "1 - Introduction",
    "section": "‚Ä¶ and so on",
    "text": "‚Ä¶ and so on\nOnce we find an acceptable model and feature set, the process is to\n\nConfirm our results on the test set.\nDocument the data and model development process.\nDeploy, monitor, etc."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#lets-install-some-packages",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Let‚Äôs install some packages",
    "text": "Let‚Äôs install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\ninstall.packages(c(\"DALEXtra\", \"doParallel\", \"embed\", \"forcats\",\n                   \"lme4\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@v0.0.2\")\n\n\n\n Or log in to RStudio Cloud:\nbit.ly/tidymodels-rstudioconf-2022"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#our-versions",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\n\n\n\nbroom (1.0.0, CRAN), DALEX (2.4.0, CRAN), DALEXtra (2.2.0, CRAN), dials (1.0.0, CRAN), doParallel (1.0.17, CRAN), dplyr (1.0.9, CRAN), embed (1.0.0, CRAN), ggplot2 (3.3.6, CRAN), modeldata (1.0.0, CRAN), ongoal (0.0.2, Github (topepo/ongoal@02cd6b233), parsnip (1.0.0, CRAN), purrr (0.3.4, CRAN), ranger (0.13.1, CRAN), recipes (1.0.1, local), rpart (4.1.16, CRAN), rpart.plot (3.1.1, CRAN), rsample (1.0.0, CRAN), scales (1.2.0, CRAN), stacks (0.2.3, CRAN), tibble (3.1.8, CRAN), tidymodels (1.0.0, CRAN), tidyr (1.2.0, CRAN), tune (1.0.0, CRAN), vetiver (0.1.5, CRAN), workflows (1.0.0, CRAN), workflowsets (1.0.0, CRAN), xgboost (1.6.0.1, CRAN), and yardstick (1.0.0, CRAN)\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-1",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\nRed-eyed tree frog embryos can hatch earlier than their normal ~7 days if they detect potential predator threat!\nType ?stacks::tree_frogs to learn more about this dataset, including references.\nWe are using a slightly modified version from stacks.\n\n\nlibrary(tidymodels)\n\ndata(\"tree_frogs\", package = \"stacks\")\ntree_frogs <- tree_frogs %>%\n  mutate(t_o_d = factor(t_o_d),\n         age = age / 86400) %>%\n  filter(!is.na(latency)) %>%\n  select(-c(clutch, hatched))"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-2",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-2",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\n\n\nN = 572\nA numeric outcome, latency\n4 other variables\n\ntreatment, reflex, and t_o_d are nominal predictors\nage is a numeric predictor\n\n\n\n\n\n\n\n\nlatency: How long it took the frog to hatch after being stimulated - i.e.¬†after being poked by a blunt probe (in seconds).\ntreatment: Whether or not they got gentamicin, a compound that knocks out the embryo‚Äôs lateral line (a sensory organ).\nreflex: A measure of ear function (low, mid, full)\nt_o_d: Time that the stimulus was applied (morning, afternoon, night)\nage: Age at the time it was stimulated (in days)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-3",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-3",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\ntree_frogs\n#> # A tibble: 572 √ó 5\n#>    treatment  reflex   age t_o_d     latency\n#>    <chr>      <fct>  <dbl> <fct>       <dbl>\n#>  1 control    full    5.40 morning        22\n#>  2 control    low     4.18 night         360\n#>  3 control    full    4.65 afternoon     106\n#>  4 control    mid     4.14 night         180\n#>  5 control    full    4.6  afternoon      60\n#>  6 gentamicin full    5.36 morning        39\n#>  7 control    full    4.56 afternoon     214\n#>  8 control    full    5.43 morning        50\n#>  9 control    full    4.63 afternoon     224\n#> 10 control    full    5.40 morning        63\n#> # ‚Ä¶ with 562 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not üö´ use the test set during training."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs)\nfrog_split\n#> <Training/Testing/Total>\n#> <429/143/572>\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#accessing-the-data",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nfrog_train <- training(frog_split)\nfrog_test <- testing(frog_split)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#the-training-set",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nfrog_train\n#> # A tibble: 429 √ó 5\n#>    treatment  reflex   age t_o_d     latency\n#>    <chr>      <fct>  <dbl> <fct>       <dbl>\n#>  1 control    full    5.36 morning        36\n#>  2 gentamicin full    5.37 morning        72\n#>  3 gentamicin full    4.65 afternoon     141\n#>  4 control    full    5.42 morning        27\n#>  5 control    full    5.43 morning        27\n#>  6 gentamicin full    5.38 morning        73\n#>  7 gentamicin full    5.42 morning        68\n#>  8 gentamicin full    4.75 afternoon     124\n#>  9 control    full    5.00 night          62\n#> 10 control    full    5.39 morning        25\n#> # ‚Ä¶ with 419 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#the-test-set",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \n\nfrog_test\n#> # A tibble: 143 √ó 5\n#>    treatment  reflex   age t_o_d     latency\n#>    <chr>      <fct>  <dbl> <fct>       <dbl>\n#>  1 control    full    5.40 morning        22\n#>  2 control    low     4.18 night         360\n#>  3 control    full    4.63 afternoon     224\n#>  4 gentamicin full    4.75 afternoon     158\n#>  5 control    mid     4.22 night          91\n#>  6 gentamicin full    4.89 night         301\n#>  7 control    full    5.38 morning         2\n#>  8 control    full    4.80 afternoon      56\n#>  9 control    full    5.36 morning        11\n#> 10 control    full    5.40 morning        64\n#> # ‚Ä¶ with 133 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-4",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-4",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs, prop = 0.8)\nfrog_train <- training(frog_split)\nfrog_test <- testing(frog_split)\n\nnrow(frog_train)\n#> [1] 457\nnrow(frog_test)\n#> [1] 115"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "We will use this tomorrow"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the frog_train data on your own!\n\nWhat‚Äôs the distribution of the outcome, latency?\nWhat‚Äôs the distribution of numeric variables like age?\nHow does latency differ across the categorical variables?\n\n\n\n\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-3",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\n\nThis histogram brings up a concern. What if in our training set we get unlucky and sample few or none of these large values? That could mean that our model wouldn‚Äôt be able to predict such values. Let‚Äôs come back to that!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-4",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency, treatment, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-5",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "frog_train %>%\n  ggplot(aes(latency, reflex, fill = reflex)) +\n  geom_boxplot(alpha = 0.3, show.legend = FALSE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-6",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(age, latency, color = reflex)) +\n  geom_point(alpha = .8, size = 2)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-7",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "Stratified sampling would split within each quartile\n\nBased on our exploration, we realized that stratifying by latency might help get a consistent distribution. For instance, we‚Äôd include high and low latency in both the test and training"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#stratification",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#stratification",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nUse strata = latency\n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)\nfrog_split\n#> <Training/Testing/Total>\n#> <456/116/572>\n\n\nStratification often helps, with very little downside\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\n03:00\n\n\n\n\n\nlm for linear model\nglm for generalized linear model (e.g.¬†logistic regression)\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nlinear_reg()\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %>%\n  set_engine(\"glmnet\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: glmnet"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %>%\n  set_engine(\"stan\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: stan"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#> Decision Tree Model Specification (unknown)\n#> \n#> Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %>% \n  set_mode(\"regression\")\n#> Decision Tree Model Specification (regression)\n#> \n#> Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code so it creates a different model.\n\n\n\n05:00\n\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models we‚Äôll be using today",
    "text": "Models we‚Äôll be using today\n\nLinear regression\nDecision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-1",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-2",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\n\n\n\n\n\n\nOutcome modeled as linear combination of predictors:\n\n\\(\\mbox{latency} = \\beta_0 + \\beta_1\\cdot\\mbox{age} + \\epsilon\\)\n\nFind a line that minimizes the mean squared error (MSE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLinear regression\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\n\n\n\n\n\n\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering tomorrow!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your ‚Äúnew‚Äù data just doesn‚Äôt have an instance of that level)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\ntree_spec %>% \n  fit(latency ~ ., data = frog_train) \n#> parsnip model object\n#> \n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=4.947975 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 4.947975 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 4.664439 30  102190.20 147.83330  \n#>         28) age>=4.566638 22   53953.86 129.77270 *\n#>         29) age< 4.566638 8   21326.00 197.50000 *\n#>       15) age>=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow() %>%\n  add_formula(latency ~ .) %>%\n  add_model(tree_spec) %>%\n  fit(data = frog_train) \n#> ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=4.947975 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 4.947975 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 4.664439 30  102190.20 147.83330  \n#>         28) age>=4.566638 22   53953.86 129.77270 *\n#>         29) age< 4.566638 8   21326.00 197.50000 *\n#>       15) age>=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow(latency ~ ., tree_spec) %>% \n  fit(data = frog_train) \n#> ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=4.947975 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 4.947975 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 4.664439 30  102190.20 147.83330  \n#>         28) age>=4.566638 22   53953.86 129.77270 *\n#>         29) age< 4.566638 8   21326.00 197.50000 *\n#>       15) age>=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code so it uses a linear model.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\ntree_fit <-\n  workflow(latency ~ ., tree_spec) %>% \n  fit(data = frog_train)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nExtract the model engine object from your fitted linear workflow.\n‚ö†Ô∏è Never predict() with any extracted components!\n\n\n\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.¬†give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploying-a-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploying-a-model",
    "title": "3 - What makes a model?",
    "section": "Deploying a model ",
    "text": "Deploying a model \nHow do you use your new tree_fit model in production?\n\nlibrary(vetiver)\nv <- vetiver_model(tree_fit, \"frog_hatching\")\nv\n#> \n#> ‚îÄ‚îÄ frog_hatching ‚îÄ <butchered_workflow> model for deployment \n#> A rpart regression modeling workflow using 4 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploy-your-model-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploy-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new model tree_fit in production?\n\nlibrary(plumber)\npr() %>%\n  vetiver_api(v)\n#> # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#> # Use `pr_run()` on this object to start the API.\n#> ‚îú‚îÄ‚îÄ[queryString]\n#> ‚îú‚îÄ‚îÄ[body]\n#> ‚îú‚îÄ‚îÄ[cookieParser]\n#> ‚îú‚îÄ‚îÄ[sharedSecret]\n#> ‚îú‚îÄ‚îÄ/logo\n#> ‚îÇ  ‚îÇ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/vetiver\n#> ‚îú‚îÄ‚îÄ/ping (GET)\n#> ‚îî‚îÄ‚îÄ/predict (POST)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-6",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-6",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\n05:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\n\naugment(tree_fit, new_data = frog_test) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 rsq     standard       0.380\n#> 3 mae     standard      40.2\n\n\n\nRMSE: difference between the predicted and observed values ‚¨áÔ∏è\n\\(R^2\\): squared correlation between the predicted and observed values ‚¨ÜÔ∏è\nMAE: similar to RMSE, but mean absolute error ‚¨áÔ∏è"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        59.2"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %>%\n  group_by(reflex) %>%\n  rmse(latency, .pred)\n#> # A tibble: 3 √ó 4\n#>   reflex .metric .estimator .estimate\n#>   <fct>  <chr>   <chr>          <dbl>\n#> 1 low    rmse    standard        94.3\n#> 2 mid    rmse    standard       101. \n#> 3 full   rmse    standard        51.2"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\nfrog_metrics <- metric_set(rmse, msd)\naugment(tree_fit, new_data = frog_test) %>%\n  frog_metrics(latency, .pred)\n#> # A tibble: 2 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 msd     standard      -0.908"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è",
    "text": "Dangers of overfitting ‚ö†Ô∏è"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è",
    "text": "Dangers of overfitting ‚ö†Ô∏è"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\ntree_fit %>%\n  augment(frog_train)\n#> # A tibble: 456 √ó 6\n#>    treatment  reflex   age t_o_d     latency .pred\n#>    <chr>      <fct>  <dbl> <fct>       <dbl> <dbl>\n#>  1 control    full    5.42 morning        33  39.8\n#>  2 control    full    5.38 morning        19  66.7\n#>  3 control    full    5.38 morning         2  66.7\n#>  4 control    full    5.44 morning        39  39.8\n#>  5 control    full    5.41 morning        42  39.8\n#>  6 control    full    4.75 afternoon      20  59.8\n#>  7 control    full    4.95 night          31  83.1\n#>  8 control    full    5.42 morning        21  39.8\n#>  9 gentamicin full    5.39 morning        30  64.6\n#> 10 control    full    4.55 afternoon      43 174. \n#> # ‚Ä¶ with 446 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n\nWe call this ‚Äúresubstitution‚Äù or ‚Äúrepredicting the training set‚Äù"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4\n\nWe call this a ‚Äúresubstitution estimate‚Äù"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4\n\n\n\ntree_fit %>%\n  augment(frog_test) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        59.2\n\n\n\n\n‚ö†Ô∏è Remember that we‚Äôre demonstrating overfitting\n\n\n‚ö†Ô∏è Don‚Äôt use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and metrics() to compute a regression metric like mae().\nCompute the metrics for both training and testing data.\nNotice the evidence of overfitting! ‚ö†Ô∏è\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ‚ö†Ô∏è ",
    "text": "Dangers of overfitting ‚ö†Ô∏è \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      49.4  \n#> 2 rsq     standard       0.494\n#> 3 mae     standard      33.4\n\n\n\ntree_fit %>%\n  augment(frog_test) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 rsq     standard       0.380\n#> 3 mae     standard      40.2\n\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train) # v = 10 is default\n#> #  10-fold cross-validation \n#> # A tibble: 10 √ó 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [410/46]> Fold01\n#>  2 <split [410/46]> Fold02\n#>  3 <split [410/46]> Fold03\n#>  4 <split [410/46]> Fold04\n#>  5 <split [410/46]> Fold05\n#>  6 <split [410/46]> Fold06\n#>  7 <split [411/45]> Fold07\n#>  8 <split [411/45]> Fold08\n#>  9 <split [411/45]> Fold09\n#> 10 <split [411/45]> Fold10"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nfrog_folds <- vfold_cv(frog_train)\nfrog_folds$splits[1:3]\n#> [[1]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n#> \n#> [[2]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n#> \n#> [[3]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, v = 5)\n#> #  5-fold cross-validation \n#> # A tibble: 5 √ó 2\n#>   splits           id   \n#>   <list>           <chr>\n#> 1 <split [364/92]> Fold1\n#> 2 <split [365/91]> Fold2\n#> 3 <split [365/91]> Fold3\n#> 4 <split [365/91]> Fold4\n#> 5 <split [365/91]> Fold5"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, strata = latency)\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [408/48]> Fold01\n#>  2 <split [408/48]> Fold02\n#>  3 <split [408/48]> Fold03\n#>  4 <split [409/47]> Fold04\n#>  5 <split [411/45]> Fold05\n#>  6 <split [412/44]> Fold06\n#>  7 <split [412/44]> Fold07\n#>  8 <split [412/44]> Fold08\n#>  9 <split [412/44]> Fold09\n#> 10 <split [412/44]> Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-6",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWe‚Äôll use this setup:\n\nset.seed(123)\nfrog_folds <- vfold_cv(frog_train, v = 10, strata = latency)\nfrog_folds\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [408/48]> Fold01\n#>  2 <split [408/48]> Fold02\n#>  3 <split [408/48]> Fold03\n#>  4 <split [409/47]> Fold04\n#>  5 <split [411/45]> Fold05\n#>  6 <split [412/44]> Fold06\n#>  7 <split [412/44]> Fold07\n#>  8 <split [412/44]> Fold08\n#>  9 <split [412/44]> Fold09\n#> 10 <split [412/44]> Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\ntree_res <- fit_resamples(tree_wflow, frog_folds)\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 4\n#>    splits           id     .metrics         .notes          \n#>    <list>           <chr>  <list>           <list>          \n#>  1 <split [408/48]> Fold01 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  2 <split [408/48]> Fold02 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  3 <split [408/48]> Fold03 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  4 <split [409/47]> Fold04 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  5 <split [411/45]> Fold05 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  6 <split [412/44]> Fold06 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  7 <split [412/44]> Fold07 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  8 <split [412/44]> Fold08 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#>  9 <split [412/44]> Fold09 <tibble [2 √ó 4]> <tibble [0 √ó 3]>\n#> 10 <split [412/44]> Fold10 <tibble [2 √ó 4]> <tibble [0 √ó 3]>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res %>%\n  collect_metrics()\n#> # A tibble: 2 √ó 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   59.6      10  2.31   Preprocessor1_Model1\n#> 2 rsq     standard    0.305    10  0.0342 Preprocessor1_Model1\n\n\nWe can reliably measure performance using only the training data üéâ"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#comparing-metrics",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\n\n\n\ntree_res %>%\n  collect_metrics() %>% \n  select(.metric, mean, n)\n#> # A tibble: 2 √ó 3\n#>   .metric   mean     n\n#>   <chr>    <dbl> <int>\n#> 1 rmse    59.6      10\n#> 2 rsq      0.305    10\n\n\nThe RMSE previously was\n\n49.36 for the training set\n59.16 for test set\n\n\n\n\nRemember that:\n‚ö†Ô∏è the training set gives you overly optimistic metrics\n‚ö†Ô∏è the test set is precious"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_frog <- control_resamples(save_pred = TRUE)\ntree_res <- fit_resamples(tree_wflow, frog_folds, control = ctrl_frog)\n\ntree_preds <- collect_predictions(tree_res)\ntree_preds\n#> # A tibble: 456 √ó 5\n#>    id     .pred  .row latency .config             \n#>    <chr>  <dbl> <int>   <dbl> <chr>               \n#>  1 Fold01  39.6     1      33 Preprocessor1_Model1\n#>  2 Fold01  72.1     3       2 Preprocessor1_Model1\n#>  3 Fold01  63.8     9      30 Preprocessor1_Model1\n#>  4 Fold01  72.1    13      46 Preprocessor1_Model1\n#>  5 Fold01  43.3    28      11 Preprocessor1_Model1\n#>  6 Fold01  61.7    35      41 Preprocessor1_Model1\n#>  7 Fold01  39.6    51      43 Preprocessor1_Model1\n#>  8 Fold01 134.     70      20 Preprocessor1_Model1\n#>  9 Fold01  70.6    74      21 Preprocessor1_Model1\n#> 10 Fold01  39.6   106      14 Preprocessor1_Model1\n#> # ‚Ä¶ with 446 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-3",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "tree_preds %>% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 5\n#>    splits           id     .metrics         .notes           .predictions     \n#>    <list>           <chr>  <list>           <list>           <list>           \n#>  1 <split [408/48]> Fold01 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  2 <split [408/48]> Fold02 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  3 <split [408/48]> Fold03 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  4 <split [409/47]> Fold04 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [47 √ó 4]>\n#>  5 <split [411/45]> Fold05 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [45 √ó 4]>\n#>  6 <split [412/44]> Fold06 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  7 <split [412/44]> Fold07 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  8 <split [412/44]> Fold08 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  9 <split [412/44]> Fold09 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#> 10 <split [412/44]> Fold10 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n\n\nüóëÔ∏è"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(frog_train)\n#> # Bootstrap sampling \n#> # A tibble: 25 √ó 2\n#>    splits            id         \n#>    <list>            <chr>      \n#>  1 <split [456/163]> Bootstrap01\n#>  2 <split [456/166]> Bootstrap02\n#>  3 <split [456/173]> Bootstrap03\n#>  4 <split [456/177]> Bootstrap04\n#>  5 <split [456/166]> Bootstrap05\n#>  6 <split [456/163]> Bootstrap06\n#>  7 <split [456/164]> Bootstrap07\n#>  8 <split [456/165]> Bootstrap08\n#>  9 <split [456/170]> Bootstrap09\n#> 10 <split [456/177]> Bootstrap10\n#> # ‚Ä¶ with 15 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nbootstrap folds (change times from the default)\nvalidation set (use the reference guide to find the function)\n\nDon‚Äôt forget to set a seed when you resample!\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-2",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(322)\nbootstraps(frog_train, times = 10)\n#> # Bootstrap sampling \n#> # A tibble: 10 √ó 2\n#>    splits            id         \n#>    <list>            <chr>      \n#>  1 <split [456/173]> Bootstrap01\n#>  2 <split [456/168]> Bootstrap02\n#>  3 <split [456/170]> Bootstrap03\n#>  4 <split [456/164]> Bootstrap04\n#>  5 <split [456/176]> Bootstrap05\n#>  6 <split [456/156]> Bootstrap06\n#>  7 <split [456/166]> Bootstrap07\n#>  8 <split [456/168]> Bootstrap08\n#>  9 <split [456/167]> Bootstrap09\n#> 10 <split [456/170]> Bootstrap10"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#validation-set",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nvalidation_split(frog_train, strata = latency)\n#> # Validation Set Split (0.75/0.25)  using stratification \n#> # A tibble: 1 √ó 2\n#>   splits            id        \n#>   <list>            <chr>     \n#> 1 <split [340/116]> validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#random-forest-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ",
    "text": "Random forest üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\nEnsemble many decision tree models\nAll the trees vote! üó≥Ô∏è\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this tomorrow!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec <- rand_forest(trees = 1000, mode = \"regression\")\nrf_spec\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow <- workflow(latency ~ ., rf_spec)\nrf_wflow\n#> ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\nplot true vs.¬†predicted values\n\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_frog <- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res <- fit_resamples(rf_wflow, frog_folds, control = ctrl_frog)\ncollect_metrics(rf_res)\n#> # A tibble: 2 √ó 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   55.9      10  1.71   Preprocessor1_Model1\n#> 2 rsq     standard    0.370    10  0.0306 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-5",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(rf_res) %>% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "4 - Evaluating models",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec))\n#> # A workflow set/tibble: 2 √ó 4\n#>   wflow_id              info             option    result    \n#>   <chr>                 <list>           <list>    <list>    \n#> 1 formula_decision_tree <tibble [1 √ó 4]> <opts[0]> <list [0]>\n#> 2 formula_rand_forest   <tibble [1 √ó 4]> <opts[0]> <list [0]>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %>%\n  workflow_map(\"fit_resamples\", resamples = frog_folds)\n#> # A workflow set/tibble: 2 √ó 4\n#>   wflow_id              info             option    result   \n#>   <chr>                 <list>           <list>    <list>   \n#> 1 formula_decision_tree <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n#> 2 formula_rand_forest   <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %>%\n  workflow_map(\"fit_resamples\", resamples = frog_folds) %>%\n  rank_results()\n#> # A tibble: 4 √ó 9\n#>   wflow_id              .config .metric   mean std_err     n prepr‚Ä¶¬π model  rank\n#>   <chr>                 <chr>   <chr>    <dbl>   <dbl> <int> <chr>   <chr> <int>\n#> 1 formula_rand_forest   Prepro‚Ä¶ rmse    55.8    1.71      10 formula rand‚Ä¶     1\n#> 2 formula_rand_forest   Prepro‚Ä¶ rsq      0.371  0.0301    10 formula rand‚Ä¶     1\n#> 3 formula_decision_tree Prepro‚Ä¶ rmse    59.6    2.31      10 formula deci‚Ä¶     2\n#> 4 formula_decision_tree Prepro‚Ä¶ rsq      0.305  0.0342    10 formula deci‚Ä¶     2\n#> # ‚Ä¶ with abbreviated variable name ¬π‚Äãpreprocessor\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#the-final-fit",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLet‚Äôs fit the model on the training set and verify our performance using the test set.\n\nWe‚Äôve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# frog_split has train + test info\nfinal_fit <- last_fit(rf_wflow, frog_split) \n\nfinal_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 √ó 6\n#>   splits            id               .metrics .notes   .predictions .workflow \n#>   <list>            <chr>            <list>   <list>   <list>       <list>    \n#> 1 <split [456/116]> train/test split <tibble> <tibble> <tibble>     <workflow>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#> # A tibble: 2 √ó 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 rmse    standard      57.1   Preprocessor1_Model1\n#> 2 rsq     standard       0.420 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#> # A tibble: 116 √ó 5\n#>    id               .pred  .row latency .config             \n#>    <chr>            <dbl> <int>   <dbl> <chr>               \n#>  1 train/test split  43.5     1      22 Preprocessor1_Model1\n#>  2 train/test split 104.      3     106 Preprocessor1_Model1\n#>  3 train/test split  76.2     6      39 Preprocessor1_Model1\n#>  4 train/test split  42.4     8      50 Preprocessor1_Model1\n#>  5 train/test split  43.5    10      63 Preprocessor1_Model1\n#>  6 train/test split  43.1    14      25 Preprocessor1_Model1\n#>  7 train/test split  51.5    16      48 Preprocessor1_Model1\n#>  8 train/test split 160.     17      91 Preprocessor1_Model1\n#>  9 train/test split  50.9    32      11 Preprocessor1_Model1\n#> 10 train/test split 171.     33     109 Preprocessor1_Model1\n#> # ‚Ä¶ with 106 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nThese are predictions for the test set"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-6",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(final_fit) %>%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, col = \"deeppink4\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#> ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> latency ~ .\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  1000 \n#> Sample size:                      456 \n#> Number of independent variables:  4 \n#> Mtry:                             2 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       3124.583 \n#> R squared (OOB):                  0.3531813\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nEnd of the day discussion!\nWhich model do you think you would decide to use?\nWhat surprised you the most?\nWhat is one thing you are looking forward to for tomorrow?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-3",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-4",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-5",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nIteratively add candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-1",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nstack_ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-2",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nDefine candidate members\n\nStart out with a linear regression:\n\nlr_res <- \n  # define model spec\n  linear_reg() %>%\n  set_mode(\"regression\") %>%\n  # add to workflow\n  workflow(preprocessor = latency ~ .) %>%\n  # fit to resamples\n  fit_resamples(frog_folds, control = stack_ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-3",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlr_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 5\n#>    splits           id     .metrics         .notes           .predictions     \n#>    <list>           <chr>  <list>           <list>           <list>           \n#>  1 <split [408/48]> Fold01 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  2 <split [408/48]> Fold02 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  3 <split [408/48]> Fold03 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  4 <split [409/47]> Fold04 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [47 √ó 4]>\n#>  5 <split [411/45]> Fold05 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [45 √ó 4]>\n#>  6 <split [412/44]> Fold06 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  7 <split [412/44]> Fold07 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  8 <split [412/44]> Fold08 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  9 <split [412/44]> Fold09 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#> 10 <split [412/44]> Fold10 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-4",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \nThen, a random forest:\n\nrf_res <- \n  # define model spec\n  rand_forest() %>%\n  set_mode(\"regression\") %>%\n  # add to workflow\n  workflow(preprocessor = latency ~ .) %>%\n  # fit to resamples\n  fit_resamples(frog_folds, control = stack_ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-5",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nrf_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 5\n#>    splits           id     .metrics         .notes           .predictions     \n#>    <list>           <chr>  <list>           <list>           <list>           \n#>  1 <split [408/48]> Fold01 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  2 <split [408/48]> Fold02 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  3 <split [408/48]> Fold03 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [48 √ó 4]>\n#>  4 <split [409/47]> Fold04 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [47 √ó 4]>\n#>  5 <split [411/45]> Fold05 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [45 √ó 4]>\n#>  6 <split [412/44]> Fold06 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  7 <split [412/44]> Fold07 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  8 <split [412/44]> Fold08 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#>  9 <split [412/44]> Fold09 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>\n#> 10 <split [412/44]> Fold10 <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [44 √ó 4]>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-6",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nInitialize a data stack object\n\n\n\nfrog_st <- stacks()\n\nfrog_st\n#> # A data stack with 0 model definitions and 0 candidate members."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-7",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-7",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nIteratively add candidate ensemble members to the data stack\n\n\nfrog_st <- frog_st %>%\n  add_candidates(lr_res) %>%\n  add_candidates(rf_res)\n\nfrog_st\n#> # A data stack with 2 model definitions and 2 candidate members:\n#> #   lr_res: 1 model configuration\n#> #   rf_res: 1 model configuration\n#> # Outcome: latency (numeric)\n\nTomorrow we‚Äôll discuss tuning parameters where there are different configurations of models (e.g.¬†10 different variations of the random forest model).\nThese configurations can greatly improve the performance of the stacking ensemble."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-8",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-8",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nEvaluate how to combine their predictions\n\n\nfrog_st_res <- frog_st %>%\n  blend_predictions()\n\nfrog_st_res\n#> # A tibble: 2 √ó 3\n#>   member     type        weight\n#>   <chr>      <chr>        <dbl>\n#> 1 rf_res_1_1 rand_forest  0.635\n#> 2 lr_res_1_1 linear_reg   0.344"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-9",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-9",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nFit candidate ensemble members with non-zero stacking coefficients\n\n\nfrog_st_res <- frog_st_res %>%\n  fit_members()\n\nfrog_st_res\n#> # A tibble: 2 √ó 3\n#>   member     type        weight\n#>   <chr>      <chr>        <dbl>\n#> 1 rf_res_1_1 rand_forest  0.635\n#> 2 lr_res_1_1 linear_reg   0.344"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-10",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-10",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nPredict on new data!\n\n\n\nfrog_test %>%\n  select(latency) %>%\n  bind_cols(\n    predict(frog_st_res, frog_test)\n  ) %>%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, \n              col = \"deeppink4\", \n              size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#working-with-our-predictors",
    "title": "5 - Feature engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\n\nThe model requires them in a different format (e.g.¬†dummy variables for lm()).\nThe model needs certain data qualities (e.g.¬†same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a ‚Äúfeature engineering‚Äù).\n\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-feature-engineering",
    "title": "5 - Feature engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPCA feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#example-dates",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#example-dates",
    "title": "5 - Feature engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n\nIt can be re-engineered as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#general-definitions",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#general-definitions",
    "title": "5 - Feature engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\nIn a little bit, we‚Äôll see successful (and unsuccessful) feature engineering methods for our example data.\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "The NHL data üèí",
    "text": "The NHL data üèí\n\nFrom Pittsburgh Penguins games, 12,147 shots\nData from the 2015-2016 season\n\n\nLet‚Äôs predict whether a shot is on-goal (a goal or blocked by goaltender) or not."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#case-study",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#case-study",
    "title": "5 - Feature engineering",
    "section": "Case study",
    "text": "Case study\n\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nglimpse(season_2015)\n#> Rows: 12,147\n#> Columns: 17\n#> $ on_goal           <fct> yes, no, no, yes, no, no, yes, no, yes, no, no, no, ‚Ä¶\n#> $ period            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n#> $ period_type       <fct> regular, regular, regular, regular, regular, regular‚Ä¶\n#> $ coord_x           <dbl> -53, 68, -42, -77, -67, 55, 77, 62, 59, 76, 44, 62, ‚Ä¶\n#> $ coord_y           <dbl> -18, -12, -18, 9, -5, -12, 13, 14, -5, -6, 7, -2, -2‚Ä¶\n#> $ game_time         <dbl> 0.300000, 0.900000, 1.250000, 1.783333, 2.050000, 3.‚Ä¶\n#> $ strength          <fct> even, even, even, even, even, even, even, even, even‚Ä¶\n#> $ player            <fct> victor_hedman, evgeni_malkin, jason_garrison, ondrej‚Ä¶\n#> $ player_diff       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#> $ offense_team      <fct> TBL, PIT, TBL, TBL, TBL, PIT, PIT, PIT, PIT, PIT, PI‚Ä¶\n#> $ defense_team      <fct> PIT, TBL, PIT, PIT, PIT, TBL, TBL, TBL, TBL, TBL, TB‚Ä¶\n#> $ offense_goal_diff <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#> $ game_type         <fct> playoff, playoff, playoff, playoff, playoff, playoff‚Ä¶\n#> $ position          <fct> defenseman, center, defenseman, left_wing, defensema‚Ä¶\n#> $ dow               <fct> Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sa‚Ä¶\n#> $ month             <fct> May, May, May, May, May, May, May, May, May, May, Ma‚Ä¶\n#> $ year              <dbl> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016‚Ä¶"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#data-splitting-strategy",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#data-splitting-strategy",
    "title": "5 - Feature engineering",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#why-a-validation-set",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#why-a-validation-set",
    "title": "5 - Feature engineering",
    "section": "Why a validation set?",
    "text": "Why a validation set?\nRecall that resampling gives us performance measures without using the test set.\nIt‚Äôs important to get good resampling statistics (e.g.¬†\\(R^2\\)).\n\nThat usually means having enough data to estimate performance.\n\nWhen you have ‚Äúa lot‚Äù of data, a validation set can be an efficient way to do this."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#splitting-the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#splitting-the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "Splitting the NHL data ",
    "text": "Splitting the NHL data \n\nset.seed(23)\nnhl_split <- initial_split(season_2015, prop = 3/4)\nnhl_split\n#> <Training/Testing/Total>\n#> <9110/3037/12147>\n\nnhl_train_and_val <- training(nhl_split)\nnhl_test  <- testing(nhl_split)\n\n## not testing\nnrow(nhl_train_and_val)\n#> [1] 9110\n \n## testing\nnrow(nhl_test)\n#> [1] 3037"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#validation-split",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#validation-split",
    "title": "5 - Feature engineering",
    "section": "Validation split ",
    "text": "Validation split \nSince there are a lot of observations, we‚Äôll use a validation set:\n\nset.seed(234)\nnhl_val <- validation_split(nhl_train_and_val, prop = 0.80)\nnhl_val\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 2\n#>   splits              id        \n#>   <list>              <chr>     \n#> 1 <split [7288/1822]> validation\n\n\nRemember that a validation split is a type of resample."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nLet‚Äôs explore the training set data.\nUse the function plot_nhl_shots() for nice spatial plots of the data.\n\n\n\nnhl_train <- analysis(nhl_val$splits[[1]])\n\nset.seed(100)\nnhl_train %>% \n  sample_n(200) %>%\n  plot_nhl_shots(emphasis = position)\n\n\n\n\n\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "5 - Feature engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train)\n\n\n\nThe recipe() function assigns columns to roles of ‚Äúoutcome‚Äù or ‚Äúpredictor‚Äù using the formula"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe-1",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(nhl_rec)\n#> # A tibble: 17 √ó 4\n#>    variable          type    role      source  \n#>    <chr>             <chr>   <chr>     <chr>   \n#>  1 period            numeric predictor original\n#>  2 period_type       nominal predictor original\n#>  3 coord_x           numeric predictor original\n#>  4 coord_y           numeric predictor original\n#>  5 game_time         numeric predictor original\n#>  6 strength          nominal predictor original\n#>  7 player            nominal predictor original\n#>  8 player_diff       numeric predictor original\n#>  9 offense_team      nominal predictor original\n#> 10 defense_team      nominal predictor original\n#> 11 offense_goal_diff numeric predictor original\n#> 12 game_type         nominal predictor original\n#> 13 position          nominal predictor original\n#> 14 dow               nominal predictor original\n#> 15 month             nominal predictor original\n#> 16 year              numeric predictor original\n#> 17 on_goal           nominal outcome   original"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#create-indicator-variables",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#create-indicator-variables",
    "title": "5 - Feature engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#filter-out-constant-columns",
    "title": "5 - Feature engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of ‚Äúpredictor‚Äù"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#normalization",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#normalization",
    "title": "5 - Feature engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#reduce-correlation",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#reduce-correlation",
    "title": "5 - Feature engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extraction‚Ä¶"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-1",
    "title": "5 - Feature engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  embed::step_umap(all_numeric_predictors(), outcome = on_goal)\n\n\nA fancy machine learning supervised dimension reduction technique‚Ä¶\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-2",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_ns(coord_y, coord_x, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-1",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the on-goal data to :\n\ncreate one-hot indicator variables\nremove zero-variance variables\n\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#minimal-recipe",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#minimal-recipe",
    "title": "5 - Feature engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nnhl_indicators <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#using-a-workflow",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#using-a-workflow",
    "title": "5 - Feature engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nnhl_glm_wflow <-\n  workflow() %>%\n  add_recipe(nhl_indicators) %>%\n  add_model(logistic_reg())\n \nctrl <- control_resamples(save_pred = TRUE)\nnhl_glm_res <-\n  nhl_glm_wflow %>%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#> # A tibble: 2 √ó 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.555     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.558     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-2",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#holdout-predictions",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#holdout-predictions",
    "title": "5 - Feature engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nglm_val_pred <- collect_predictions(nhl_glm_res)\nglm_val_pred %>% slice(1:7)\n#> # A tibble: 7 √ó 7\n#>   id         .pred_yes .pred_no  .row .pred_class on_goal .config             \n#>   <chr>          <dbl>    <dbl> <int> <fct>       <fct>   <chr>               \n#> 1 validation     0.198 8.02e- 1    10 no          no      Preprocessor1_Model1\n#> 2 validation     0.264 7.36e- 1    17 no          yes     Preprocessor1_Model1\n#> 3 validation     0.189 8.11e- 1    23 no          no      Preprocessor1_Model1\n#> 4 validation     1.00  8.39e-11    40 yes         yes     Preprocessor1_Model1\n#> 5 validation     0.322 6.78e- 1    41 no          yes     Preprocessor1_Model1\n#> 6 validation     1.00  8.39e-11    46 yes         no      Preprocessor1_Model1\n#> 7 validation     0.354 6.46e- 1    55 no          no      Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#two-class-data-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#two-class-data-1",
    "title": "5 - Feature engineering",
    "section": "Two class data",
    "text": "Two class data\nThese definitions assume that we know the threshold for converting ‚Äúsoft‚Äù probability predictions into ‚Äúhard‚Äù class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity ‚¨áÔ∏è, specificity ‚¨ÜÔ∏è\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity ‚¨ÜÔ∏è, specificity ‚¨áÔ∏è"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves",
    "title": "5 - Feature engineering",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 üíØ\nROC AUC = 1/2 üò¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves-1",
    "title": "5 - Feature engineering",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\nroc_curve_points <- glm_val_pred %>% roc_curve(truth = on_goal, estimate = .pred_yes)\nroc_curve_points %>% slice(1, 50, 100)\n#> # A tibble: 3 √ó 3\n#>   .threshold specificity sensitivity\n#>        <dbl>       <dbl>       <dbl>\n#> 1   -Inf          0            1    \n#> 2      0.139      0.0303       0.977\n#> 3      0.272      0.0642       0.955\n\nglm_val_pred %>% roc_auc(truth = on_goal, estimate = .pred_yes)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.558"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curve-plot",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curve-plot",
    "title": "5 - Feature engineering",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\nautoplot(roc_curve_points)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-3",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-4",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nWhat data are being used for this ROC curve plot?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "title": "5 - Feature engineering",
    "section": "What do we do with the player data? üèí",
    "text": "What do we do with the player data? üèí\nThere are 597 unique player values in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables üò≥\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the player column with the estimated effect of that predictor\n\n\n\nLet‚Äôs use an effect encoding."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-an-effect-encoding",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-an-effect-encoding",
    "title": "5 - Feature engineering",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitative‚Äôs predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#> # A tibble: 7 √ó 3\n#>   on_goal player            .row\n#>   <fct>   <fct>            <int>\n#> 1 yes     brian_dumoulin       1\n#> 2 yes     patric_hornqvist     2\n#> 3 yes     nikita_nesterov      3\n#> 4 yes     jack_eichel          4\n#> 5 yes     justin_williams      5\n#> 6 yes     seth_jones           6\n#> 7 yes     kris_letang          7\n\n\nData after:\n\nafter\n#> # A tibble: 7 √ó 3\n#>   on_goal  player  .row\n#>   <fct>     <dbl> <int>\n#> 1 yes     -0.114      1\n#> 2 yes      0.631      2\n#> 3 yes      0.142      3\n#> 4 yes      0.220      4\n#> 5 yes      0.248      5\n#> 6 yes      0.0733     6\n#> 7 yes      0.0774     7\n\n\n\nThe player column is replaced with the log-odds of being on goal.\n\nAs a reminder:\n\\[\\text{log-odds} = log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)\\]\nwhere \\(\\hat{p}\\) is the on goal rate estimate.\nFor logistic regression, this is what the predictors are modeling. The log-odds are more likely to be linear with the outcome."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#per-player-statistics",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#per-player-statistics",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics",
    "text": "Per-player statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these rates use partial pooling.\nPooling borrows strength across players and shrinks extreme values (e.g.¬†zero or one) towards the mean for players with very few shots.\nThe embed package has recipe steps for effect encodings.\n\n\n\n\nPartial pooling gives better estimates for players with fewer shots by shrinking the estimate to the overall on-goal rate (55.2%)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#partial-pooling",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#partial-pooling",
    "title": "5 - Feature engineering",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#player-effects",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#player-effects",
    "title": "5 - Feature engineering",
    "section": "Player effects  ",
    "text": "Player effects  \n\nlibrary(embed)\n\nnhl_effect_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#recipes-are-estimated",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#recipes-are-estimated",
    "title": "5 - Feature engineering",
    "section": "Recipes are estimated ",
    "text": "Recipes are estimated \nPreprocessing steps in a recipe use the training set to compute quantities.\n\nWhat kind of quantities are computed for preprocessing?\n\nLevels of a factor\nWhether a column has zero variance\nNormalization\nFeature extraction\nEffect encodings\n\n\n\nWhen a recipe is part of a workflow, this estimation occurs when fit() is called."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#effect-encoding-results",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#effect-encoding-results",
    "title": "5 - Feature engineering",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nnhl_effect_wflow <-\n  nhl_glm_wflow %>%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res <-\n  nhl_effect_wflow %>%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_effect_res)\n#> # A tibble: 2 √ó 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.540     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.551     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#angle",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#angle",
    "title": "5 - Feature engineering",
    "section": "Angle",
    "text": "Angle\n\nnhl_angle_rec <-\n  nhl_indicators %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi))\n  )\n\n\n\n\n\n\n\n\n\n\n\nNote the danger of using step_mutate() ‚Äì easy to have data leakage\ncoord_x is ‚Äúdistance from goal‚Äù. We subtract it from 89 to get the distance from the center of the ice. The abs() calls account for the fact that the goals might be on either side of (0, 0). The rest of it is the formula for going from (x, y) to angle in degrees."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#distance",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#distance",
    "title": "5 - Feature engineering",
    "section": "Distance",
    "text": "Distance\n\nnhl_distance_rec <-\n  nhl_angle_rec %>%\n  step_mutate(\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance)\n  )"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#behind-goal-line",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#behind-goal-line",
    "title": "5 - Feature engineering",
    "section": "Behind goal line",
    "text": "Behind goal line\n\nnhl_behind_rec <-\n  nhl_distance_rec %>%\n  step_mutate(\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  )"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#fit-different-recipes",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#fit-different-recipes",
    "title": "5 - Feature engineering",
    "section": "Fit different recipes    ",
    "text": "Fit different recipes    \nA workflow set can cross models and/or preprocessors and then resample them en masse.\n\nset.seed(9)\n\nnhl_glm_set_res <-\n  workflow_set(\n    list(`1_dummy` = nhl_indicators, `2_angle` = nhl_angle_rec, \n         `3_dist` = nhl_distance_rec, `4_bgl` = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %>%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val, verbose = TRUE, control = ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-5",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a workflow set with 2 or 3 recipes.\n(Consider using recipes we‚Äôve already created.)\nUse workflow_map() to resample the workflow set.\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %>%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point(size = 3) +\n  labs(y = NULL, x = \"ROC AUC (validation set)\")"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes-1",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#debugging-a-recipe",
    "title": "5 - Feature engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.¬†encoded_players) can be estimated manually with a function called prep(). It is analogous to fit().\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#more-on-recipes",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#more-on-recipes",
    "title": "5 - Feature engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters.\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search üí† which tests a pre-defined set of candidate values\nIterative search üåÄ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet‚Äôs take our previous recipe and add a few changes:\n\nglm_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  ) %>%\n  step_rm(coord_x, coord_y) %>%\n  step_zv(all_predictors()) %>%\n  step_ns(angle, deg_free = tune(\"angle\")) %>%\n  step_ns(distance, deg_free = tune(\"distance\")) %>%\n  step_normalize(all_numeric_predictors())\n\n\nLet‚Äôs tune() our spline terms!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet‚Äôs take our previous recipe and add a few changes:\n\nglm_spline_wflow <-\n  workflow() %>%\n  add_model(logistic_reg()) %>%\n  add_recipe(glm_rec)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#section",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#section",
    "title": "6 - Tuning Hyperparameters",
    "section": "",
    "text": "Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.\nMore spline terms = more ‚Äúwiggly‚Äù, i.e.¬†flexibly model a nonlinear relationship\nHow many spline terms? This is called degrees of freedom\n2 and 5 look like they underfit; 20 and 100 look like they overfit"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "title": "6 - Tuning Hyperparameters",
    "section": "Splines and nonlinear relationships",
    "text": "Splines and nonlinear relationships\n\n\n\n\n\n\n\n\n\n\nOur hockey data exhibits nonlinear relationships\nWe can model nonlinearity like this via a model (later this afternoon) or feature engineering\nHow do we decide how ‚Äúwiggly‚Äù or flexible to make our spline features? TUNING"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-search",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nParameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nglm_spline_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>       angle deg_free nparam[+]\n#>    distance deg_free nparam[+]\n\n\nA parameter set can be updated (e.g.¬†to change the ranges)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 23 √ó 2\n#>    angle distance\n#>    <int>    <int>\n#>  1    12        4\n#>  2    15        8\n#>  3     6       14\n#>  4    10        5\n#>  5    12       12\n#>  6     7        8\n#>  7    14        3\n#>  8    14       13\n#>  9    11       12\n#> 10     8       11\n#> # ‚Ä¶ with 13 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n\n\n\n\nA space-filling design like this tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 25)\n\ngrid\n#> # A tibble: 225 √ó 2\n#>    angle distance\n#>    <int>    <int>\n#>  1     1        1\n#>  2     2        1\n#>  3     3        1\n#>  4     4        1\n#>  5     5        1\n#>  6     6        1\n#>  7     7        1\n#>  8     8        1\n#>  9     9        1\n#> 10    10        1\n#> # ‚Ä¶ with 215 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the deg_free parameters only have a range of 1->15."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "6 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(angle = spline_degree(c(2L, 20L)),\n         distance = spline_degree(c(2L, 20L))) %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 24 √ó 2\n#>    angle distance\n#>    <int>    <int>\n#>  1    16        6\n#>  2    20       11\n#>  3     8       19\n#>  4    14        7\n#>  5    16       17\n#>  6    10       11\n#>  7    19        5\n#>  8    18       17\n#>  9    15       16\n#> 10    11       15\n#> # ‚Ä¶ with 14 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nEven though angle is a deg_free parameter in step_ns(), we don‚Äôt use the dials deg_free() object here. We have a special spline_degree() function that has better defaults for splines."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-results",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %>% \n  ggplot(aes(angle, distance)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#spline-grid-search",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#spline-grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Spline grid search   ",
    "text": "Spline grid search   \n\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglm_spline_res <-\n  glm_spline_wflow %>%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\n\nglm_spline_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes            .predictions         \n#>   <list>              <chr>      <list>            <list>            <list>               \n#> 1 <split [7288/1822]> validation <tibble [48 √ó 6]> <tibble [24 √ó 3]> <tibble [43,728 √ó 8]>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x24: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTune our glm_wflow.\nWhat happens if you don‚Äôt supply a grid argument to tune_grid()?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-results",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(glm_spline_res)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res)\n#> # A tibble: 48 √ó 8\n#>    angle distance .metric  .estimator  mean     n std_err .config              \n#>    <int>    <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1    16        6 accuracy binary     0.610     1      NA Preprocessor01_Model1\n#>  2    16        6 roc_auc  binary     0.649     1      NA Preprocessor01_Model1\n#>  3    20       11 accuracy binary     0.613     1      NA Preprocessor02_Model1\n#>  4    20       11 roc_auc  binary     0.649     1      NA Preprocessor02_Model1\n#>  5     8       19 accuracy binary     0.619     1      NA Preprocessor03_Model1\n#>  6     8       19 roc_auc  binary     0.652     1      NA Preprocessor03_Model1\n#>  7    14        7 accuracy binary     0.610     1      NA Preprocessor04_Model1\n#>  8    14        7 roc_auc  binary     0.650     1      NA Preprocessor04_Model1\n#>  9    16       17 accuracy binary     0.617     1      NA Preprocessor05_Model1\n#> 10    16       17 roc_auc  binary     0.649     1      NA Preprocessor05_Model1\n#> # ‚Ä¶ with 38 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#> # A tibble: 48 √ó 7\n#>    id         angle distance .metric  .estimator .estimate .config              \n#>    <chr>      <int>    <int> <chr>    <chr>          <dbl> <chr>                \n#>  1 validation    16        6 accuracy binary         0.610 Preprocessor01_Model1\n#>  2 validation    16        6 roc_auc  binary         0.649 Preprocessor01_Model1\n#>  3 validation    20       11 accuracy binary         0.613 Preprocessor02_Model1\n#>  4 validation    20       11 roc_auc  binary         0.649 Preprocessor02_Model1\n#>  5 validation     8       19 accuracy binary         0.619 Preprocessor03_Model1\n#>  6 validation     8       19 roc_auc  binary         0.652 Preprocessor03_Model1\n#>  7 validation    14        7 accuracy binary         0.610 Preprocessor04_Model1\n#>  8 validation    14        7 roc_auc  binary         0.650 Preprocessor04_Model1\n#>  9 validation    16       17 accuracy binary         0.617 Preprocessor05_Model1\n#> 10 validation    16       17 roc_auc  binary         0.649 Preprocessor05_Model1\n#> # ‚Ä¶ with 38 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 8\n#>   angle distance .metric .estimator  mean     n std_err .config              \n#>   <int>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1     5        8 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n#> 2     6        9 roc_auc binary     0.653     1      NA Preprocessor17_Model1\n#> 3     3       15 roc_auc binary     0.653     1      NA Preprocessor13_Model1\n#> 4     3       13 roc_auc binary     0.652     1      NA Preprocessor11_Model1\n#> 5     5       19 roc_auc binary     0.652     1      NA Preprocessor24_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\n\n\nThis best result has:\n\nlow-degree spline for angle (less ‚Äúwiggly‚Äù, less complex)\nhigher-degree spline for distance (more ‚Äúwiggly‚Äù, more complex)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTry an alternative selection strategy.\nRead the docs for select_by_pct_loss().\nTry choosing a model that has a simpler (less ‚Äúwiggly‚Äù) relationship for distance.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\nselect_by_pct_loss(glm_spline_res, distance, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 10\n#>   angle distance .metric .estimator  mean     n std_err .config               .best .loss\n#>   <int>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 <dbl> <dbl>\n#> 1    13        4 roc_auc binary     0.646     1      NA Preprocessor20_Model1 0.653 0.984"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ",
    "text": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\nEnsemble many decision tree models\n\n\n\nReview how a decision tree model works:\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#single-decision-tree",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#single-decision-tree",
    "title": "6 - Tuning Hyperparameters",
    "section": "Single decision tree",
    "text": "Single decision tree"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ",
    "text": "Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\nBoosting methods fit a sequence of tree-based models.\n\n\nEach tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\nThis is like gradient-based steepest ascent methods from calculus."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nMost modern boosting methods have a lot of tuning parameters!\n\n\nFor tree growth and pruning (min_n, max_depth, etc)\nFor boosting (trees, stop_iter, learn_rate)\n\n\n\nWe‚Äôll use early stopping to stop boosting when a few iterations produce consecutively worse results."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "title": "6 - Tuning Hyperparameters",
    "section": "Comparing tree ensembles",
    "text": "Comparing tree ensembles\n\n\nRandom forest\n\nIndependent trees\nBootstrapped data\nNo pruning\n1000‚Äôs of trees\n\n\nBoosting\n\nDependent trees\nDifferent case weights\nTune tree parameters\nFar fewer trees\n\n\n\nThe general consensus for tree-based models is, in terms of performance: boosting > random forest > bagging > single trees."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-code",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-code",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree code",
    "text": "Boosted tree code\n\nxgb_spec <-\n  boost_tree(\n    trees = 500, min_n = tune(), stop_iter = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %>%\n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\", validation = 1/10) # <- for better early stopping\n\nxgb_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_lencode_mixed(player, outcome = vars(on_goal)) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nxgb_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(xgb_rec)\n\n\nvalidation is an argument to parsnip::xgb_train(), not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to xgb.train(watchlist = list(validation = data)).\nSee translate(xgb_spec) to see where it is passed to parsnip::xgb_train()."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate your boosted tree workflow.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models don‚Äôt depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores <- parallel::detectCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\n\n\n\n\n\n\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning ",
    "text": "Tuning \nThis will take some time to run ‚è≥\n\nset.seed(9)\n\nxgb_res <-\n  xgb_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 15, control = ctrl) # automatic grid now!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-4",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nStart tuning the boosted tree model!\nWe won‚Äôt wait for everyone‚Äôs tuning to finish, but take this time to get it started before we move on.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nxgb_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes           .predictions          \n#>   <list>              <chr>      <list>            <list>           <list>                \n#> 1 <split [7288/1822]> validation <tibble [30 √ó 9]> <tibble [0 √ó 3]> <tibble [27,330 √ó 11]>"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-3",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nautoplot(xgb_res)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#again-with-the-location-features",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#again-with-the-location-features",
    "title": "6 - Tuning Hyperparameters",
    "section": "Again with the location features",
    "text": "Again with the location features\n\ncoord_rec <- \n  xgb_rec %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  ) %>% \n  step_rm(coord_x, coord_y)\n\nxgb_coord_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res <-\n  xgb_coord_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 20, control = ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "title": "6 - Tuning Hyperparameters",
    "section": "Did the machine figure it out?",
    "text": "Did the machine figure it out?\n\nshow_best(xgb_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 11\n#>   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#>   <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    19          2    0.311         1.46e- 4        14 roc_auc binary     0.633     1      NA Preprocessor1_Model12\n#> 2    11          3    0.0255        5.61e- 7        19 roc_auc binary     0.628     1      NA Preprocessor1_Model05\n#> 3    27          5    0.0120        6.04e- 6        12 roc_auc binary     0.627     1      NA Preprocessor1_Model07\n#> 4    25         14    0.0379        3.62e- 5         8 roc_auc binary     0.626     1      NA Preprocessor1_Model13\n#> 5    31         11    0.00585       1.02e-10         7 roc_auc binary     0.625     1      NA Preprocessor1_Model08\n\nshow_best(xgb_coord_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 11\n#>   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#>   <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    30         13     0.0578  0.00000000141        18 roc_auc binary     0.648     1      NA Preprocessor1_Model07\n#> 2    39         12     0.0803  0.000411             10 roc_auc binary     0.643     1      NA Preprocessor1_Model12\n#> 3    14          2     0.146   0.00244              19 roc_auc binary     0.642     1      NA Preprocessor1_Model14\n#> 4    26         15     0.0365  2.51                 17 roc_auc binary     0.642     1      NA Preprocessor1_Model11\n#> 5    35          5     0.101   0.0000000784         13 roc_auc binary     0.641     1      NA Preprocessor1_Model17"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#compare-models",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#compare-models",
    "title": "6 - Tuning Hyperparameters",
    "section": "Compare models",
    "text": "Compare models\nBest logistic regression results:\n\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n\n\nBest boosting results:\n\nxgb_coord_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.648     1      NA Preprocessor1_Model07"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-5",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCan you get better ROC results with xgboost?\nTry increasing learn_rate beyond the original range.\n\n\n\n20:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#updating-the-workflow",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#updating-the-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Updating the workflow  ",
    "text": "Updating the workflow  \n\nbest_auc <- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#> # A tibble: 1 √ó 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\n\nglm_spline_wflow <-\n  glm_spline_wflow %>% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#> ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> 8 Recipe Steps\n#> \n#> ‚Ä¢ step_lencode_mixed()\n#> ‚Ä¢ step_dummy()\n#> ‚Ä¢ step_mutate()\n#> ‚Ä¢ step_rm()\n#> ‚Ä¢ step_zv()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_normalize()\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "title": "6 - Tuning Hyperparameters",
    "section": "The final fit to the NHL data  ",
    "text": "The final fit to the NHL data  \n\ntest_res <- \n  glm_spline_wflow %>% \n  last_fit(split = nhl_split)\n\ntest_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 √ó 6\n#>   splits              id               .metrics         .notes           .predictions         .workflow \n#>   <list>              <chr>            <list>           <list>           <list>               <list>    \n#> 1 <split [9110/3037]> train/test split <tibble [2 √ó 4]> <tibble [1 √ó 3]> <tibble [3,037 √ó 6]> <workflow>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n\n\nRemember that last_fit() fits one time with the combined training and validation set, then evaluates one time with the testing set."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-6",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-6",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nFinalize your workflow with the best parameters.\nCreate a final fit.\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "title": "6 - Tuning Hyperparameters",
    "section": "Estimates of ROC AUC ",
    "text": "Estimates of ROC AUC \nValidation results from tuning:\n\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, mean, n, std_err)\n#> # A tibble: 1 √ó 4\n#>   .metric  mean     n std_err\n#>   <chr>   <dbl> <int>   <dbl>\n#> 1 roc_auc 0.653     1      NA\n\n\nTest set results:\n\ntest_res %>% collect_metrics()\n#> # A tibble: 2 √ó 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.616 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.656 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#final-fitted-workflow",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#final-fitted-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Final fitted workflow",
    "text": "Final fitted workflow\nExtract the final fitted workflow, fit using the training set:\n\nfinal_glm_spline_wflow <- \n  test_res %>% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#> # A tibble: 3 √ó 1\n#>   .pred_class\n#>   <fct>      \n#> 1 no         \n#> 2 yes        \n#> 3 no"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#next-steps",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#next-steps",
    "title": "6 - Tuning Hyperparameters",
    "section": "Next steps",
    "text": "Next steps\n\nDocument the model.\n\n\n\nDeploy the model.\n\n\n\n\nCreate an applicability domain model to help monitor our data over time.\n\n\n\n\nUse explainers to characterize the model and the predictions."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-yourself",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-yourself",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain yourself",
    "text": "Explain yourself\nThere are two categories of model explanations, global and local.\n\n\nGlobal model explanations provide an overall understanding aggregated over a whole set of observations.\nLocal model explanations provide information about a prediction for a single observation.\n\n\n\nYou can also build global model explanations by aggregating local model explanations."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "title": "6 - Tuning Hyperparameters",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nWe can build explainers using:\n\noriginal, basic predictors\nderived features\n\n\nlibrary(DALEXtra)\n\nglm_explainer <- explain_tidymodels(\n  final_glm_spline_wflow,\n  data = dplyr::select(nhl_train, -on_goal),\n  # DALEX required an integer for factors:\n  y = as.integer(nhl_train$on_goal),\n  verbose = FALSE\n)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates\nWith our explainer, let‚Äôs create partial dependence profiles:\n\nset.seed(123)\npdp_coord_x <- model_profile(\n  glm_explainer,\n  variables = \"coord_x\",\n  N = 500,\n  groups = \"position\"\n)\n\n\nYou can use the default plot() method or create your own visualization."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-7",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-7",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate an explainer for our glm model.\nTry grouping by another variable, like game_type or dow.\n\n\n\n05:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/07-wrapping-up.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/07-wrapping-up.html#your-turn",
    "title": "7 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/07-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2022-07-RStudio-conf/07-wrapping-up.html#resources-to-keep-learning",
    "title": "7 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Twitter and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html",
    "href": "archive/2022-07-RStudio-conf/annotations.html",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThere‚Äôs a lot that we want to tell you. We don‚Äôt want people to have to frantically scribble down things that we say that are not on the slides.\nWe‚Äôve added sections to this document with longer explanations and links to other resources.\n\n\n\nThis is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you don‚Äôt have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single ‚Äúresample‚Äù.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just ‚Äútesting‚Äù and ‚Äútraining‚Äù. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#data-splitting-and-spending",
    "href": "archive/2022-07-RStudio-conf/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nWhat does set.seed() do?\nWe‚Äôll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the ‚Äúseed‚Äù) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our ‚Äúrandom‚Äù numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n\n#> [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n\n#> [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n\n#> [1] 0.2655087 0.3721239 0.5728534\n\n\nIf we don‚Äôt set the seed, R uses the clock time and the process ID to create a seed. This isn‚Äôt reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we don‚Äôt get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to ‚Äúspread the randomness around‚Äù. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n\n#> set.seed(9725)\n#> set.seed(8462)\n#> set.seed(4050)\n#> set.seed(8789)\n#> set.seed(1301)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#what-is-wrong-with-this",
    "href": "archive/2022-07-RStudio-conf/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.¬†you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesn‚Äôt work.\n\nThe big issue here is that you won‚Äôt be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in ‚ÄòSelection bias in gene extraction on the basis of microarray gene-expression data‚Äô. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#where-are-the-fitted-models",
    "href": "archive/2022-07-RStudio-conf/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we don‚Äôt keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit",
    "href": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit",
    "title": "Annotations",
    "section": "The final fit",
    "text": "The final fit\nSince our data spending scheme created the resamples from the training set, last_fit() will use all of the training data to fit the final workflow.\nAs shown in the Whole Game slides, there is a slightly different scheme used when we have a validation set (instead of multiple resamples like 10-fold CV)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#per-player-statistics",
    "href": "archive/2022-07-RStudio-conf/annotations.html#per-player-statistics",
    "title": "Annotations",
    "section": "Per-player statistics",
    "text": "Per-player statistics\nThe effect encoding method essentially takes the effect of a variable, like player, and makes a data column for that effect. In our example, the ability of a player to have an on-goal shot is quantified by a model and then added as a data column to be used in the model.\nSuppose NHL rookie Max has a single shot in the data and it was on goal. If we used a naive estimate for Max‚Äôs effect, the model is being told that Max should have a 100% chance of being on goal. That‚Äôs a very poor estimate since it is from a single data point.\nContrast this with seasoned player Davis, who has taken 250 shots and 75% of these were on goal. Davis‚Äôs proportion is more predictive because it is estimated with better data (i.e., more total shots). Partial pooling leverages the entire data set and can borrow strength from all of the players. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a player‚Äôs data is of good quality, the partial pooling effect estimate is closer to the raw proportion. Max‚Äôs data is not great and is ‚Äúshrunk‚Äù towards the center of the overall on goal proportion. Since there is so little known about Max‚Äôs shot history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nIf the outcome were numeric, the effect would be the mean of the outcome per player. In this case, partial pooling is very similar to the James‚ÄìStein estimator: https://en.wikipedia.org/wiki/James‚ÄìStein_estimator"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#player-effects",
    "href": "archive/2022-07-RStudio-conf/annotations.html#player-effects",
    "title": "Annotations",
    "section": "Player effects",
    "text": "Player effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we aren‚Äôt overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for players with small sample sizes. It can‚Äôt correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#angle",
    "href": "archive/2022-07-RStudio-conf/annotations.html#angle",
    "title": "Annotations",
    "section": "Angle",
    "text": "Angle\nAbout these formulas‚Ä¶\nThe coordinates for the rink are centered at (0, 0) and the goal lines are both 89 ft from center. The center of the goal on the left is at (-89, 0) and the right-hand goal is centered at (89, 0).\nFor the distance to center ice, the formula is\n\\[d_{center} = \\sqrt{x^2 + y^2}\\]\nWe want distance to the goal line(s), so we first use x* = (89 - abs(coord_x)) to make the side of the rink irrelevant and then compute\n\\[d_{goal} = \\sqrt{x^{*2} + y^2}\\]\nIn the code, we log the distance value to help its distribution become more symmetric.\nFor angle to center, the formula is\n\\[a = \\tan^{-1}\\left(\\frac{y}{x}\\right)\\]\nThis is in radian units and we can convert to degrees using\n\\[a = \\frac{180}{\\pi}\\tan^{-1}\\left(\\frac{y}{x}\\right)\\]\nFor the angle to the goal, we need to alter \\(x\\) and \\(y\\) again. We‚Äôll use \\(x^*\\) from above and also use the absolute value of \\(y\\) so that the degrees range from 0 to 180."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#update-parameter-ranges",
    "href": "archive/2022-07-RStudio-conf/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %>% \n  update(mtry = mtry(c(1, 100)))\n\nIn our case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable <- \n  recipe(mpg ~ ., data = mtcars) %>% \n  step_ns(dis, deg_free = tune()) %>% \n  tunable()\n\nns_tunable\n\n#> # A tibble: 1 √ó 5\n#>   name     call_info        source component component_id\n#>   <chr>    <list>           <chr>  <chr>     <chr>       \n#> 1 deg_free <named list [3]> recipe step_ns   ns_P1Tjg\n\nns_tunable$call_info\n\n#> [[1]]\n#> [[1]]$pkg\n#> [1] \"dials\"\n#> \n#> [[1]]$fun\n#> [1] \"spline_degree\"\n#> \n#> [[1]]$range\n#> [1]  1 15"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#spline-grid-search",
    "href": "archive/2022-07-RStudio-conf/annotations.html#spline-grid-search",
    "title": "Annotations",
    "section": "Spline grid search",
    "text": "Spline grid search\nWhat‚Äôs going on with the\n\nprediction from a rank-deficient fit may be misleading\n\nwarnings?\nFor linear regression, a computation is used called matrix inversion. The matrix in question is called the ‚Äúmodel matrix‚Äù and it contains the predictor set for the training data.\nMatrix inversion can fail if two or more columns:\n\nare identical, or\nadd up to some other column.\n\nThese situations are called linear dependencies.\nWhen this happens, lm() is pretty tolerant. It does not fail but does not compute regression coefficients for a minimal number of predictors involved in the dependency (and issues the warning above).\nFor these data, there are three dependencies between:\n\ndefense_team_PIT and offense_team_PIT\nstrength_short_handed, player_diff, and strength_power_play\nyear, month_Oct, month_Nov, and month_Dec\n\nThe first one is easy to explain. For each row, when one these two PIT column has a one, the other must have a zero. The linear regression intercept is represented in the model matrix as a column of all ones. The dependency is\n(Intercept) = defense_team_PIT + offense_team_PIT\nThe way to avoid this problem is to use step_lincomb(all_numeric_predictors()) in the recipe. This step removes the minimum number of columns to avoid the issue.\ntl;dr\nLinear regression detects some redundancies in the predictor set. We can ignore the warnings since lm() can deal with it or use step_lincomb() to avoid the warnings."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-tuning-parameters",
    "title": "Annotations",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesn‚Äôt make any improvements for stop_iter iterations, training stops.\nHere‚Äôs an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-code",
    "href": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-code",
    "title": "Annotations",
    "section": "Boosted tree code",
    "text": "Boosted tree code\nWe set an engine argument called validation here. That‚Äôs not an argument to any function in the xgboost package.\nparsnip has its own wrapper around (xgboost::xgb.train()) called xgb_train(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the xgboost entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for xgboost)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit-to-the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit-to-the-nhl-data",
    "title": "Annotations",
    "section": "The final fit to the NHL data",
    "text": "The final fit to the NHL data\nRecall that last_fit() uses the objects produced by initial_split() to determine what data are used for the final model fit and which are used as the test set.\nFor the validation set, last_fit() will use the non-testing data to create the final model fit. This includes the training and validation set.\nThere is no agreement in the community on whether this is the best approach or if we should just use the training set. There are good arguments either way.\nIf you only want to use the training set for the final model, you can do this via:\n\ntraining_data <- nhl_val$splits[[1]] %>% analysis()\n\n# Use `fit()` to train the model on just the training set\nfinal_glm_spline_wflow <- \n  glm_spline_wflow %>% \n  fit(data = training_data)\n\n# Create test set predictions\ntest_set_pred <- augment(final_glm_spline_wflow, nhl_test)\n\n# Setup and compute the test set metrics\ncls_metrics <- metric_set(roc_auc, accuracy)\n\ntest_res <- \n  test_set_pred %>% \n  cls_metrics(on_goal, estimate = .pred_class, .pred_yes)\ntest_res"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#explain-yourself",
    "href": "archive/2022-07-RStudio-conf/annotations.html#explain-yourself",
    "title": "Annotations",
    "section": "Explain yourself",
    "text": "Explain yourself\nSome other resources:\n\nTMwR chapter Explaining Models and Predictions\nExplanatory Model Analysis book\nInterpretable Machine Learning book\nDefinitions, methods, and applications in interpretable machine learning"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#a-tidymodels-explainer",
    "href": "archive/2022-07-RStudio-conf/annotations.html#a-tidymodels-explainer",
    "title": "Annotations",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nFor our example, the angle was an original predictor. Recall that we made spline terms from this predictor, so there are derived features such as angle_ns_1 and so on.\nOriginal versus derived doesn‚Äôt affect local explainers since we are focused on a single prediction.\nFor global explainers, we should decide between:\n\nexplaining the overall affect of angle (lumping all its features into one importance score), or\nexplaining the effect of each term in the model (including angle_ns_1 and so on).\n\nThe choice depends on what you want. For example, if we have an original date predictor and make features for month and year, is it more informative to know if date is important (overall) or exactly how the date is important? You might want to look at it both ways."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html",
    "href": "archive/2022-07-RStudio-conf/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels presented at the 2022 RStudio conference. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, we‚Äôll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#is-this-workshop-for-me",
    "href": "archive/2022-07-RStudio-conf/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %>% and/or native pipe |>\nYou are familiar with functions from dplyr, tidyr, and ggplot2\nYou can read data into R, transform and reshape data, and make a wide variety of graphs\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#preparation",
    "href": "archive/2022-07-RStudio-conf/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download\nThe following R packages, which you can install from the R console:\n\n\ninstall.packages(c(\"DALEXtra\", \"doParallel\", \"embed\", \"forcats\",\n                   \"lme4\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@v0.0.2\")"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#slides",
    "href": "archive/2022-07-RStudio-conf/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participants‚Äô convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nDay One\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n\n\n\nDay Two\n\n05: Feature engineering\n06: Tuning hyperparameters\n07: Wrapping up\n\nThere‚Äôs also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#code",
    "href": "archive/2022-07-RStudio-conf/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Don‚Äôt worry if you haven‚Äôt used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#acknowledgments",
    "href": "archive/2022-07-RStudio-conf/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#reuse-and-licensing",
    "href": "archive/2022-07-RStudio-conf/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.¬†not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "slides/Advanced-01-tuning-hyperparameters.html#hotel-data",
    "href": "slides/Advanced-01-tuning-hyperparameters.html#hotel-data",
    "title": "6 - Tuning Hyperparameters",
    "section": "Hotel Data",
    "text": "Hotel Data\nWe‚Äôll use a version of the hotel data. We‚Äôll use a version where we try to predict the cost of a room.\nThe data are in the modeldatatoo package. We‚Äôll sample down the data and refactor some columns:\n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "slides/Advanced-01-tuning-hyperparameters.html#your-turn",
    "href": "slides/Advanced-01-tuning-hyperparameters.html#your-turn",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\nLet‚Äôs take some time and investigate the training data. The outcome is avg_price_per_room.\nAre there any characteristic of the data that are interesting?\n\n\n\n‚àí+\n10:00"
  },
  {
    "objectID": "slides/Advanced-01-tuning-hyperparameters.html#data-spending",
    "href": "slides/Advanced-01-tuning-hyperparameters.html#data-spending",
    "title": "6 - Tuning Hyperparameters",
    "section": "Data Spending",
    "text": "Data Spending\nLet‚Äôs split the data into a training set (75%) and testing set (25%):\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)"
  },
  {
    "objectID": "slides/Advanced-01-tuning-hyperparameters.html#feature-engineering",
    "href": "slides/Advanced-01-tuning-hyperparameters.html#feature-engineering",
    "title": "6 - Tuning Hyperparameters",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nHow should we represent our predictors? There are:\n\nCategorical predictors, some with many levels\nSome skewed numeric predictors\nInteractions?\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, we‚Äôll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#hotel-data",
    "href": "slides/advanced-01-tuning-hyperparameters.html#hotel-data",
    "title": "1 - Tuning Hyperparameters",
    "section": "Hotel Data",
    "text": "Hotel Data\nWe‚Äôll use a version of the hotel data. We‚Äôll use a version where we try to predict the cost of a room.\nThe data are in the modeldatatoo package. We‚Äôll sample down the data and refactor some columns:\n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#data-spending",
    "href": "slides/advanced-01-tuning-hyperparameters.html#data-spending",
    "title": "1 - Tuning Hyperparameters",
    "section": "Data Spending",
    "text": "Data Spending\nLet‚Äôs split the data into a training set (75%) and testing set (25%):\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#your-turn",
    "href": "slides/advanced-01-tuning-hyperparameters.html#your-turn",
    "title": "1 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\nLet‚Äôs take some time and investigate the training data. The outcome is avg_price_per_room.\nAre there any characteristic of the data that are interesting?\n\n\n\n‚àí+\n10:00"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#feature-engineering",
    "href": "slides/advanced-01-tuning-hyperparameters.html#feature-engineering",
    "title": "1 - Tuning Hyperparameters",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nHow should we represent our predictors? There are:\n\nCategorical predictors, some with many levels\nSome skewed numeric predictor(s)\nInteractions?\n\n\nLet‚Äôs start a basic recipe:\n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_YeoJohnson(lead_time)\n\n\n\nWe‚Äôll add to this for different types of models."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#tuning-parameters",
    "href": "slides/advanced-01-tuning-hyperparameters.html#tuning-parameters",
    "title": "1 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "slides/advanced-01-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "1 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search üí† which tests a pre-defined set of candidate values\nIterative search üåÄ which suggests/estimates new values of candidate parameters to evaluate\n\nWe‚Äôll focus on grid search for a while."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#a-baseline-model",
    "href": "slides/advanced-01-tuning-hyperparameters.html#a-baseline-model",
    "title": "1 - Tuning Hyperparameters",
    "section": "A Baseline Model",
    "text": "A Baseline Model\nA simple linear model with main effects\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_tr, strata = avg_price_per_room)\n\nbase_rec &lt;- \n  hotel_rec %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\nbase_wflow &lt;- workflow(base_rec, linear_reg())"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#a-baseline-model-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#a-baseline-model-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "A Baseline Model",
    "text": "A Baseline Model\nWe‚Äôll try to optimize the mean absolute error:\n\nreg_metrics &lt;- metric_set(mae, rsq)\nctrl_rs &lt;- control_resamples(save_pred = TRUE)\n\nbase_res &lt;- base_wflow %&gt;% fit_resamples(hotel_rs, metrics = reg_metrics)\n\ncollect_metrics(base_res)\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.3      10 0.199   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.874    10 0.00400 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#boosted-trees",
    "href": "slides/advanced-01-tuning-hyperparameters.html#boosted-trees",
    "title": "1 - Tuning Hyperparameters",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nThese are popular ensemble methods that build a sequence of tree models.\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble.\nWe‚Äôll focus on the popular lightgbm implementation."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "slides/advanced-01-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "1 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\n\nThe number of predictors randomly sampled at each split (aka mtry in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\nThe number of trees (trees in \\([1, \\infty]\\), but usually up to thousands)\nThe number of samples needed to further split (min_n in \\([1, n]\\)).\nThe rate that each tree adapts form previous iterations (learn_rate in \\((0, \\infty]\\), usual maximum is 0.1).\nThe number of iterations of boosting where no improvement was shown (early_stop)"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nTBH it is usually not difficult to optimize these models.\nOften, there are multiple candidate tuning parameter combinations that have very good results.\nTo demonstrate simple concepts, we‚Äôll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate (\\(10^{-5}\\) to \\(10^{-1}\\))."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "href": "slides/advanced-01-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "title": "1 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters   ",
    "text": "Boosted Tree Tuning Parameters   \nWe‚Äôll need to load the bonsai package. This has the information needed to use lightgbm\n\nlibrary(bonsai)\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#grid-search",
    "href": "slides/advanced-01-tuning-hyperparameters.html#grid-search",
    "title": "1 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#create-a-grid",
    "href": "slides/advanced-01-tuning-hyperparameters.html#create-a-grid",
    "title": "1 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nlgbm_wflow %&gt;% \n  extract_parameter_set_dials()\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;  identifier       type    object\n#&gt;       trees      trees nparam[+]\n#&gt;  learn_rate learn_rate nparam[+]\n\n# Individual functions: \ntrees()\n#&gt; # Trees (quantitative)\n#&gt; Range: [1, 2000]\nlearn_rate()\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, -1]\n\n\nA parameter set can be updated (e.g.¬†to change the ranges)."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#create-a-grid-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#create-a-grid-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 √ó 2\n#&gt;    trees learn_rate\n#&gt;    &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1   510   4.40e- 6\n#&gt;  2    67   3.59e- 3\n#&gt;  3  1013   2.85e- 7\n#&gt;  4  1401   3.70e- 8\n#&gt;  5   453   1.44e-10\n#&gt;  6   768   5.78e- 5\n#&gt;  7  1226   5.57e- 2\n#&gt;  8  1564   7.02e- 3\n#&gt;  9   933   1.05e- 2\n#&gt; 10  1353   1.17e- 7\n#&gt; # ‚Ñπ 15 more rows\n\n\n\n\nA space-filling design like this tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#your-turn-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#your-turn-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#create-a-grid-2",
    "href": "slides/advanced-01-tuning-hyperparameters.html#create-a-grid-2",
    "title": "1 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 25)\n\ngrid\n#&gt; # A tibble: 625 √ó 2\n#&gt;    trees   learn_rate\n#&gt;    &lt;int&gt;        &lt;dbl&gt;\n#&gt;  1     1 0.0000000001\n#&gt;  2    84 0.0000000001\n#&gt;  3   167 0.0000000001\n#&gt;  4   250 0.0000000001\n#&gt;  5   334 0.0000000001\n#&gt;  6   417 0.0000000001\n#&gt;  7   500 0.0000000001\n#&gt;  8   584 0.0000000001\n#&gt;  9   667 0.0000000001\n#&gt; 10   750 0.0000000001\n#&gt; # ‚Ñπ 615 more rows\n\n\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the deg_free parameters only have a range of 1-&gt;15."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "slides/advanced-01-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "1 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nset.seed(121)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1))) %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 √ó 2\n#&gt;    trees learn_rate\n#&gt;    &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1    17  0.00756  \n#&gt;  2    35  0.0000339\n#&gt;  3    77  0.000112 \n#&gt;  4    43  0.00124  \n#&gt;  5    58  0.0230   \n#&gt;  6     6  0.000471 \n#&gt;  7    91  0.000326 \n#&gt;  8    88  0.00899  \n#&gt;  9    15  0.00112  \n#&gt; 10    24  0.0777   \n#&gt; # ‚Ñπ 15 more rows"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#the-results",
    "href": "slides/advanced-01-tuning-hyperparameters.html#the-results",
    "title": "1 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %&gt;% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\nNote that the learning rates are uniform on the log-10 scale."
  },
  {
    "objectID": "slides/annotations.html#section",
    "href": "slides/annotations.html#section",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThere‚Äôs a lot that we want to tell you. We don‚Äôt want people to have to frantically scribble down things that we say that are not on the slides.\nWe‚Äôve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "slides/annotations.html#finalize-and-verify",
    "href": "slides/annotations.html#finalize-and-verify",
    "title": "Annotations",
    "section": "",
    "text": "This is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you don‚Äôt have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single ‚Äúresample‚Äù.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just ‚Äútesting‚Äù and ‚Äútraining‚Äù. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "slides/advanced-01-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "1 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet‚Äôs take our previous model and add more parameters:\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), mtry = tune(), \n             min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#grid-search-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#grid-search-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nIn reality we would probably sample the space more densely:"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#grid-results",
    "href": "slides/advanced-01-tuning-hyperparameters.html#grid-results",
    "title": "1 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(lgbm_res)"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#tuning-results",
    "href": "slides/advanced-01-tuning-hyperparameters.html#tuning-results",
    "title": "1 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res)\n#&gt; # A tibble: 50 √ó 10\n#&gt;     mtry trees min_n   learn_rate .metric .estimator   mean     n std_err .config              \n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1     1  1632     3 0.0000000262 mae     standard   53.2      10 0.427   Preprocessor1_Model01\n#&gt;  2     1  1632     3 0.0000000262 rsq     standard    0.734    10 0.00749 Preprocessor1_Model01\n#&gt;  3     3   935    37 0.0780       mae     standard   11.1      10 0.155   Preprocessor1_Model02\n#&gt;  4     3   935    37 0.0780       rsq     standard    0.936    10 0.00272 Preprocessor1_Model02\n#&gt;  5     4   663    33 0.00000396   mae     standard   53.1      10 0.426   Preprocessor1_Model03\n#&gt;  6     4   663    33 0.00000396   rsq     standard    0.841    10 0.00516 Preprocessor1_Model03\n#&gt;  7     5   532    27 0.00000816   mae     standard   53.1      10 0.425   Preprocessor1_Model04\n#&gt;  8     5   532    27 0.00000816   rsq     standard    0.857    10 0.00502 Preprocessor1_Model04\n#&gt;  9     6  1166     5 0.000104     mae     standard   48.5      10 0.389   Preprocessor1_Model05\n#&gt; 10     6  1166     5 0.000104     rsq     standard    0.859    10 0.00456 Preprocessor1_Model05\n#&gt; # ‚Ñπ 40 more rows"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#tuning-results-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#tuning-results-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res, summarize = FALSE)\n#&gt; # A tibble: 500 √ó 9\n#&gt;    id      mtry trees min_n   learn_rate .metric .estimator .estimate .config              \n#&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1 Fold01     1  1632     3 0.0000000262 mae     standard      51.8   Preprocessor1_Model01\n#&gt;  2 Fold01     1  1632     3 0.0000000262 rsq     standard       0.773 Preprocessor1_Model01\n#&gt;  3 Fold02     1  1632     3 0.0000000262 mae     standard      52.1   Preprocessor1_Model01\n#&gt;  4 Fold02     1  1632     3 0.0000000262 rsq     standard       0.742 Preprocessor1_Model01\n#&gt;  5 Fold03     1  1632     3 0.0000000262 mae     standard      52.2   Preprocessor1_Model01\n#&gt;  6 Fold03     1  1632     3 0.0000000262 rsq     standard       0.737 Preprocessor1_Model01\n#&gt;  7 Fold04     1  1632     3 0.0000000262 mae     standard      51.7   Preprocessor1_Model01\n#&gt;  8 Fold04     1  1632     3 0.0000000262 rsq     standard       0.702 Preprocessor1_Model01\n#&gt;  9 Fold05     1  1632     3 0.0000000262 mae     standard      55.2   Preprocessor1_Model01\n#&gt; 10 Fold05     1  1632     3 0.0000000262 rsq     standard       0.771 Preprocessor1_Model01\n#&gt; # ‚Ñπ 490 more rows"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "slides/advanced-01-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "1 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(lgbm_res, metric = \"rsq\")\n#&gt; # A tibble: 5 √ó 10\n#&gt;    mtry trees min_n learn_rate .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1    13  1309    13    0.0127  rsq     standard   0.940    10 0.00335 Preprocessor1_Model12\n#&gt; 2    14  1895    24    0.00730 rsq     standard   0.939    10 0.00310 Preprocessor1_Model13\n#&gt; 3    19   581    28    0.0231  rsq     standard   0.937    10 0.00315 Preprocessor1_Model18\n#&gt; 4     3   935    37    0.0780  rsq     standard   0.936    10 0.00272 Preprocessor1_Model02\n#&gt; 5    26  1094    17    0.00242 rsq     standard   0.918    10 0.00396 Preprocessor1_Model25"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nlgbm_best &lt;- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#&gt; # A tibble: 1 √ó 5\n#&gt;    mtry trees min_n learn_rate .config              \n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1    13  1309    13     0.0127 Preprocessor1_Model12"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#grid-search-2",
    "href": "slides/advanced-01-tuning-hyperparameters.html#grid-search-2",
    "title": "1 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nlgbm_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    control = ctrl,\n    metrics = reg_metrics\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#checking-calibration",
    "href": "slides/advanced-01-tuning-hyperparameters.html#checking-calibration",
    "title": "1 - Tuning Hyperparameters",
    "section": "Checking Calibration  ",
    "text": "Checking Calibration  \n\n\nlibrary(probably)\nlgbm_res %&gt;%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %&gt;%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred,\n    alpha = 1 / 3\n  )"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#updating-the-workflow",
    "href": "slides/advanced-01-tuning-hyperparameters.html#updating-the-workflow",
    "title": "1 - Tuning Hyperparameters",
    "section": "Updating the Workflow",
    "text": "Updating the Workflow\nIf we like this model, we might want to splice the best tuning parameter values into our lgbm_wflow object.\nThere is a function for that:\n\nlgbm_wflow &lt;- lgbm_wflow %&gt;% finalize_workflow(lgbm_best)\nlgbm_wflow\n#&gt; ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#&gt; Preprocessor: Recipe\n#&gt; Model: boost_tree()\n#&gt; \n#&gt; ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; ‚Ä¢ step_YeoJohnson()\n#&gt; \n#&gt; ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; Boosted Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   mtry = 13\n#&gt;   trees = 1309\n#&gt;   min_n = 13\n#&gt;   learn_rate = 0.012739722191806\n#&gt; \n#&gt; Computational engine: lightgbm"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#your-turn-2",
    "href": "slides/advanced-01-tuning-hyperparameters.html#your-turn-2",
    "title": "1 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nWhat if we used early stopping instead of optimizing the number of trees?\nWe could set trees = 1000 and use tune the stop_iter parameter.\n\n\n\n‚àí+\n10:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#running-in-parallel",
    "href": "slides/advanced-01-tuning-hyperparameters.html#running-in-parallel",
    "title": "1 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models don‚Äôt depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\ncl &lt;- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "slides/advanced-01-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "1 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#parameters",
    "href": "slides/advanced-01-tuning-hyperparameters.html#parameters",
    "title": "1 - Tuning Hyperparameters",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#sequential-search",
    "href": "slides/advanced-01-tuning-hyperparameters.html#sequential-search",
    "title": "1 - Tuning Hyperparameters",
    "section": "Sequential Search",
    "text": "Sequential Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#grid-search-3",
    "href": "slides/advanced-01-tuning-hyperparameters.html#grid-search-3",
    "title": "1 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nlgbm_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    control = ctrl,\n    metrics = reg_metrics\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "slides/advanced-01-tuning-hyperparameters.html#grid-search-4",
    "href": "slides/advanced-01-tuning-hyperparameters.html#grid-search-4",
    "title": "1 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nlgbm_res\n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 √ó 5\n#&gt;    splits             id     .metrics          .notes           .predictions        \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;              \n#&gt;  1 &lt;split [3372/377]&gt; Fold01 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,425 √ó 8]&gt;\n#&gt;  2 &lt;split [3373/376]&gt; Fold02 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,400 √ó 8]&gt;\n#&gt;  3 &lt;split [3373/376]&gt; Fold03 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,400 √ó 8]&gt;\n#&gt;  4 &lt;split [3373/376]&gt; Fold04 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,400 √ó 8]&gt;\n#&gt;  5 &lt;split [3373/376]&gt; Fold05 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,400 √ó 8]&gt;\n#&gt;  6 &lt;split [3374/375]&gt; Fold06 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,375 √ó 8]&gt;\n#&gt;  7 &lt;split [3375/374]&gt; Fold07 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,350 √ó 8]&gt;\n#&gt;  8 &lt;split [3376/373]&gt; Fold08 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,325 √ó 8]&gt;\n#&gt;  9 &lt;split [3376/373]&gt; Fold09 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,325 √ó 8]&gt;\n#&gt; 10 &lt;split [3376/373]&gt; Fold10 &lt;tibble [50 √ó 8]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [9,325 √ó 8]&gt;"
  }
]