[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, we’ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "index.html#is-this-workshop-for-me",
    "href": "index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %>% and/or native pipe |>\nYou are familiar with functions from dplyr, tidyr, and ggplot2\nYou can read data into R, transform and reshape data, and make a wide variety of graphs\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning."
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download\nThe following R packages, which you can install from the R console:\n\n\ninstall.packages(c(\"doParallel\", \"embed\", \"forcats\",\n                   \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal\")"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participants’ convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nDay One\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n\n\n\nDay Two\n\n05: Feature engineering\n06: Tuning hyperparameters\n07: Wrapping up"
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Don’t worry if you haven’t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "index.html#past-and-planned-workshops",
    "href": "index.html#past-and-planned-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past and planned workshops",
    "text": "Past and planned workshops\n\n25-26 July 2022 at rstudio::conf()"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "index.html#reuse-and-licensing",
    "href": "index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e. not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "slides/01-introduction.html#workshop-policies",
    "href": "slides/01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nIdentify the exits closest to you in case of emergency\nPlease do not photograph people wearing red lanyards\nA chill-out room is available for neurologically diverse attendees on TKTK"
  },
  {
    "objectID": "slides/01-introduction.html#workshop-policies-1",
    "href": "slides/01-introduction.html#workshop-policies-1",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the rstudio::conf code of conduct, which applies to all workshops\nCoC issues can be addressed three ways:\n\nIn person: contact any RStudio staff member or the conference registration desk\nBy email: send a message to conf@rstudio.com\nBy phone: call TKTK\n\nMore here about whatever masking thing we are saying"
  },
  {
    "objectID": "slides/01-introduction.html#who-are-you",
    "href": "slides/01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %>% or base R |> pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "slides/01-introduction.html#who-are-we",
    "href": "slides/01-introduction.html#who-are-we",
    "title": "1 - Introduction",
    "section": "Who are we?",
    "text": "Who are we?\n\n\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\n\nJulia Silge\nDavid Robinson\nDavis Vaughan"
  },
  {
    "objectID": "slides/01-introduction.html#who-are-we-1",
    "href": "slides/01-introduction.html#who-are-we-1",
    "title": "1 - Introduction",
    "section": "Who are we?",
    "text": "Who are we?\n\n\n\nKelly Bodwin\nMichael Chow\nPritam Dalal\nMatt Dancho\nJon Harmon\n\n\n\nMike Mahoney\nEdgar Ruiz\nAsmae Toumi\nQiushi Yan\n\n\n\n\nMany thanks to Julie Jung, Alison Hill, and Desirée De Leon for their role in creating these materials!"
  },
  {
    "objectID": "slides/01-introduction.html#asking-for-help",
    "href": "slides/01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\n🟪 “I’m stuck and need help!”\n\n\n🟩 “I finished the exercise”"
  },
  {
    "objectID": "slides/01-introduction.html#plan-for-this-workshop",
    "href": "slides/01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nToday:\n\nYour data budget\nWhat makes a model\nEvaluating models\n\nTomorrow:\n\nFeature engineering\nTuning hyperparameters\nWrapping up!"
  },
  {
    "objectID": "slides/01-introduction.html#section-2",
    "href": "slides/01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors 👋\n\n Log in to RStudio Cloud here (free):\nbit.ly/tidymodels-workshop"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-machine-learning",
    "href": "slides/01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-machine-learning-1",
    "href": "slides/01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-machine-learning-2",
    "href": "slides/01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn",
    "href": "slides/01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\n03:00\n\n\n\n\nthe “two cultures”\nmodel first vs. data first\ninference vs. prediction"
  },
  {
    "objectID": "slides/01-introduction.html#what-is-tidymodels",
    "href": "slides/01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#> ── Attaching packages ──────────────────────────── tidymodels 1.0.0 ──\n#> ✔ broom        1.0.0     ✔ rsample      1.0.0\n#> ✔ dials        1.0.0     ✔ tibble       3.1.7\n#> ✔ dplyr        1.0.9     ✔ tidyr        1.2.0\n#> ✔ infer        1.0.2     ✔ tune         1.0.0\n#> ✔ modeldata    1.0.0     ✔ workflows    1.0.0\n#> ✔ parsnip      1.0.0     ✔ workflowsets 0.2.1\n#> ✔ purrr        0.3.4     ✔ yardstick    1.0.0\n#> ✔ recipes      1.0.1\n#> ── Conflicts ─────────────────────────────── tidymodels_conflicts() ──\n#> ✖ purrr::discard() masks scales::discard()\n#> ✖ dplyr::filter()  masks stats::filter()\n#> ✖ dplyr::lag()     masks stats::lag()\n#> ✖ recipes::step()  masks stats::step()\n#> • Use tidymodels_prefer() to resolve common conflicts."
  },
  {
    "objectID": "slides/01-introduction.html#the-whole-game",
    "href": "slides/01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nTomorrow we will walk through a case study in detail to illustrate feature engineering and model tuning.\nToday we will walk through the analysis at a higher level to show the model development process as a whole and give you an introduction to the data set.\nThe data are from the NHL where we want to predict whether a shot was on-goal or not! emo::ji(\"ice_hockey\")\nIt’s a good example to show how model development works."
  },
  {
    "objectID": "slides/01-introduction.html#shots-on-goal",
    "href": "slides/01-introduction.html#shots-on-goal",
    "title": "1 - Introduction",
    "section": "Shots on goal",
    "text": "Shots on goal"
  },
  {
    "objectID": "slides/01-introduction.html#data-spending",
    "href": "slides/01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data spending",
    "text": "Data spending"
  },
  {
    "objectID": "slides/01-introduction.html#a-first-model",
    "href": "slides/01-introduction.html#a-first-model",
    "title": "1 - Introduction",
    "section": "A first model",
    "text": "A first model"
  },
  {
    "objectID": "slides/01-introduction.html#starting-point-logistic-regression",
    "href": "slides/01-introduction.html#starting-point-logistic-regression",
    "title": "1 - Introduction",
    "section": "Starting point: logistic regression",
    "text": "Starting point: logistic regression\n\nWe’ll start by using basic logistic regression to predict our binary outcome.\nOur first model will have 16 simple predictor columns.\nOne initial question: there are 640 players taking shots.\nFor logistic regression, do we convert these to binary indicators (a.k.a. “dummies”)?"
  },
  {
    "objectID": "slides/01-introduction.html#basic-features-inc-dummy-variables",
    "href": "slides/01-introduction.html#basic-features-inc-dummy-variables",
    "title": "1 - Introduction",
    "section": "Basic features (inc dummy variables)",
    "text": "Basic features (inc dummy variables)"
  },
  {
    "objectID": "slides/01-introduction.html#different-player-encoding",
    "href": "slides/01-introduction.html#different-player-encoding",
    "title": "1 - Introduction",
    "section": "Different player encoding",
    "text": "Different player encoding"
  },
  {
    "objectID": "slides/01-introduction.html#what-about-location",
    "href": "slides/01-introduction.html#what-about-location",
    "title": "1 - Introduction",
    "section": "What about location",
    "text": "What about location\nThe previous models used the x/y coordinates.\nAre there better ways to represent shot location?\nHow can we make location more usable for the model?"
  },
  {
    "objectID": "slides/01-introduction.html#add-shot-angle",
    "href": "slides/01-introduction.html#add-shot-angle",
    "title": "1 - Introduction",
    "section": "Add shot angle?",
    "text": "Add shot angle?"
  },
  {
    "objectID": "slides/01-introduction.html#add-shot-distance",
    "href": "slides/01-introduction.html#add-shot-distance",
    "title": "1 - Introduction",
    "section": "Add shot distance?",
    "text": "Add shot distance?"
  },
  {
    "objectID": "slides/01-introduction.html#add-shot-behind-goal-line",
    "href": "slides/01-introduction.html#add-shot-behind-goal-line",
    "title": "1 - Introduction",
    "section": "Add shot behind goal line?",
    "text": "Add shot behind goal line?"
  },
  {
    "objectID": "slides/01-introduction.html#nonlinear-terms-for-angle-and-distiance",
    "href": "slides/01-introduction.html#nonlinear-terms-for-angle-and-distiance",
    "title": "1 - Introduction",
    "section": "Nonlinear terms for angle and distiance",
    "text": "Nonlinear terms for angle and distiance"
  },
  {
    "objectID": "slides/01-introduction.html#try-another-model",
    "href": "slides/01-introduction.html#try-another-model",
    "title": "1 - Introduction",
    "section": "Try another model",
    "text": "Try another model"
  },
  {
    "objectID": "slides/01-introduction.html#switch-to-boosting-and-basic-features",
    "href": "slides/01-introduction.html#switch-to-boosting-and-basic-features",
    "title": "1 - Introduction",
    "section": "Switch to boosting and basic features",
    "text": "Switch to boosting and basic features"
  },
  {
    "objectID": "slides/01-introduction.html#boosting-with-location-features",
    "href": "slides/01-introduction.html#boosting-with-location-features",
    "title": "1 - Introduction",
    "section": "boosting with location features",
    "text": "boosting with location features"
  },
  {
    "objectID": "slides/01-introduction.html#choose-wisely",
    "href": "slides/01-introduction.html#choose-wisely",
    "title": "1 - Introduction",
    "section": "Choose wisely…",
    "text": "Choose wisely…"
  },
  {
    "objectID": "slides/01-introduction.html#finalize-and-verify",
    "href": "slides/01-introduction.html#finalize-and-verify",
    "title": "1 - Introduction",
    "section": "Finalize and verify",
    "text": "Finalize and verify"
  },
  {
    "objectID": "slides/01-introduction.html#and-so-on",
    "href": "slides/01-introduction.html#and-so-on",
    "title": "1 - Introduction",
    "section": "… and so on",
    "text": "… and so on\nOnce we find an acceptable model and feature set, the process is to\n\nConfirm our results on the test set.\nDocument the data and model development process.\nDeploy, monitor, etc."
  },
  {
    "objectID": "slides/01-introduction.html#lets-install-some-packages",
    "href": "slides/01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Let’s install some packages",
    "text": "Let’s install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\ninstall.packages(c(\"doParallel\", \"embed\", \"forcats\",\n                   \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal\")\n\n\n\n Or log in to RStudio Cloud:\nbit.ly/tidymodels-workshop\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching-1",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching-1",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\nRed-eyed tree frog embryos can hatch earlier than their normal ~7 days if they detect potential predator threat!\nType ?stacks::tree_frogs to learn more about this dataset, including references.\nWe are using a slightly modified version from stacks.\n\n\nlibrary(tidymodels)\n\ndata(\"tree_frogs\", package = \"stacks\")\ntree_frogs <- tree_frogs %>%\n  mutate(t_o_d = factor(t_o_d)) %>%\n  filter(!is.na(latency)) %>%\n  select(-c(clutch, hatched))"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching-2",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching-2",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\n\n\nN = 572\nA numeric outcome, latency\n4 other variables\n\ntreatment, reflex, and t_o_d are nominal predictors\nage is a numeric predictor"
  },
  {
    "objectID": "slides/02-data-budget.html#data-on-tree-frog-hatching-3",
    "href": "slides/02-data-budget.html#data-on-tree-frog-hatching-3",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\ntree_frogs\n#> # A tibble: 572 × 5\n#>    treatment  reflex    age t_o_d     latency\n#>    <chr>      <fct>   <dbl> <fct>       <dbl>\n#>  1 control    full   466965 morning        22\n#>  2 control    low    361180 night         360\n#>  3 control    full   401595 afternoon     106\n#>  4 control    mid    357810 night         180\n#>  5 control    full   397440 afternoon      60\n#>  6 gentamicin full   463230 morning        39\n#>  7 control    full   393900 afternoon     214\n#>  8 control    full   469065 morning        50\n#>  9 control    full   400240 afternoon     224\n#> 10 control    full   466160 morning        63\n#> # … with 562 more rows"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending",
    "href": "slides/02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not 🚫 use the test set during training."
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-1",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-2",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "slides/02-data-budget.html#your-turn",
    "href": "slides/02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-3",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs, strata = latency)\nfrog_split\n#> <Training/Testing/Total>\n#> <428/144/572>"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-4",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-4",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nfrog_train <- training(frog_split)\nfrog_test <- testing(frog_split)"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-5",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-5",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nfrog_train\n#> # A tibble: 428 × 5\n#>    treatment  reflex    age t_o_d     latency\n#>    <chr>      <fct>   <dbl> <fct>       <dbl>\n#>  1 control    full   467950 morning        33\n#>  2 control    full   464870 morning        19\n#>  3 control    full   464610 morning         2\n#>  4 control    full   469650 morning        39\n#>  5 control    full   467600 morning        42\n#>  6 control    full   410460 afternoon      20\n#>  7 control    full   427685 night          31\n#>  8 control    full   468530 morning        21\n#>  9 gentamicin full   465800 morning        30\n#> 10 control    full   393475 afternoon      43\n#> # … with 418 more rows"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-6",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-6",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nfrog_test\n#> # A tibble: 144 × 5\n#>    treatment  reflex    age t_o_d     latency\n#>    <chr>      <fct>   <dbl> <fct>       <dbl>\n#>  1 control    full   466965 morning        22\n#>  2 control    low    361180 night         360\n#>  3 control    full   401595 afternoon     106\n#>  4 control    mid    357810 night         180\n#>  5 gentamicin full   463230 morning        39\n#>  6 control    full   469065 morning        50\n#>  7 control    full   466160 morning        63\n#>  8 control    full   465800 morning        25\n#>  9 control    full   463800 morning        48\n#> 10 control    full   392680 afternoon     126\n#> # … with 134 more rows"
  },
  {
    "objectID": "slides/02-data-budget.html#your-turn-1",
    "href": "slides/02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\n05:00"
  },
  {
    "objectID": "slides/02-data-budget.html#data-splitting-and-spending-7",
    "href": "slides/02-data-budget.html#data-splitting-and-spending-7",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)\nfrog_train <- training(frog_split)\nfrog_test <- testing(frog_split)\n\nnrow(frog_train)\n#> [1] 456\nnrow(frog_test)\n#> [1] 116"
  },
  {
    "objectID": "slides/02-data-budget.html#section-3",
    "href": "slides/02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency)) +\n  geom_histogram(bins = 20)\n\n\n\nRemember how we used strata = latency?"
  },
  {
    "objectID": "slides/02-data-budget.html#section-4",
    "href": "slides/02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "quartiles <- quantile(frog_train$latency, probs = c(1:3)/4)\nggplot(frog_train, aes(latency)) +\n  geom_histogram(bins = 20) +\n  geom_vline(xintercept = quartiles, color = train_color, \n             size = 1.5, lty = 2)\n\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "slides/02-data-budget.html#section-5",
    "href": "slides/02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency, treatment, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE)"
  },
  {
    "objectID": "slides/02-data-budget.html#section-6",
    "href": "slides/02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "frog_train %>%\n  ggplot(aes(latency, reflex, fill = reflex)) +\n  geom_boxplot(alpha = 0.3, show.legend = FALSE)"
  },
  {
    "objectID": "slides/02-data-budget.html#section-7",
    "href": "slides/02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(reflex, age)) +\n  stat_summary_2d(aes(z = latency), \n                  alpha = 0.7, binwidth = c(1, 5e3)) +\n  scale_fill_viridis_c() +\n  labs(fill = \"mean latency\")"
  },
  {
    "objectID": "slides/02-data-budget.html#your-turn-2",
    "href": "slides/02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the frog_train data on your own!\n\n\n\n\n05:00\n\n\n\n\nMake a plot or summary and then share with neighbor\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn",
    "href": "slides/03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\n03:00\n\n\n\n\n\nlm for generalized linear model\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nlinear_reg()\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %>%\n  set_engine(\"glmnet\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: glmnet"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %>%\n  set_engine(\"stan\")\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: stan"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#> Decision Tree Model Specification (unknown)\n#> \n#> Computational engine: rpart"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %>% \n  set_mode(\"regression\")\n#> Decision Tree Model Specification (regression)\n#> \n#> Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "slides/03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-1",
    "href": "slides/03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code so it creates a different model.\n\n\n\n05:00\n\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#all-models-are-wrong-but",
    "href": "slides/03-what-makes-a-model.html#all-models-are-wrong-but",
    "title": "3 - What makes a model?",
    "section": "All models are wrong but…",
    "text": "All models are wrong but…"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-1",
    "href": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-1",
    "title": "3 - What makes a model?",
    "section": "All models are wrong but…",
    "text": "All models are wrong but…"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-2",
    "href": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-2",
    "title": "3 - What makes a model?",
    "section": "All models are wrong but…",
    "text": "All models are wrong but…\n\n\n\n\n\n\n\n\nLinear regression\n\nOrdinary least squares (OLS)\nOutcome modeled as linear combination of predictors"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-3",
    "href": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-3",
    "title": "3 - What makes a model?",
    "section": "All models are wrong but…",
    "text": "All models are wrong but…"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-4",
    "href": "slides/03-what-makes-a-model.html#all-models-are-wrong-but-4",
    "title": "3 - What makes a model?",
    "section": "All models are wrong but…",
    "text": "All models are wrong but…\n\n\n\n\n\n\n\n\nDecision tree\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#why-a-workflow",
    "href": "slides/03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than model.matrix() in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering tomorrow!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than model.matrix():\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your “new” data just doesn’t have an instance of that level)"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-1",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\ntree_spec %>% \n  fit(latency ~ ., data = frog_train) \n#> parsnip model object\n#> \n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=427505 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 427505 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 403007.5 30  102190.20 147.83330  \n#>         28) age>=394557.5 22   53953.86 129.77270 *\n#>         29) age< 394557.5 8   21326.00 197.50000 *\n#>       15) age>=403007.5 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-2",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow() %>%\n  add_formula(latency ~ .) %>%\n  add_model(tree_spec) %>%\n  fit(data = frog_train) \n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> latency ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=427505 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 427505 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 403007.5 30  102190.20 147.83330  \n#>         28) age>=394557.5 22   53953.86 129.77270 *\n#>         29) age< 394557.5 8   21326.00 197.50000 *\n#>       15) age>=403007.5 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-3",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow() %>%\n  add_variables(outcomes = latency, predictors = everything()) %>%\n  add_model(tree_spec) %>%\n  fit(data = frog_train) \n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Variables\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> Outcomes: latency\n#> Predictors: everything()\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=427505 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 427505 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 403007.5 30  102190.20 147.83330  \n#>         28) age>=394557.5 22   53953.86 129.77270 *\n#>         29) age< 394557.5 8   21326.00 197.50000 *\n#>       15) age>=403007.5 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#a-model-workflow-4",
    "href": "slides/03-what-makes-a-model.html#a-model-workflow-4",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\nworkflow(latency ~ ., tree_spec) %>% \n  fit(data = frog_train) \n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> latency ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> n= 456 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 456 2197966.00  92.90351  \n#>    2) age>=427505 256  252347.40  60.89844  \n#>      4) treatment=control 131   91424.06  48.42748 *\n#>      5) treatment=gentamicin 125  119197.90  73.96800 *\n#>    3) age< 427505 200 1347741.00 133.87000  \n#>      6) treatment=control 140  986790.70 118.25710  \n#>       12) reflex=mid,full 129  754363.70 111.56590 *\n#>       13) reflex=low 11  158918.20 196.72730 *\n#>      7) treatment=gentamicin 60  247194.60 170.30000  \n#>       14) age< 403007.5 30  102190.20 147.83330  \n#>         28) age>=394557.5 22   53953.86 129.77270 *\n#>         29) age< 394557.5 8   21326.00 197.50000 *\n#>       15) age>=403007.5 30  114719.40 192.76670 *"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-2",
    "href": "slides/03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code so it:\n\ncreates a different model\nuses one of the other workflow() interfaces\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#predict-with-your-model",
    "href": "slides/03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"regression\")\n\ntree_fit <-\n  workflow(latency ~ ., tree_spec) %>% \n  fit(data = frog_train)"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-3",
    "href": "slides/03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-4",
    "href": "slides/03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#understand-your-model",
    "href": "slides/03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#understand-your-model-1",
    "href": "slides/03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#understand-your-model-2",
    "href": "slides/03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %>%\n  extract_fit_engine() %>%\n  rpart.plot()\n\nYou can extract_*() several components of your fitted workflow."
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-5",
    "href": "slides/03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRead the documentation for object extraction.\nTry out some extraction methods on your trained workflow.\n⚠️ Never predict() with any extracted components!\n\n\n\n05:00"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#deploy-your-model",
    "href": "slides/03-what-makes-a-model.html#deploy-your-model",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new tree_fit model in production?\n\nlibrary(vetiver)\nv <- vetiver_model(tree_fit, \"frog_hatching\")\nv\n#> \n#> ── frog_hatching ─ <butchered_workflow> model for deployment \n#> A rpart regression modeling workflow using 4 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#deploy-your-model-1",
    "href": "slides/03-what-makes-a-model.html#deploy-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new model tree_fit in production?\n\nlibrary(plumber)\npr() %>%\n  vetiver_api(v)\n#> # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#> # Use `pr_run()` on this object to start the API.\n#> ├──[queryString]\n#> ├──[body]\n#> ├──[cookieParser]\n#> ├──[sharedSecret]\n#> ├──/logo\n#> │  │ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2/Resources/library/vetiver\n#> ├──/ping (GET)\n#> └──/predict (POST)\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "slides/03-what-makes-a-model.html#your-turn-6",
    "href": "slides/03-what-makes-a-model.html#your-turn-6",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\n05:00\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\n\naugment(tree_fit, new_data = frog_test) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 rsq     standard       0.380\n#> 3 mae     standard      40.2\n\n\n\nRMSE: difference between the predicted and observed values ⬇️\n\\(R^2\\): squared correlation between the predicted and observed values ⬆️\nMAE: similar to RMSE, but mean absolute error ⬇️"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        59.2"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %>%\n  group_by(reflex) %>%\n  rmse(latency, .pred)\n#> # A tibble: 3 × 4\n#>   reflex .metric .estimator .estimate\n#>   <fct>  <chr>   <chr>          <dbl>\n#> 1 low    rmse    standard        94.3\n#> 2 mid    rmse    standard       101. \n#> 3 full   rmse    standard        51.2"
  },
  {
    "objectID": "slides/04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "slides/04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\nfrog_metrics <- metric_set(rmse, msd)\naugment(tree_fit, new_data = frog_test) %>%\n  frog_metrics(latency, .pred)\n#> # A tibble: 2 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 msd     standard      -0.908"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️",
    "text": "Dangers of overfitting ⚠️"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️",
    "text": "Dangers of overfitting ⚠️"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️ ",
    "text": "Dangers of overfitting ⚠️ \n\ntree_fit %>%\n  augment(frog_train)\n#> # A tibble: 456 × 6\n#>    treatment  reflex    age t_o_d     latency .pred\n#>    <chr>      <fct>   <dbl> <fct>       <dbl> <dbl>\n#>  1 control    full   467950 morning        33  39.8\n#>  2 control    full   464870 morning        19  66.7\n#>  3 control    full   464610 morning         2  66.7\n#>  4 control    full   469650 morning        39  39.8\n#>  5 control    full   467600 morning        42  39.8\n#>  6 control    full   410460 afternoon      20  59.8\n#>  7 control    full   427685 night          31  83.1\n#>  8 control    full   468530 morning        21  39.8\n#>  9 gentamicin full   465800 morning        30  64.6\n#> 10 control    full   393475 afternoon      43 174. \n#> # … with 446 more rows\n\nWe call this “resubstition” or “repredicting the training set”"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️ ",
    "text": "Dangers of overfitting ⚠️ \n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4\n\nWe call this a “resubstition metric”"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️ ",
    "text": "Dangers of overfitting ⚠️ \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️ ",
    "text": "Dangers of overfitting ⚠️ \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        49.4\n\n\n\ntree_fit %>%\n  augment(frog_test) %>%\n  rmse(latency, .pred)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        59.2\n\n\n\n\n⚠️ Remember that we’re demonstrating overfitting\n\n\n⚠️ Don’t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn",
    "href": "slides/04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and metrics() to compute regression metrics.\nCompute the metrics for both training and testing data.\nNotice the evidence of overfitting! ⚠️\n\n\n\n05:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "slides/04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting ⚠️ ",
    "text": "Dangers of overfitting ⚠️ \n\n\n\ntree_fit %>%\n  augment(frog_train) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      49.4  \n#> 2 rsq     standard       0.494\n#> 3 mae     standard      33.4\n\n\n\ntree_fit %>%\n  augment(frog_test) %>%\n  metrics(latency, .pred)\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard      59.2  \n#> 2 rsq     standard       0.380\n#> 3 mae     standard      40.2\n\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation",
    "href": "slides/04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/04-evaluating-models.html#cross-validation-1",
    "href": "slides/04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-1",
    "href": "slides/04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling",
    "href": "slides/04-evaluating-models.html#resampling",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \n\nvfold_cv(frog_train) ## v = 10 is default\n#> #  10-fold cross-validation \n#> # A tibble: 10 × 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [410/46]> Fold01\n#>  2 <split [410/46]> Fold02\n#>  3 <split [410/46]> Fold03\n#>  4 <split [410/46]> Fold04\n#>  5 <split [410/46]> Fold05\n#>  6 <split [410/46]> Fold06\n#>  7 <split [411/45]> Fold07\n#>  8 <split [411/45]> Fold08\n#>  9 <split [411/45]> Fold09\n#> 10 <split [411/45]> Fold10"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling-1",
    "href": "slides/04-evaluating-models.html#resampling-1",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \n\nvfold_cv(frog_train, v = 5)\n#> #  5-fold cross-validation \n#> # A tibble: 5 × 2\n#>   splits           id   \n#>   <list>           <chr>\n#> 1 <split [364/92]> Fold1\n#> 2 <split [365/91]> Fold2\n#> 3 <split [365/91]> Fold3\n#> 4 <split [365/91]> Fold4\n#> 5 <split [365/91]> Fold5"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling-2",
    "href": "slides/04-evaluating-models.html#resampling-2",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \nWhat is in this?\n\nfrog_folds <- vfold_cv(frog_train, v = 10)\nfrog_folds$splits[1:3]\n#> [[1]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n#> \n#> [[2]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n#> \n#> [[3]]\n#> <Analysis/Assess/Total>\n#> <410/46/456>\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "slides/04-evaluating-models.html#bootstrapping",
    "href": "slides/04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling-3",
    "href": "slides/04-evaluating-models.html#resampling-3",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \n\nbootstraps(frog_train)\n#> # Bootstrap sampling \n#> # A tibble: 25 × 2\n#>    splits            id         \n#>    <list>            <chr>      \n#>  1 <split [456/159]> Bootstrap01\n#>  2 <split [456/168]> Bootstrap02\n#>  3 <split [456/175]> Bootstrap03\n#>  4 <split [456/176]> Bootstrap04\n#>  5 <split [456/162]> Bootstrap05\n#>  6 <split [456/169]> Bootstrap06\n#>  7 <split [456/161]> Bootstrap07\n#>  8 <split [456/169]> Bootstrap08\n#>  9 <split [456/177]> Bootstrap09\n#> 10 <split [456/174]> Bootstrap10\n#> # … with 15 more rows"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-2",
    "href": "slides/04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\ncross-validation folds with stratification\nbootstrap folds (change times from the default)\nvalidation resample\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling-4",
    "href": "slides/04-evaluating-models.html#resampling-4",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \n\nvfold_cv(frog_train, strata = latency)\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 × 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [408/48]> Fold01\n#>  2 <split [408/48]> Fold02\n#>  3 <split [408/48]> Fold03\n#>  4 <split [409/47]> Fold04\n#>  5 <split [411/45]> Fold05\n#>  6 <split [412/44]> Fold06\n#>  7 <split [412/44]> Fold07\n#>  8 <split [412/44]> Fold08\n#>  9 <split [412/44]> Fold09\n#> 10 <split [412/44]> Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling-5",
    "href": "slides/04-evaluating-models.html#resampling-5",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \n\nbootstraps(frog_train, times = 10)\n#> # Bootstrap sampling \n#> # A tibble: 10 × 2\n#>    splits            id         \n#>    <list>            <chr>      \n#>  1 <split [456/162]> Bootstrap01\n#>  2 <split [456/165]> Bootstrap02\n#>  3 <split [456/159]> Bootstrap03\n#>  4 <split [456/169]> Bootstrap04\n#>  5 <split [456/168]> Bootstrap05\n#>  6 <split [456/171]> Bootstrap06\n#>  7 <split [456/175]> Bootstrap07\n#>  8 <split [456/168]> Bootstrap08\n#>  9 <split [456/166]> Bootstrap09\n#> 10 <split [456/156]> Bootstrap10"
  },
  {
    "objectID": "slides/04-evaluating-models.html#resampling-6",
    "href": "slides/04-evaluating-models.html#resampling-6",
    "title": "4 - Evaluating models",
    "section": "Resampling ",
    "text": "Resampling \n\nvalidation_split(frog_train, strata = latency)\n#> # Validation Set Split (0.75/0.25)  using stratification \n#> # A tibble: 1 × 2\n#>   splits            id        \n#>   <list>            <chr>     \n#> 1 <split [340/116]> validation\n\n\nA validation split is just another type of resample"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nset.seed(123)\nfrog_folds <- vfold_cv(frog_train, v = 10, strata = latency)\nfrog_folds\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 × 2\n#>    splits           id    \n#>    <list>           <chr> \n#>  1 <split [408/48]> Fold01\n#>  2 <split [408/48]> Fold02\n#>  3 <split [408/48]> Fold03\n#>  4 <split [409/47]> Fold04\n#>  5 <split [411/45]> Fold05\n#>  6 <split [412/44]> Fold06\n#>  7 <split [412/44]> Fold07\n#>  8 <split [412/44]> Fold08\n#>  9 <split [412/44]> Fold09\n#> 10 <split [412/44]> Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-1",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res <- fit_resamples(tree_wflow, frog_folds)\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 4\n#>    splits           id     .metrics         .notes          \n#>    <list>           <chr>  <list>           <list>          \n#>  1 <split [408/48]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  2 <split [408/48]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  3 <split [408/48]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  4 <split [409/47]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  5 <split [411/45]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  6 <split [412/44]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  7 <split [412/44]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  8 <split [412/44]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  9 <split [412/44]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 10 <split [412/44]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]>\n\n\nWhere are the fitted models??!??"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-2",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res <- fit_resamples(tree_wflow, frog_folds)\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 4\n#>    splits           id     .metrics         .notes          \n#>    <list>           <chr>  <list>           <list>          \n#>  1 <split [408/48]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  2 <split [408/48]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  3 <split [408/48]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  4 <split [409/47]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  5 <split [411/45]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  6 <split [412/44]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  7 <split [412/44]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  8 <split [412/44]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]>\n#>  9 <split [412/44]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]>\n#> 10 <split [412/44]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]>\n\nWhere are the fitted models??!?? 🗑️\n\nFor more advanced use cases, you can extract and save them: https://www.tmwr.org/resampling.html#extract"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-3",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res %>%\n  collect_metrics()\n#> # A tibble: 2 × 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   59.6      10  2.31   Preprocessor1_Model1\n#> 2 rsq     standard    0.305    10  0.0342 Preprocessor1_Model1\n\n\nWe can reliably measure performance using only the training data 🎉"
  },
  {
    "objectID": "slides/04-evaluating-models.html#comparing-metrics",
    "href": "slides/04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\n\n\n\ntree_res %>%\n  collect_metrics() %>% \n  select(.metric, mean, n)\n#> # A tibble: 2 × 3\n#>   .metric   mean     n\n#>   <chr>    <dbl> <int>\n#> 1 rmse    59.6      10\n#> 2 rsq      0.305    10\n\n\nThe RMSE previously was\n\n49.36 for the training set\n59.16 for test set\n\n\n\n\nRemember that:\n⚠️ the training set gives you overly optimistic metrics\n⚠️ the test set is precious"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-4",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_frog <- control_resamples(save_pred = TRUE)\ntree_res <- fit_resamples(tree_wflow, frog_folds, control = ctrl_frog)\n\ntree_preds <- collect_predictions(tree_res)\ntree_preds\n#> # A tibble: 456 × 5\n#>    id     .pred  .row latency .config             \n#>    <chr>  <dbl> <int>   <dbl> <chr>               \n#>  1 Fold01  39.6     1      33 Preprocessor1_Model1\n#>  2 Fold01  72.1     3       2 Preprocessor1_Model1\n#>  3 Fold01  63.8     9      30 Preprocessor1_Model1\n#>  4 Fold01  72.1    13      46 Preprocessor1_Model1\n#>  5 Fold01  43.3    28      11 Preprocessor1_Model1\n#>  6 Fold01  61.7    35      41 Preprocessor1_Model1\n#>  7 Fold01  39.6    51      43 Preprocessor1_Model1\n#>  8 Fold01 134.     70      20 Preprocessor1_Model1\n#>  9 Fold01  70.6    74      21 Preprocessor1_Model1\n#> 10 Fold01  39.6   106      14 Preprocessor1_Model1\n#> # … with 446 more rows"
  },
  {
    "objectID": "slides/04-evaluating-models.html#section-4",
    "href": "slides/04-evaluating-models.html#section-4",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "tree_preds %>% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "slides/04-evaluating-models.html#random-forest-1",
    "href": "slides/04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵",
    "text": "Random forest 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵\n\nEnsemble many decision tree models\nAll the trees vote! 🗳️\nBootstrap aggregating + random feature selection\n\n\n\nOften works well without tuning hyperparameters (more on this tomorrow!), as long as there are enough trees"
  },
  {
    "objectID": "slides/04-evaluating-models.html#create-a-random-forest-model",
    "href": "slides/04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec <- rand_forest(trees = 1000, mode = \"regression\")\nrf_spec\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger"
  },
  {
    "objectID": "slides/04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "slides/04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow <- workflow(latency ~ ., rf_spec)\nrf_wflow\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> latency ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-3",
    "href": "slides/04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\nplot true vs. predicted values\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluating-model-performance-5",
    "href": "slides/04-evaluating-models.html#evaluating-model-performance-5",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_frog <- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res <- fit_resamples(rf_wflow, frog_folds, control = ctrl_frog)\ncollect_metrics(rf_res)\n#> # A tibble: 2 × 6\n#>   .metric .estimator   mean     n std_err .config             \n#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   55.9      10  1.71   Preprocessor1_Model1\n#> 2 rsq     standard    0.370    10  0.0306 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/04-evaluating-models.html#section-5",
    "href": "slides/04-evaluating-models.html#section-5",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(rf_res) %>% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "slides/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "slides/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "4 - Evaluating models",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluate-a-workflow-set",
    "href": "slides/04-evaluating-models.html#evaluate-a-workflow-set",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec))\n#> # A workflow set/tibble: 2 × 4\n#>   wflow_id              info             option    result    \n#>   <chr>                 <list>           <list>    <list>    \n#> 1 formula_decision_tree <tibble [1 × 4]> <opts[0]> <list [0]>\n#> 2 formula_rand_forest   <tibble [1 × 4]> <opts[0]> <list [0]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "href": "slides/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %>%\n  workflow_map(\"fit_resamples\", resamples = frog_folds)\n#> # A workflow set/tibble: 2 × 4\n#>   wflow_id              info             option    result   \n#>   <chr>                 <list>           <list>    <list>   \n#> 1 formula_decision_tree <tibble [1 × 4]> <opts[1]> <rsmp[+]>\n#> 2 formula_rand_forest   <tibble [1 × 4]> <opts[1]> <rsmp[+]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "href": "slides/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %>%\n  workflow_map(\"fit_resamples\", resamples = frog_folds) %>%\n  rank_results()\n#> # A tibble: 4 × 9\n#>   wflow_id         .config .metric   mean std_err     n preprocessor model  rank\n#>   <chr>            <chr>   <chr>    <dbl>   <dbl> <int> <chr>        <chr> <int>\n#> 1 formula_rand_fo… Prepro… rmse    55.8    1.71      10 formula      rand…     1\n#> 2 formula_rand_fo… Prepro… rsq      0.371  0.0301    10 formula      rand…     1\n#> 3 formula_decisio… Prepro… rmse    59.6    2.31      10 formula      deci…     2\n#> 4 formula_decisio… Prepro… rsq      0.305  0.0342    10 formula      deci…     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-4",
    "href": "slides/04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#the-final-fit",
    "href": "slides/04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLet’s verify our performance using the test set.\n\nWe’ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# frog_split has train + test info\nfinal_fit <- last_fit(rf_wflow, frog_split) \n\nfinal_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 × 6\n#>   splits            id               .metrics .notes   .predictions .workflow \n#>   <list>            <chr>            <list>   <list>   <list>       <list>    \n#> 1 <split [456/116]> train/test split <tibble> <tibble> <tibble>     <workflow>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#what-is-in-final_fit",
    "href": "slides/04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#> # A tibble: 2 × 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 rmse    standard      57.1   Preprocessor1_Model1\n#> 2 rsq     standard       0.420 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "slides/04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "slides/04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#> # A tibble: 116 × 5\n#>    id               .pred  .row latency .config             \n#>    <chr>            <dbl> <int>   <dbl> <chr>               \n#>  1 train/test split  43.5     1      22 Preprocessor1_Model1\n#>  2 train/test split 104.      3     106 Preprocessor1_Model1\n#>  3 train/test split  76.2     6      39 Preprocessor1_Model1\n#>  4 train/test split  42.5     8      50 Preprocessor1_Model1\n#>  5 train/test split  43.5    10      63 Preprocessor1_Model1\n#>  6 train/test split  43.1    14      25 Preprocessor1_Model1\n#>  7 train/test split  51.5    16      48 Preprocessor1_Model1\n#>  8 train/test split 160.     17      91 Preprocessor1_Model1\n#>  9 train/test split  50.9    32      11 Preprocessor1_Model1\n#> 10 train/test split 171.     33     109 Preprocessor1_Model1\n#> # … with 106 more rows\n\n\nThese are predictions for the test set"
  },
  {
    "objectID": "slides/04-evaluating-models.html#section-6",
    "href": "slides/04-evaluating-models.html#section-6",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(final_fit) %>%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, col = \"deeppink4\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "slides/04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "slides/04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#> ══ Workflow [trained] ════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> latency ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  1000 \n#> Sample size:                      456 \n#> Number of independent variables:  4 \n#> Mtry:                             2 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       3124.581 \n#> R squared (OOB):                  0.3531818\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "slides/04-evaluating-models.html#your-turn-5",
    "href": "slides/04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nEnd of the day discussion!\nWhich model do you think you would decide to use?\nWhat surprised you the most?\nWhat is one thing you are looking forward to for tomorrow?\n\n\n\n05:00"
  },
  {
    "objectID": "slides/04-evaluating-models.html#why-choose-just-one-final_fit",
    "href": "slides/04-evaluating-models.html#why-choose-just-one-final_fit",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-1",
    "href": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-2",
    "href": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-3",
    "href": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-3",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-4",
    "href": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-4",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-5",
    "href": "slides/04-evaluating-models.html#why-choose-just-one-final_fit-5",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack",
    "href": "slides/04-evaluating-models.html#building-a-model-stack",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nIteratively add candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-1",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-1",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nstack_ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE)"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-2",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-2",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nDefine candidate members\n\nStart out with a linear regression:\n\nlr_res <- \n  # define model spec\n  linear_reg() %>%\n  set_mode(\"regression\") %>%\n  # add to workflow\n  workflow(preprocessor = latency ~ .) %>%\n  # fit to resamples\n  fit_resamples(frog_folds, control = stack_ctrl)"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-3",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-3",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlr_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 5\n#>    splits           id     .metrics         .notes           .predictions     \n#>    <list>           <chr>  <list>           <list>           <list>           \n#>  1 <split [408/48]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [48 × 4]>\n#>  2 <split [408/48]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [48 × 4]>\n#>  3 <split [408/48]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [48 × 4]>\n#>  4 <split [409/47]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [47 × 4]>\n#>  5 <split [411/45]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [45 × 4]>\n#>  6 <split [412/44]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#>  7 <split [412/44]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#>  8 <split [412/44]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#>  9 <split [412/44]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#> 10 <split [412/44]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-4",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-4",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \nThen, a random forest:\n\nrf_res <- \n  # define model spec\n  rand_forest() %>%\n  set_mode(\"regression\") %>%\n  # add to workflow\n  workflow(preprocessor = latency ~ .) %>%\n  # fit to resamples\n  fit_resamples(frog_folds, control = stack_ctrl)"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-5",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-5",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nrf_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 5\n#>    splits           id     .metrics         .notes           .predictions     \n#>    <list>           <chr>  <list>           <list>           <list>           \n#>  1 <split [408/48]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [48 × 4]>\n#>  2 <split [408/48]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [48 × 4]>\n#>  3 <split [408/48]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [48 × 4]>\n#>  4 <split [409/47]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [47 × 4]>\n#>  5 <split [411/45]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [45 × 4]>\n#>  6 <split [412/44]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#>  7 <split [412/44]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#>  8 <split [412/44]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#>  9 <split [412/44]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>\n#> 10 <split [412/44]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [44 × 4]>"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-6",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-6",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nInitialize a data stack object\n\n\n\nst <- stacks()\n\nst\n#> # A data stack with 0 model definitions and 0 candidate members."
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-7",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-7",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nIteratively add candidate ensemble members to the data stack\n\n\nst <- st %>%\n  add_candidates(lr_res) %>%\n  add_candidates(rf_res)\n\nst\n#> # A data stack with 2 model definitions and 2 candidate members:\n#> #   lr_res: 1 model configuration\n#> #   rf_res: 1 model configuration\n#> # Outcome: latency (numeric)"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-8",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-8",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nEvaluate how to combine their predictions\n\n\nst <- st %>%\n  blend_predictions()\n\nst\n#> # A tibble: 2 × 3\n#>   member     type        weight\n#>   <chr>      <chr>        <dbl>\n#> 1 rf_res_1_1 rand_forest  0.635\n#> 2 lr_res_1_1 linear_reg   0.344"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-9",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-9",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nFit candidate ensemble members with non-zero stacking coefficients\n\n\nst <- st %>%\n  fit_members()\n\nst\n#> # A tibble: 2 × 3\n#>   member     type        weight\n#>   <chr>      <chr>        <dbl>\n#> 1 rf_res_1_1 rand_forest  0.635\n#> 2 lr_res_1_1 linear_reg   0.344"
  },
  {
    "objectID": "slides/04-evaluating-models.html#building-a-model-stack-10",
    "href": "slides/04-evaluating-models.html#building-a-model-stack-10",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nPredict on new data!\n\n\n\nfrog_test %>%\n  select(latency) %>%\n  bind_cols(predict(st, frog_test)) %>%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, \n              col = \"deeppink4\", \n              size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/05-feature-engineering.html#what-is-feature-engineering",
    "href": "slides/05-feature-engineering.html#what-is-feature-engineering",
    "title": "5 - Feature engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPCA feature extraction\n\n\n\n“Feature engineering” sounds pretty cool, but let’s take a minute to talk about preprocessing data."
  },
  {
    "objectID": "slides/05-feature-engineering.html#two-types-of-preprocessing",
    "href": "slides/05-feature-engineering.html#two-types-of-preprocessing",
    "title": "5 - Feature engineering",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "slides/05-feature-engineering.html#two-types-of-preprocessing-1",
    "href": "slides/05-feature-engineering.html#two-types-of-preprocessing-1",
    "title": "5 - Feature engineering",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "slides/05-feature-engineering.html#two-types-of-preprocessing-2",
    "href": "slides/05-feature-engineering.html#two-types-of-preprocessing-2",
    "title": "5 - Feature engineering",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing\n\nWhere does creation of dummy variables belong on the diagram?"
  },
  {
    "objectID": "slides/05-feature-engineering.html#two-types-of-preprocessing-3",
    "href": "slides/05-feature-engineering.html#two-types-of-preprocessing-3",
    "title": "5 - Feature engineering",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing\n\nWhere does engineering a date column belong in the diagram?\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n\nIt can be re-encoded as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays"
  },
  {
    "objectID": "slides/05-feature-engineering.html#original-column",
    "href": "slides/05-feature-engineering.html#original-column",
    "title": "5 - Feature engineering",
    "section": "Original column",
    "text": "Original column"
  },
  {
    "objectID": "slides/05-feature-engineering.html#features",
    "href": "slides/05-feature-engineering.html#features",
    "title": "5 - Feature engineering",
    "section": "Features",
    "text": "Features\n\n\n\n\n\n\nAt least that’s what we hope the difference looks like."
  },
  {
    "objectID": "slides/05-feature-engineering.html#general-definitions",
    "href": "slides/05-feature-engineering.html#general-definitions",
    "title": "5 - Feature engineering",
    "section": "General definitions",
    "text": "General definitions\n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "slides/05-feature-engineering.html#general-definitions-1",
    "href": "slides/05-feature-engineering.html#general-definitions-1",
    "title": "5 - Feature engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!"
  },
  {
    "objectID": "slides/05-feature-engineering.html#the-nhl-data",
    "href": "slides/05-feature-engineering.html#the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "The NHL data 🏒",
    "text": "The NHL data 🏒\n\nFrom Pittsburgh Penguins games, 12,147 shots on goal\nSeason from the 2015-2016 season\n\n\nLet’s predict whether a shot is on-goal (a goal or blocked by goaltender) or not."
  },
  {
    "objectID": "slides/05-feature-engineering.html#case-study",
    "href": "slides/05-feature-engineering.html#case-study",
    "title": "5 - Feature engineering",
    "section": "Case study",
    "text": "Case study\n\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nstr(season_2015)\n#> tibble [12,147 × 17] (S3: tbl_df/tbl/data.frame)\n#>  $ on_goal          : Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 2 2 1 2 1 2 ...\n#>  $ period           : int [1:12147] 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ period_type      : Factor w/ 3 levels \"overtime\",\"regular\",..: 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ coord_x          : num [1:12147] -53 68 -42 -77 -67 55 77 62 59 76 ...\n#>  $ coord_y          : num [1:12147] -18 -12 -18 9 -5 -12 13 14 -5 -6 ...\n#>  $ game_time        : num [1:12147] 0.3 0.9 1.25 1.78 2.05 ...\n#>  $ strength         : Factor w/ 4 levels \"even\",\"even_short_handed\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ player           : Factor w/ 640 levels \"aaron_ekblad\",..: 622 218 265 482 472 218 218 218 122 566 ...\n#>  $ player_diff      : num [1:12147] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ offense_team     : Factor w/ 31 levels \"ANA\",\"ARI\",\"BOS\",..: 26 23 26 26 26 23 23 23 23 23 ...\n#>  $ defense_team     : Factor w/ 31 levels \"ANA\",\"ARI\",\"BOS\",..: 23 26 23 23 23 26 26 26 26 26 ...\n#>  $ offense_goal_diff: num [1:12147] 0 0 0 0 0 0 0 0 0 0 ...\n#>  $ game_type        : Factor w/ 2 levels \"regular\",\"playoff\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ position         : Factor w/ 5 levels \"center\",\"defenseman\",..: 2 1 2 4 2 1 1 1 4 1 ...\n#>  $ dow              : Factor w/ 7 levels \"Sun\",\"Mon\",\"Tue\",..: 7 7 7 7 7 7 7 7 7 7 ...\n#>  $ month            : Factor w/ 12 levels \"Jan\",\"Feb\",\"Mar\",..: 5 5 5 5 5 5 5 5 5 5 ...\n#>  $ year             : num [1:12147] 2016 2016 2016 2016 2016 ..."
  },
  {
    "objectID": "slides/05-feature-engineering.html#splitting-the-nhl-data",
    "href": "slides/05-feature-engineering.html#splitting-the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "Splitting the NHL data ",
    "text": "Splitting the NHL data \n\nset.seed(23)\nnhl_split <- initial_split(season_2015, prop = 3/4)\nnhl_split\n#> <Training/Testing/Total>\n#> <9110/3037/12147>\n\nnhl_train <- training(nhl_split)\nnhl_test  <- testing(nhl_split)\n\nc(training = nrow(nhl_train), testing = nrow(nhl_test))\n#> training  testing \n#>     9110     3037"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn",
    "href": "slides/05-feature-engineering.html#your-turn",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nLet’s explore the training set data.\nUse the function plot_nhl_shots() for nice spatial plots of the data.\n\n\n\nset.seed(100)\nnhl_train %>% \n  sample_n(200) %>%\n  plot_nhl_shots(emphasis = position)\n\n\n\n\n\n\n\n\n15:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#validation-split",
    "href": "slides/05-feature-engineering.html#validation-split",
    "title": "5 - Feature engineering",
    "section": "Validation split ",
    "text": "Validation split \nSince there are a lot of observations, we’ll use a validation set:\n\nset.seed(234)\nnhl_val <- validation_split(nhl_train, prop = 0.80)\nnhl_val\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 × 2\n#>   splits              id        \n#>   <list>              <chr>     \n#> 1 <split [7288/1822]> validation\n\n\nRemember that a validation split is a type of resample."
  },
  {
    "objectID": "slides/05-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "slides/05-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "5 - Feature engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "slides/05-feature-engineering.html#a-first-recipe",
    "href": "slides/05-feature-engineering.html#a-first-recipe",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train)\n\n\n\nThe recipe() function assigns columns to roles of “outcome” or “predictor” using the formula\n\n\n\n\nIf the number of columns in the dataset is large, use recipe(data = nhl_train) instead"
  },
  {
    "objectID": "slides/05-feature-engineering.html#a-first-recipe-1",
    "href": "slides/05-feature-engineering.html#a-first-recipe-1",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(nhl_rec)\n#> # A tibble: 17 × 4\n#>    variable          type    role      source  \n#>    <chr>             <chr>   <chr>     <chr>   \n#>  1 period            numeric predictor original\n#>  2 period_type       nominal predictor original\n#>  3 coord_x           numeric predictor original\n#>  4 coord_y           numeric predictor original\n#>  5 game_time         numeric predictor original\n#>  6 strength          nominal predictor original\n#>  7 player            nominal predictor original\n#>  8 player_diff       numeric predictor original\n#>  9 offense_team      nominal predictor original\n#> 10 defense_team      nominal predictor original\n#> 11 offense_goal_diff numeric predictor original\n#> 12 game_type         nominal predictor original\n#> 13 position          nominal predictor original\n#> 14 dow               nominal predictor original\n#> 15 month             nominal predictor original\n#> 16 year              numeric predictor original\n#> 17 on_goal           nominal outcome   original"
  },
  {
    "objectID": "slides/05-feature-engineering.html#create-indicator-variables",
    "href": "slides/05-feature-engineering.html#create-indicator-variables",
    "title": "5 - Feature engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns."
  },
  {
    "objectID": "slides/05-feature-engineering.html#filter-out-constant-columns",
    "href": "slides/05-feature-engineering.html#filter-out-constant-columns",
    "title": "5 - Feature engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n\n\nIn case there is factor level that never was observed in the training data, we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of “predictor”"
  },
  {
    "objectID": "slides/05-feature-engineering.html#normalization",
    "href": "slides/05-feature-engineering.html#normalization",
    "title": "5 - Feature engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "slides/05-feature-engineering.html#reduce-correlation",
    "href": "slides/05-feature-engineering.html#reduce-correlation",
    "title": "5 - Feature engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum predictor set to remove to make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "slides/05-feature-engineering.html#other-possible-steps",
    "href": "slides/05-feature-engineering.html#other-possible-steps",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extraction…"
  },
  {
    "objectID": "slides/05-feature-engineering.html#other-possible-steps-1",
    "href": "slides/05-feature-engineering.html#other-possible-steps-1",
    "title": "5 - Feature engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nlibrary(embed)\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_umap(all_numeric_predictors(), outcome = on_goal)\n\n\nA fancy machine learning supervised dimension reduction technique…"
  },
  {
    "objectID": "slides/05-feature-engineering.html#other-possible-steps-2",
    "href": "slides/05-feature-engineering.html#other-possible-steps-2",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_ns(coord_y, coord_x, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-1",
    "href": "slides/05-feature-engineering.html#your-turn-1",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the on-goal data to :\n\ncreate dummy variables\nremove zero-variance variables\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#minimal-recipe",
    "href": "slides/05-feature-engineering.html#minimal-recipe",
    "title": "5 - Feature engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nnhl_indicators <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/05-feature-engineering.html#using-a-workflow",
    "href": "slides/05-feature-engineering.html#using-a-workflow",
    "title": "5 - Feature engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nnhl_glm_wflow <-\n  workflow() %>%\n  add_recipe(nhl_indicators) %>%\n  add_model(logistic_reg())\n \nctrl <- control_resamples(save_pred = TRUE)\nnhl_glm_res <-\n  nhl_glm_wflow %>%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#> # A tibble: 2 × 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.555     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.557     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-2",
    "href": "slides/05-feature-engineering.html#your-turn-2",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#holdout-predictions",
    "href": "slides/05-feature-engineering.html#holdout-predictions",
    "title": "5 - Feature engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nglm_val_pred <- collect_predictions(nhl_glm_res)\nglm_val_pred %>% slice(1:7)\n#> # A tibble: 7 × 7\n#>   id         .pred_yes .pred_no  .row .pred_class on_goal .config             \n#>   <chr>          <dbl>    <dbl> <int> <fct>       <fct>   <chr>               \n#> 1 validation     0.194 8.06e- 1    10 no          no      Preprocessor1_Model1\n#> 2 validation     0.259 7.41e- 1    17 no          yes     Preprocessor1_Model1\n#> 3 validation     0.185 8.15e- 1    23 no          no      Preprocessor1_Model1\n#> 4 validation     1.00  4.63e-12    40 yes         yes     Preprocessor1_Model1\n#> 5 validation     0.317 6.83e- 1    41 no          yes     Preprocessor1_Model1\n#> 6 validation     1.00  4.50e-12    46 yes         no      Preprocessor1_Model1\n#> 7 validation     0.348 6.52e- 1    55 no          no      Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-feature-engineering.html#two-class-data-1",
    "href": "slides/05-feature-engineering.html#two-class-data-1",
    "title": "5 - Feature engineering",
    "section": "Two class data",
    "text": "Two class data\nThese definitions assume that we know the threshold for class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity ⬇️, specificity ⬆️\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity ⬆️, specificity ⬇️"
  },
  {
    "objectID": "slides/05-feature-engineering.html#roc-curves",
    "href": "slides/05-feature-engineering.html#roc-curves",
    "title": "5 - Feature engineering",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 💯\nROC AUC = 1/2 😢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "slides/05-feature-engineering.html#roc-curves-1",
    "href": "slides/05-feature-engineering.html#roc-curves-1",
    "title": "5 - Feature engineering",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\nroc_curve_points <- glm_val_pred %>% roc_curve(truth = on_goal, .pred_yes)\nroc_curve_points %>% slice(1, 50, 100)\n#> # A tibble: 3 × 3\n#>   .threshold specificity sensitivity\n#>        <dbl>       <dbl>       <dbl>\n#> 1   -Inf          0            1    \n#> 2      0.139      0.0315       0.977\n#> 3      0.267      0.0642       0.954\n\nglm_val_pred %>% roc_auc(truth = on_goal, .pred_yes)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.557"
  },
  {
    "objectID": "slides/05-feature-engineering.html#roc-curve-plot",
    "href": "slides/05-feature-engineering.html#roc-curve-plot",
    "title": "5 - Feature engineering",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\nautoplot(roc_curve_points)"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-3",
    "href": "slides/05-feature-engineering.html#your-turn-3",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\n\n\n\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-4",
    "href": "slides/05-feature-engineering.html#your-turn-4",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nWhat data is being used for this ROC curve plot?\n\n\n\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "href": "slides/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "title": "5 - Feature engineering",
    "section": "What do we do with the player data? 🏒",
    "text": "What do we do with the player data? 🏒\nThere are 619 unique player values in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables 😳\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the player column with the estimated effect of that predictor\n\n\n\nLet’s use an effect encoding."
  },
  {
    "objectID": "slides/05-feature-engineering.html#per-player-statistics",
    "href": "slides/05-feature-engineering.html#per-player-statistics",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics",
    "text": "Per-player statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these rates use partial pooling.\nPooling borrows strength across players and shrinks extreme values (e.g. zero or one) towards the mean for players with very few shots.\nThe embed package has recipe steps for effect encodings."
  },
  {
    "objectID": "slides/05-feature-engineering.html#partial-pooling",
    "href": "slides/05-feature-engineering.html#partial-pooling",
    "title": "5 - Feature engineering",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "slides/05-feature-engineering.html#player-effects",
    "href": "slides/05-feature-engineering.html#player-effects",
    "title": "5 - Feature engineering",
    "section": "Player effects  ",
    "text": "Player effects  \n\nlibrary(embed)\n\nnhl_effect_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "slides/05-feature-engineering.html#recipes-are-estimated",
    "href": "slides/05-feature-engineering.html#recipes-are-estimated",
    "title": "5 - Feature engineering",
    "section": "Recipes are estimated ",
    "text": "Recipes are estimated \nPreprocessing steps in a recipe use the training set to compute quantities.\n\nWhat kind of quantities are computed for preprocessing?\n\nLevels of a factor\nWhether a column has zero variance\nNormalization\nFeature extraction\nEffect encodings\n\n\n\nWhen a recipe is part of a workflow, this estimation occurs when fit() is called."
  },
  {
    "objectID": "slides/05-feature-engineering.html#effect-encoding-results",
    "href": "slides/05-feature-engineering.html#effect-encoding-results",
    "title": "5 - Feature engineering",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nnhl_effect_wflow <-\n  nhl_glm_wflow %>%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res <-\n  nhl_effect_wflow %>%\n  fit_resamples(nhl_val)\n\ncollect_metrics(nhl_effect_res)\n#> # A tibble: 2 × 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.540     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.551     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-feature-engineering.html#section-1",
    "href": "slides/05-feature-engineering.html#section-1",
    "title": "5 - Feature engineering",
    "section": "",
    "text": "nhl_angle_rec <-\n  nhl_indicators %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi))\n  )\n\n\n\n\n\n\n\n\n\n\n\nNote the danger of using step_mutate() – easy to have data leakage"
  },
  {
    "objectID": "slides/05-feature-engineering.html#section-2",
    "href": "slides/05-feature-engineering.html#section-2",
    "title": "5 - Feature engineering",
    "section": "",
    "text": "nhl_distance_rec <-\n  nhl_angle_rec %>%\n  step_mutate(\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance)\n  )"
  },
  {
    "objectID": "slides/05-feature-engineering.html#fit-different-recipes",
    "href": "slides/05-feature-engineering.html#fit-different-recipes",
    "title": "5 - Feature engineering",
    "section": "Fit different recipes    ",
    "text": "Fit different recipes    \nA workflow set can cross models and/or preprocessors and then resample them en masse.\n\n\nnhl_glm_set_res <-\n  workflow_set(\n    list(dummy = nhl_indicators, encoded = nhl_effect_rec,\n         angle = nhl_angle_rec, dist = nhl_distance_rec, \n         bgl = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %>%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val, verbose = TRUE, control = ctrl)"
  },
  {
    "objectID": "slides/05-feature-engineering.html#your-turn-5",
    "href": "slides/05-feature-engineering.html#your-turn-5",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a workflow set with 2 or 3 recipes.\nUse workflow_map() to resample the workflow set.\n\n\n\n05:00"
  },
  {
    "objectID": "slides/05-feature-engineering.html#compare-recipes",
    "href": "slides/05-feature-engineering.html#compare-recipes",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %>%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point(size = 3) +\n  labs(y = NULL, x = \"ROC AUC (validation set)\")"
  },
  {
    "objectID": "slides/05-feature-engineering.html#compare-recipes-1",
    "href": "slides/05-feature-engineering.html#compare-recipes-1",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes"
  },
  {
    "objectID": "slides/05-feature-engineering.html#debugging-a-recipe",
    "href": "slides/05-feature-engineering.html#debugging-a-recipe",
    "title": "5 - Feature engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g. encoded_players) can be estimated manually with a function called prep(). It is analogous to fit().\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "slides/05-feature-engineering.html#more-on-recipes",
    "href": "slides/05-feature-engineering.html#more-on-recipes",
    "title": "5 - Feature engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters.\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#tuning-parameters",
    "title": "6 - Tuning hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "6 - Tuning hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "slides/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search 💠 which tests a pre-defined set of candidate values\nIterative search 🌀 which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "slides/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "6 - Tuning hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet’s take our previous recipe and add a few changes:\n\nglm_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  ) %>%\n  step_rm(coord_x, coord_y) %>%\n  step_zv(all_predictors()) %>%\n  step_ns(angle, deg_free = tune(\"angle\")) %>%\n  step_ns(distance, deg_free = tune(\"distance\")) %>%\n  step_normalize(all_numeric_predictors())\nglm_spline_wflow <-\n  workflow() %>%\n  add_model(logistic_reg()) %>%\n  add_recipe(glm_rec)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#grid-search",
    "href": "slides/06-tuning-hyperparameters.html#grid-search",
    "title": "6 - Tuning hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nParameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc)\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info\n\n\n\nGrids\n\nCreate your grid manually or automatically\nThe grid_*() functions can make a grid\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#create-a-grid",
    "href": "slides/06-tuning-hyperparameters.html#create-a-grid",
    "title": "6 - Tuning hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nglm_spline_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>       angle deg_free nparam[+]\n#>    distance deg_free nparam[+]\n\n\nA parameter set can be updated (e.g. to change the ranges)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#create-a-grid-1",
    "href": "slides/06-tuning-hyperparameters.html#create-a-grid-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 23 × 2\n#>    angle distance\n#>    <int>    <int>\n#>  1    12        4\n#>  2    15        8\n#>  3     6       14\n#>  4    10        5\n#>  5    12       12\n#>  6     7        8\n#>  7    14        3\n#>  8    14       13\n#>  9    11       12\n#> 10     8       11\n#> # … with 13 more rows\n\n\n\n\nA space-filling design like this tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn",
    "href": "slides/06-tuning-hyperparameters.html#your-turn",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#create-a-grid-2",
    "href": "slides/06-tuning-hyperparameters.html#create-a-grid-2",
    "title": "6 - Tuning hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 25)\n\ngrid\n#> # A tibble: 225 × 2\n#>    angle distance\n#>    <int>    <int>\n#>  1     1        1\n#>  2     2        1\n#>  3     3        1\n#>  4     4        1\n#>  5     5        1\n#>  6     6        1\n#>  7     7        1\n#>  8     8        1\n#>  9     9        1\n#> 10    10        1\n#> # … with 215 more rows"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "slides/06-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "6 - Tuning hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(angle = spline_degree(c(2L, 20L)),\n         distance = spline_degree(c(2L, 20L))) %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 24 × 2\n#>    angle distance\n#>    <int>    <int>\n#>  1    16        6\n#>  2    20       11\n#>  3     8       19\n#>  4    14        7\n#>  5    16       17\n#>  6    10       11\n#>  7    19        5\n#>  8    18       17\n#>  9    15       16\n#> 10    11       15\n#> # … with 14 more rows"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#the-results",
    "href": "slides/06-tuning-hyperparameters.html#the-results",
    "title": "6 - Tuning hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %>% \n  ggplot(aes(angle, distance)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#grid-search-1",
    "href": "slides/06-tuning-hyperparameters.html#grid-search-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Grid search   ",
    "text": "Grid search   \n\nctrl <- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nset.seed(9)\nglm_spline_res <-\n  glm_spline_wflow %>%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\nglm_spline_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 × 5\n#>   splits              id         .metrics          .notes            .predictions         \n#>   <list>              <chr>      <list>            <list>            <list>               \n#> 1 <split [7288/1822]> validation <tibble [48 × 6]> <tibble [24 × 3]> <tibble [43,728 × 8]>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x24: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-1",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTune our glm_flow.\nWhat happens if you change grid to use the default?\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#grid-results",
    "href": "slides/06-tuning-hyperparameters.html#grid-results",
    "title": "6 - Tuning hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(glm_spline_res)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results",
    "title": "6 - Tuning hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res)\n#> # A tibble: 48 × 8\n#>    angle distance .metric  .estimator  mean     n std_err .config              \n#>    <int>    <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1    16        6 accuracy binary     0.610     1      NA Preprocessor01_Model1\n#>  2    16        6 roc_auc  binary     0.649     1      NA Preprocessor01_Model1\n#>  3    20       11 accuracy binary     0.613     1      NA Preprocessor02_Model1\n#>  4    20       11 roc_auc  binary     0.649     1      NA Preprocessor02_Model1\n#>  5     8       19 accuracy binary     0.619     1      NA Preprocessor03_Model1\n#>  6     8       19 roc_auc  binary     0.652     1      NA Preprocessor03_Model1\n#>  7    14        7 accuracy binary     0.610     1      NA Preprocessor04_Model1\n#>  8    14        7 roc_auc  binary     0.650     1      NA Preprocessor04_Model1\n#>  9    16       17 accuracy binary     0.617     1      NA Preprocessor05_Model1\n#> 10    16       17 roc_auc  binary     0.649     1      NA Preprocessor05_Model1\n#> # … with 38 more rows"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results-1",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#> # A tibble: 48 × 7\n#>    id         angle distance .metric  .estimator .estimate .config              \n#>    <chr>      <int>    <int> <chr>    <chr>          <dbl> <chr>                \n#>  1 validation    16        6 accuracy binary         0.610 Preprocessor01_Model1\n#>  2 validation    16        6 roc_auc  binary         0.649 Preprocessor01_Model1\n#>  3 validation    20       11 accuracy binary         0.613 Preprocessor02_Model1\n#>  4 validation    20       11 roc_auc  binary         0.649 Preprocessor02_Model1\n#>  5 validation     8       19 accuracy binary         0.619 Preprocessor03_Model1\n#>  6 validation     8       19 roc_auc  binary         0.652 Preprocessor03_Model1\n#>  7 validation    14        7 accuracy binary         0.610 Preprocessor04_Model1\n#>  8 validation    14        7 roc_auc  binary         0.650 Preprocessor04_Model1\n#>  9 validation    16       17 accuracy binary         0.617 Preprocessor05_Model1\n#> 10 validation    16       17 roc_auc  binary         0.649 Preprocessor05_Model1\n#> # … with 38 more rows"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "6 - Tuning hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 5 × 8\n#>   angle distance .metric .estimator  mean     n std_err .config              \n#>   <int>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1     5        8 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n#> 2     6        9 roc_auc binary     0.653     1      NA Preprocessor17_Model1\n#> 3     3       15 roc_auc binary     0.653     1      NA Preprocessor13_Model1\n#> 4     3       13 roc_auc binary     0.652     1      NA Preprocessor11_Model1\n#> 5     5       19 roc_auc binary     0.652     1      NA Preprocessor24_Model1"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 × 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\n\n\nThis best result has:\n\nlow-degree spline for angle (less “wiggly”, less complex)\nhigher-degree spline for distance (more “wiggly”, more complex)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-2",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-2",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTry an alternative selection strategy.\nRead the docs for select_by_pct_loss().\nTry choosing a model that has a simpler (less “wiggly”) relationship for distance.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination-2",
    "href": "slides/06-tuning-hyperparameters.html#choose-a-parameter-combination-2",
    "title": "6 - Tuning hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 × 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\nselect_by_pct_loss(glm_spline_res, distance, metric = \"roc_auc\")\n#> # A tibble: 1 × 10\n#>   angle distance .metric .estimator  mean     n std_err .config               .best .loss\n#>   <int>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 <dbl> <dbl>\n#> 1    13        4 roc_auc binary     0.646     1      NA Preprocessor20_Model1 0.653 0.984"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-trees-1",
    "href": "slides/06-tuning-hyperparameters.html#boosted-trees-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Boosted trees 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵",
    "text": "Boosted trees 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵\n\nEnsemble many decision tree models\n\n\n\nReview how a decision tree model works:\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#single-decision-tree",
    "href": "slides/06-tuning-hyperparameters.html#single-decision-tree",
    "title": "6 - Tuning hyperparameters",
    "section": "Single decision tree",
    "text": "Single decision tree"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-trees-2",
    "href": "slides/06-tuning-hyperparameters.html#boosted-trees-2",
    "title": "6 - Tuning hyperparameters",
    "section": "Boosted trees 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵",
    "text": "Boosted trees 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵\nBoosting methods fit a sequence of tree-based models.\n\n\nEach tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\nThis is like gradient-based steepest ascent methods from calculus."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-trees-3",
    "href": "slides/06-tuning-hyperparameters.html#boosted-trees-3",
    "title": "6 - Tuning hyperparameters",
    "section": "Boosted trees 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵",
    "text": "Boosted trees 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵\nMost modern boosting methods have a lot of tuning parameters!\n\n\nFor tree growth and pruning (min_n, max_depth, etc)\nFor boosting (trees, stop_iter, learn_rate)\n\n\n\nWe’ll use early stopping to stop boosting when a few iterations produce consecutively worse results."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#boosted-trees-4",
    "href": "slides/06-tuning-hyperparameters.html#boosted-trees-4",
    "title": "6 - Tuning hyperparameters",
    "section": "Boosted trees    ",
    "text": "Boosted trees    \n\nxgb_spec <-\n  boost_tree(\n    trees = 500, min_n = tune(), stop_iter = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %>%\n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\", validation = 1/10) # <- for better early stopping\n\nxgb_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_lencode_mixed(player, outcome = vars(on_goal)) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nxgb_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(xgb_rec)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-3",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-3",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate your boosted tree workflow.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#running-in-parallel",
    "href": "slides/06-tuning-hyperparameters.html#running-in-parallel",
    "title": "6 - Tuning hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models don’t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores <- parallel::detectCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "slides/06-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "6 - Tuning hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning",
    "href": "slides/06-tuning-hyperparameters.html#tuning",
    "title": "6 - Tuning hyperparameters",
    "section": "Tuning ",
    "text": "Tuning \nThis will take some time to run ⏳\n\nset.seed(9)\nxgb_res <-\n  xgb_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 20, control = ctrl) # automatic grid now!\nxgb_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 × 5\n#>   splits              id         .metrics          .notes           .predictions          \n#>   <list>              <chr>      <list>            <list>           <list>                \n#> 1 <split [7288/1822]> validation <tibble [40 × 9]> <tibble [0 × 3]> <tibble [36,440 × 11]>"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-4",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-4",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nStart tuning the boosted tree model!\nWe won’t wait for everyone’s tuning to finish, but take this time to get it started before we move on.\n\n\n\n03:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#tuning-results-2",
    "href": "slides/06-tuning-hyperparameters.html#tuning-results-2",
    "title": "6 - Tuning hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nautoplot(xgb_res)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#again-with-the-location-features",
    "href": "slides/06-tuning-hyperparameters.html#again-with-the-location-features",
    "title": "6 - Tuning hyperparameters",
    "section": "Again with the location features",
    "text": "Again with the location features\n\ncoord_rec <- \n  xgb_rec %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  ) %>% \n  step_rm(coord_x, coord_y)\n\nxgb_coord_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res <-\n  xgb_coord_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 20, control = ctrl)"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "href": "slides/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "title": "6 - Tuning hyperparameters",
    "section": "Did the machine figure it out?",
    "text": "Did the machine figure it out?\n\nshow_best(xgb_res, metric = \"roc_auc\")\n#> # A tibble: 5 × 11\n#>   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#>   <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    31         14     0.0174  0.00000231           15 roc_auc binary     0.632     1      NA Preprocessor1_Model15\n#> 2    30         13     0.0578  0.00000000141        18 roc_auc binary     0.630     1      NA Preprocessor1_Model07\n#> 3    14          2     0.146   0.00244              19 roc_auc binary     0.630     1      NA Preprocessor1_Model14\n#> 4    12          7     0.191   0.351                 7 roc_auc binary     0.629     1      NA Preprocessor1_Model09\n#> 5    26         15     0.0365  2.51                 17 roc_auc binary     0.629     1      NA Preprocessor1_Model11\n\nshow_best(xgb_coord_res, metric = \"roc_auc\")\n#> # A tibble: 5 × 11\n#>   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#>   <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    30         13     0.0578  0.00000000141        18 roc_auc binary     0.648     1      NA Preprocessor1_Model07\n#> 2    39         12     0.0803  0.000411             10 roc_auc binary     0.643     1      NA Preprocessor1_Model12\n#> 3    14          2     0.146   0.00244              19 roc_auc binary     0.642     1      NA Preprocessor1_Model14\n#> 4    26         15     0.0365  2.51                 17 roc_auc binary     0.642     1      NA Preprocessor1_Model11\n#> 5    35          5     0.101   0.0000000784         13 roc_auc binary     0.641     1      NA Preprocessor1_Model17"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#compare-models",
    "href": "slides/06-tuning-hyperparameters.html#compare-models",
    "title": "6 - Tuning hyperparameters",
    "section": "Compare models",
    "text": "Compare models\nBest logistic regression results:\n\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 × 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n\n\nBest boosting results:\n\nxgb_coord_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 × 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.648     1      NA Preprocessor1_Model07"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-5",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-5",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCan you get better ROC results?\nTry increasing tree_depth beyond the original range.\n\n\n\n20:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#updating-the-workflow",
    "href": "slides/06-tuning-hyperparameters.html#updating-the-workflow",
    "title": "6 - Tuning hyperparameters",
    "section": "Updating the workflow  ",
    "text": "Updating the workflow  \n\nbest_auc <- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#> # A tibble: 1 × 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\n\nglm_spline_wflow <-\n  glm_spline_wflow %>% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> 8 Recipe Steps\n#> \n#> • step_lencode_mixed()\n#> • step_dummy()\n#> • step_mutate()\n#> • step_rm()\n#> • step_zv()\n#> • step_ns()\n#> • step_ns()\n#> • step_normalize()\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#the-final-fit",
    "href": "slides/06-tuning-hyperparameters.html#the-final-fit",
    "title": "6 - Tuning hyperparameters",
    "section": "The final fit  ",
    "text": "The final fit  \n\ntest_res <- \n  glm_spline_wflow %>% \n  last_fit(split = nhl_split)\ntest_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 × 6\n#>   splits              id               .metrics         .notes           .predictions         .workflow \n#>   <list>              <chr>            <list>           <list>           <list>               <list>    \n#> 1 <split [9110/3037]> train/test split <tibble [2 × 4]> <tibble [1 × 3]> <tibble [3,037 × 6]> <workflow>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n\n\nRemember that last_fit() fits one time with the non-training set and evaluates one time with the testing set."
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#your-turn-6",
    "href": "slides/06-tuning-hyperparameters.html#your-turn-6",
    "title": "6 - Tuning hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nFinalize your workflow with the best parameters.\nCreate a final fit.\n\n\n\n20:00"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "href": "slides/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "title": "6 - Tuning hyperparameters",
    "section": "Estimates of ROC AUC ",
    "text": "Estimates of ROC AUC \nValidation results from tuning:\n\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, mean, n, std_err)\n#> # A tibble: 1 × 4\n#>   .metric  mean     n std_err\n#>   <chr>   <dbl> <int>   <dbl>\n#> 1 roc_auc 0.653     1      NA\n\n\nTest set results:\n\ntest_res %>% collect_metrics()\n#> # A tibble: 2 × 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.616 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.656 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#final-fitted-workflow",
    "href": "slides/06-tuning-hyperparameters.html#final-fitted-workflow",
    "title": "6 - Tuning hyperparameters",
    "section": "Final fitted workflow",
    "text": "Final fitted workflow\nExtract the final fitted workflow, fit using the training set:\n\nfinal_glm_spline_wflow <- \n  test_res %>% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#> # A tibble: 3 × 1\n#>   .pred_class\n#>   <fct>      \n#> 1 no         \n#> 2 yes        \n#> 3 no"
  },
  {
    "objectID": "slides/06-tuning-hyperparameters.html#next-steps",
    "href": "slides/06-tuning-hyperparameters.html#next-steps",
    "title": "6 - Tuning hyperparameters",
    "section": "Next steps",
    "text": "Next steps\n\nUse explainers to characterize the model and the predictions.\n\n\n\nDocument the model.\n\n\n\n\nCreate an applicability domain model to help monitor our data over time.\n\n\n\n\nDeploy the model.\n\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/07-wrapping-up.html#resources-to-keep-learning",
    "href": "slides/07-wrapping-up.html#resources-to-keep-learning",
    "title": "7 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Twitter and at the tidyverse blog for updates!"
  }
]