{
  "hash": "3249c8486bcd82d9350779ea680f2fd1",
  "result": {
    "markdown": "---\ntitle: \"2 - Tuning Hyperparameters - Classwork\"\nsubtitle: \"Advanced tidymodels\"\neditor_options: \n  chunk_output_type: console\n---\n\n\nWe recommend restarting R between each slide deck!\n\n## Setup\n\nSetup from deck 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.5          ✔ recipes      1.0.6     \n✔ dials        1.2.0          ✔ rsample      1.1.1.9000\n✔ dplyr        1.1.2          ✔ tibble       3.2.1     \n✔ ggplot2      3.4.2          ✔ tidyr        1.3.0     \n✔ infer        1.0.4          ✔ tune         1.1.1.9001\n✔ modeldata    1.1.0          ✔ workflows    1.1.3     \n✔ parsnip      1.1.0.9003     ✔ workflowsets 1.0.1     \n✔ purrr        1.0.1          ✔ yardstick    1.2.0.9001\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n```\n:::\n\n```{.r .cell-code}\nlibrary(modeldatatoo)\nlibrary(textrecipes)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics <- metric_set(mae, rsq)\n\nset.seed(295)\nhotel_rates <- \n  data_hotel_rates() %>% \n  sample_n(5000) %>% \n  arrange(arrival_date) %>% \n  select(-arrival_date_num, -arrival_date) %>% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n\nset.seed(4028)\nhotel_split <-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr <- training(hotel_split)\nhotel_te <- testing(hotel_split)\n\nset.seed(472)\nhotel_rs <- vfold_cv(hotel_tr, strata = avg_price_per_room)\n\nhash_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %>%\n  step_YeoJohnson(lead_time) %>%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %>%\n  step_dummy_hash(company) %>%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n```\n:::\n\n\n## Tagging parameters for tuning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhash_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %>%\n  step_YeoJohnson(lead_time) %>%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %>%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %>%\n  step_zv(all_predictors())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bonsai)\n\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow <- workflow(hash_rec, lgbm_spec)\n```\n:::\n\n\n## Create a grid \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\ngrid <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 × 4\n   trees    learn_rate `agent hash` `company hash`\n   <int>         <dbl>        <int>          <int>\n 1  1629 0.00000440             524           1454\n 2  1746 0.0000000751          1009           2865\n 3    53 0.0000180             2313            367\n 4   442 0.000000445            347            460\n 5  1413 0.0000000208          3232            553\n 6  1488 0.0000578             3692            639\n 7   906 0.000385               602            332\n 8  1884 0.00000000101         1127            567\n 9  1812 0.0239                 961           1183\n10   393 0.000000117            487           1783\n# ℹ 15 more rows\n```\n:::\n:::\n\n\n## Your turn \n\nCreate a grid for our tunable workflow.\n\nTry creating a regular grid.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n\n\n## Your turn \n\nWhat advantage would a regular grid have? \n\nDiscuss with your neighbor!\n\n## Update parameter ranges \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_param <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\ngrid <- \n  lgbm_param %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 × 4\n   trees learn_rate `agent hash` `company hash`\n   <int>      <dbl>        <int>          <int>\n 1    75  0.000312          2991           1250\n 2     4  0.0000337          899           3088\n 3    15  0.0295             520           1578\n 4     8  0.0997            1256           3592\n 5    80  0.000622           419            258\n 6    70  0.000474          2499           1089\n 7    35  0.000165           287           2376\n 8    64  0.00137            389            359\n 9    58  0.0000250          616            881\n10    84  0.0639            2311           2635\n# ℹ 15 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid %>% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](advanced-02-classwork_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Grid Search\n\nLet's take our previous model and tune more parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow <- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param <-\n  lgbm_wflow %>%\n  extract_parameter_set_dials() %>%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))\n```\n:::\n\n\nRun the grid search:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE)\n\nlgbm_res <-\n  lgbm_wflow %>%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n'as(<dgTMatrix>, \"dgCMatrix\")' is deprecated.\nUse 'as(., \"CsparseMatrix\")' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n```\n:::\n\n```{.r .cell-code}\nlgbm_res \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 5\n   splits             id     .metrics          .notes           .predictions\n   <list>             <chr>  <list>            <list>           <list>      \n 1 <split [3372/377]> Fold01 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 2 <split [3373/376]> Fold02 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 3 <split [3373/376]> Fold03 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 4 <split [3373/376]> Fold04 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 5 <split [3373/376]> Fold05 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 6 <split [3374/375]> Fold06 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 7 <split [3375/374]> Fold07 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 8 <split [3376/373]> Fold08 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n 9 <split [3376/373]> Fold09 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n10 <split [3376/373]> Fold10 <tibble [50 × 9]> <tibble [0 × 3]> <tibble>    \n```\n:::\n:::\n\n\nInspect results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lgbm_res)\n```\n\n::: {.cell-output-display}\n![](advanced-02-classwork_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncollect_metrics(lgbm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 11\n   trees min_n learn_rate `agent hash` `company hash` .metric .estimator   mean\n   <int> <int>      <dbl>        <int>          <int> <chr>   <chr>       <dbl>\n 1   298    19   4.15e- 9          222             36 mae     standard   53.2  \n 2   298    19   4.15e- 9          222             36 rsq     standard    0.811\n 3  1394     5   5.82e- 6           28             21 mae     standard   52.9  \n 4  1394     5   5.82e- 6           28             21 rsq     standard    0.810\n 5   774    12   4.41e- 2           27             95 mae     standard   10.5  \n 6   774    12   4.41e- 2           27             95 rsq     standard    0.939\n 7  1342     7   6.84e-10           71             17 mae     standard   53.2  \n 8  1342     7   6.84e-10           71             17 rsq     standard    0.810\n 9   669    39   8.62e- 7          141            145 mae     standard   53.2  \n10   669    39   8.62e- 7          141            145 rsq     standard    0.808\n# ℹ 40 more rows\n# ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n\n```{.r .cell-code}\ncollect_metrics(lgbm_res, summarize = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 10\n   id     trees min_n  learn_rate `agent hash` `company hash` .metric .estimator\n   <chr>  <int> <int>       <dbl>        <int>          <int> <chr>   <chr>     \n 1 Fold01   298    19     4.15e-9          222             36 mae     standard  \n 2 Fold01   298    19     4.15e-9          222             36 rsq     standard  \n 3 Fold02   298    19     4.15e-9          222             36 mae     standard  \n 4 Fold02   298    19     4.15e-9          222             36 rsq     standard  \n 5 Fold03   298    19     4.15e-9          222             36 mae     standard  \n 6 Fold03   298    19     4.15e-9          222             36 rsq     standard  \n 7 Fold04   298    19     4.15e-9          222             36 mae     standard  \n 8 Fold04   298    19     4.15e-9          222             36 rsq     standard  \n 9 Fold05   298    19     4.15e-9          222             36 mae     standard  \n10 Fold05   298    19     4.15e-9          222             36 rsq     standard  \n# ℹ 490 more rows\n# ℹ 2 more variables: .estimate <dbl>, .config <chr>\n```\n:::\n:::\n\n\n## Choose a parameter combination\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(lgbm_res, metric = \"rsq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 11\n  trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean\n  <int> <int>      <dbl>        <int>          <int> <chr>   <chr>      <dbl>\n1  1890    10    0.0159           115            174 rsq     standard   0.940\n2   774    12    0.0441            27             95 rsq     standard   0.939\n3  1638    36    0.0409            15            120 rsq     standard   0.938\n4   963    23    0.00556          157             13 rsq     standard   0.930\n5   590     5    0.00320           85             73 rsq     standard   0.905\n# ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n\n```{.r .cell-code}\nlgbm_best <- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  trees min_n learn_rate `agent hash` `company hash` .config              \n  <int> <int>      <dbl>        <int>          <int> <chr>                \n1   774    12     0.0441           27             95 Preprocessor03_Model1\n```\n:::\n:::\n\n\n## Checking Calibration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(probably)\n\nlgbm_res %>%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %>%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred,\n    alpha = 1 / 3\n  )\n```\n\n::: {.cell-output-display}\n![](advanced-02-classwork_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Running in parallel\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncores <- parallelly::availableCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)\n```\n:::\n\n\n## Your turn\n\nTry early stopping: Set `trees = 2000` and tune the `stop_iter` parameter!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n",
    "supporting": [
      "advanced-02-classwork_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}