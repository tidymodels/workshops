{
  "hash": "c1fa12ef361dcce19074fb247be50d03",
  "result": {
    "markdown": "---\ntitle: \"4 - Iterative Search - Classwork\"\nsubtitle: \"Advanced tidymodels\"\neditor_options: \n  chunk_output_type: console\n---\n\n\nWe recommend restarting R between each slide deck!\n\n## Setup\n\nSetup from deck 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.5          ✔ recipes      1.0.6     \n✔ dials        1.2.0          ✔ rsample      1.1.1.9000\n✔ dplyr        1.1.2          ✔ tibble       3.2.1     \n✔ ggplot2      3.4.2          ✔ tidyr        1.3.0     \n✔ infer        1.0.4          ✔ tune         1.1.1.9001\n✔ modeldata    1.1.0          ✔ workflows    1.1.3     \n✔ parsnip      1.1.0.9003     ✔ workflowsets 1.0.1     \n✔ purrr        1.0.1          ✔ yardstick    1.2.0.9001\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n\n```{.r .cell-code}\nlibrary(modeldatatoo)\nlibrary(textrecipes)\nlibrary(bonsai)\nlibrary(probably)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'probably'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n```\n:::\n\n```{.r .cell-code}\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nset.seed(295)\nhotel_rates <- \n  data_hotel_rates() %>% \n  sample_n(5000) %>% \n  arrange(arrival_date) %>% \n  select(-arrival_date_num, -arrival_date) %>% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n\nset.seed(4028)\nhotel_split <-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr <- training(hotel_split)\nhotel_te <- testing(hotel_split)\n\nset.seed(472)\nhotel_rs <- vfold_cv(hotel_tr, strata = avg_price_per_room)\n\nhotel_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %>%\n  step_YeoJohnson(lead_time) %>%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %>%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %>%\n  step_zv(all_predictors())\n\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow <- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param <-\n  lgbm_wflow %>%\n  extract_parameter_set_dials() %>%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))\n```\n:::\n\n\n## Your turn\n\nYour GP makes predictions on two new candidate tuning parameters. We want to minimize MAE.\n\nWhich should we choose?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(28383)\nnum_points <- 5000\nexerc_data <- \n  tibble(MAE = c(rnorm(num_points, 10, 2), rnorm(num_points, 13, 1 / 2)),\n         `Choose:` = rep(paste(\"candidate\", 1:2), each = num_points))\n\nexerc_data %>% \n  ggplot(aes(MAE, col = `Choose:`)) + \n  geom_line(stat = \"density\", adjust = 1.25, trim = TRUE, linewidth = 1) +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](advanced-04-classwork_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## An Initial Grid\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_metrics <- metric_set(mae, rsq)\n\nset.seed(12)\ninit_res <-\n  lgbm_wflow %>%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = nrow(lgbm_param) + 2,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n'as(<dgTMatrix>, \"dgCMatrix\")' is deprecated.\nUse 'as(., \"CsparseMatrix\")' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n```\n:::\n\n```{.r .cell-code}\nshow_best(init_res, metric = \"mae\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 11\n  trees min_n   learn_rate `agent hash` `company hash` .metric .estimator  mean\n  <int> <int>        <dbl>        <int>          <int> <chr>   <chr>      <dbl>\n1   390    10 0.0139                 13             62 mae     standard    11.9\n2   718    31 0.00112                72             25 mae     standard    29.1\n3  1236    22 0.0000261              11             17 mae     standard    51.8\n4  1044    25 0.00000832             34             12 mae     standard    52.8\n5  1599     7 0.0000000402          254            179 mae     standard    53.2\n# ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n\n## Bayesian Optimization \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(15)\nlgbm_bayes_res <-\n  lgbm_wflow %>%\n  tune_bayes(\n    resamples = hotel_rs,\n    initial = init_res,     # <- initial results\n    iter = 20,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(lgbm_bayes_res, metric = \"mae\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 12\n  trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean\n  <int> <int>      <dbl>        <int>          <int> <chr>   <chr>      <dbl>\n1  1665     2     0.0593           12             59 mae     standard    10.1\n2  1179     2     0.0552          161            121 mae     standard    10.2\n3  1609     6     0.0592          186            192 mae     standard    10.2\n4  1352     6     0.0799          217             46 mae     standard    10.3\n5  1647     4     0.0819           12            240 mae     standard    10.3\n# ℹ 4 more variables: n <int>, std_err <dbl>, .config <chr>, .iter <int>\n```\n:::\n:::\n\n\nPlotting results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\")\n```\n\n::: {.cell-output-display}\n![](advanced-04-classwork_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](advanced-04-classwork_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](advanced-04-classwork_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\n## Your turn\n\nLet's try a different acquisition function: `conf_bound(kappa)`.\n\nWe'll use the `objective` argument to set it.\n\nChoose your own `kappa` value:\n\n- Larger values will explore the space more.\n- \"Large\" values are usually less than one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n\n\n## Finalize the workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(lgbm_bayes_res, metric = \"mae\")\n\nfinal_wflow <- \n  lgbm_wflow %>% \n  finalize_workflow(best_param)\n\nfinal_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_YeoJohnson()\n• step_dummy_hash()\n• step_dummy_hash()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1665\n  min_n = 2\n  learn_rate = 0.0592557571004946\n\nComputational engine: lightgbm \n```\n:::\n:::\n\n\n## The Final Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3893)\nfinal_res <- final_wflow %>% last_fit(hotel_split, metrics = reg_metrics)\n\nfinal_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits              id               .metrics .notes   .predictions .workflow \n  <list>              <chr>            <list>   <list>   <list>       <list>    \n1 <split [3749/1251]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n:::\n\n\n## Test Set Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>% \n  collect_predictions() %>% \n  cal_plot_regression(\n    truth = avg_price_per_room, \n    estimate = .pred, \n    alpha = 1 / 4)\n```\n\n::: {.cell-output-display}\n![](advanced-04-classwork_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 mae     standard      10.5   Preprocessor1_Model1\n2 rsq     standard       0.937 Preprocessor1_Model1\n```\n:::\n:::\n",
    "supporting": [
      "advanced-04-classwork_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}