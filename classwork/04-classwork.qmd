---
title: "4 - Evaluating models - Classwork"
subtitle: "Machine learning with tidymodels"
editor_options: 
  chunk_output_type: console
---

We recommend restarting R between each slide deck!

## Setup

Setup from deck 3

```{r}
library(tidymodels)

data("tree_frogs", package = "stacks")

tree_frogs <- tree_frogs %>%
  mutate(t_o_d = factor(t_o_d)) %>% 
  filter(!is.na(latency)) %>%
  select(-c(clutch, hatched))

set.seed(123)

frog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)
frog_train <- training(frog_split)
frog_test <- testing(frog_split)

tree_spec <- decision_tree(cost_complexity = 0.001, mode = "regression")
tree_wflow <- workflow(latency ~ ., tree_spec)
tree_fit <- fit(tree_wflow, frog_train)
```

## Metrics for model performance

`metrics()` returns a standard set of metrics

```{r}
augment(tree_fit, new_data = frog_test) %>%
  metrics(latency, .pred)
```

Or you can use individual metric functions

```{r}
augment(tree_fit, new_data = frog_test) %>%
  rmse(latency, .pred)
```

All yardstick metric functions work with grouped data frames!

```{r}
augment(tree_fit, new_data = frog_test) %>%
  group_by(reflex) %>%
  rmse(latency, .pred)
```

Metric sets are a way to combine multiple similar metric functions together into a new function.

```{r}
frog_metrics <- metric_set(rmse, msd)

augment(tree_fit, new_data = frog_test) %>%
  frog_metrics(latency, .pred)
```

## Dangers of overfitting

Repredicting the training set, bad!

```{r}
tree_fit %>%
  augment(frog_train)
```

"Resubstitution estimate" - This should be the best possible performance that you could ever achieve, but it can be very misleading!

```{r}
tree_fit %>%
  augment(frog_train) %>%
  rmse(latency, .pred)
```

Now on the test set, see that it performs worse? This is closer to "real" performance.

```{r}
tree_fit %>%
  augment(frog_test) %>%
  rmse(latency, .pred)
```

## Your turn

Use `augment()` and `metrics()` to compute a regression metric like `mae()`.

Compute the metrics for both training and testing data.

Notice the evidence of overfitting! ⚠️

```{r}
# Your code here!

# Use `augment()` and `metrics()` with `tree_fit`
tree_fit
```

## Your turn

If we use 10 folds, what percent of the training data:

- ends up in analysis?
- ends up in assessment?

for each fold

## Resampling

```{r}
# v = 10 is the default
vfold_cv(frog_train)
```

What is in a resampling result?

```{r}
frog_folds <- vfold_cv(frog_train, v = 10)

# Individual splits of analysis/assessment data
frog_folds$splits[1:3]
```

Stratification often helps, with very little downside

```{r}
vfold_cv(frog_train, strata = latency)
```

We'll use this setup:

```{r}
set.seed(123)
frog_folds <- vfold_cv(frog_train, v = 10, strata = latency)
frog_folds
```

## Evaluating model performance

```{r}
# Fit the workflow on each analysis set,
# then compute performance on each assessment set
tree_res <- fit_resamples(tree_wflow, frog_folds)
tree_res
```

Aggregate metrics

```{r}
tree_res %>%
  collect_metrics()
```

If you want to analyze the assessment set (i.e. holdout) predictions, then you need to adjust the control object and tell it to save them:

```{r}
# Save the assessment set results
ctrl_frog <- control_resamples(save_pred = TRUE)

tree_res <- fit_resamples(tree_wflow, frog_folds, control = ctrl_frog)

tree_preds <- collect_predictions(tree_res)
tree_preds
```

```{r}
tree_preds %>% 
  ggplot(aes(latency, .pred, color = id)) + 
  geom_abline(lty = 2, col = "gray", size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```

## Bootstrapping

```{r}
set.seed(3214)
bootstraps(frog_train)
```

## Your turn

Create:

- Bootstrap folds (change `times` from its default!)
- A validation resample (what function is used for this?)

https://rsample.tidymodels.org/reference/index.html

Don't forget to set a seed when you resample!

```{r}
# Your code here!

```

## Create a random forest model

```{r}
rf_spec <- rand_forest(trees = 1000, mode = "regression")
rf_spec
```

```{r}
rf_wflow <- workflow(latency ~ ., rf_spec)
rf_wflow
```

## Your turn

Use `fit_resamples()` and `rf_wflow` to:

- Keep predictions
- Compute metrics
- Plot true vs predicted values

```{r}
# Your code here!

```

## Evaluate a workflow set

```{r}
wf_set <- workflow_set(list(latency ~ .), list(tree_spec, rf_spec))
wf_set
```

```{r}
wf_set_fit <- wf_set %>%
  workflow_map("fit_resamples", resamples = frog_folds)

wf_set_fit
```

Rank the sets of models by their aggregate metric performance

```{r}
wf_set_fit %>%
  rank_results()
```

## Your turn

When do you think a workflow set would be useful?

Discuss with your neighbors!

## The final fit

```{r}
# `frog_split` has train + test info
final_fit <- last_fit(rf_wflow, frog_split) 

final_fit
```

Test set metrics:

```{r}
collect_metrics(final_fit)
```

Test set predictions:

```{r}
collect_predictions(final_fit)
```

```{r}
collect_predictions(final_fit) %>%
  ggplot(aes(latency, .pred)) + 
  geom_abline(lty = 2, col = "deeppink4", size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```

```{r}
extract_workflow(final_fit)
```

## Your turn

Which model do you think you would decide to use?

What surprised you the most?

What is one thing you are looking forward to for tomorrow?


## Building a model stack

```{r}
library(stacks)
```

For stacking, we need to save the predictions and the fitted workflow objects.

```{r}
stack_ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

Create a linear model:

```{r}
lr_spec <- linear_reg() %>%
  set_mode("regression")

lr_res <- workflow(latency ~ ., lr_spec) %>%
  fit_resamples(frog_folds, control = stack_ctrl)

lr_res
```

And use our random forest:

```{r}
rf_res <- workflow(latency ~ ., rf_spec) %>%
  fit_resamples(frog_folds, control = stack_ctrl)

rf_res
```

Initialize a data stack and add candidate members

```{r}
frog_st <- stacks()

frog_st

frog_st <- frog_st %>%
  add_candidates(lr_res) %>%
  add_candidates(rf_res)

frog_st
```

Fit a model that determines the "best" way to weight their predictions:

```{r}
frog_st_res <- frog_st %>%
  blend_predictions()

frog_st_res
```

Fit using the models with non-zero coefficients

```{r}
frog_st_res <- frog_st_res %>%
  fit_members()

frog_st_res
```

Predict on new data to get "blended" predictions

```{r}
frog_st_predictions <- frog_test %>%
  select(latency) %>%
  bind_cols(
    predict(frog_st_res, frog_test)
    )

frog_st_predictions
```

```{r}
ggplot(frog_st_predictions, aes(latency, .pred)) + 
  geom_abline(lty = 2, 
              col = "deeppink4", 
              size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```
