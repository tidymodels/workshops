{
  "hash": "2db389d50b34da62869645ff3ca240be",
  "result": {
    "markdown": "---\ntitle: \"6 - Tuning Hyperparameters\"\nsubtitle: \"Machine learning with tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n\n## Tuning parameters\n\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\n. . .\n\nSome examples:\n\n-   Tree depth in decision trees\n-   Number of neighbors in a K-nearest neighbor model\n\n# Activation function in neural networks?\n\nSigmoidal functions, ReLu, etc.\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Number of PCA components to retain?\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Bayesian priors for model parameters?\n\n::: fragment\nHmmmm, probably not.\nThese are based on prior belief.\n‚ùå\n:::\n\n# Covariance/correlation matrix structure in mixed models?\n\n::: fragment\nYes, but it is unlikely to affect performance.\n:::\n\n::: fragment\nIt will impact inference though.\nü§î\n:::\n\n\n\n# Is the random seed a tuning parameter?\n\n::: fragment\nNope. It is not. \n‚ùå\n:::\n\n## Optimize tuning parameters\n\n-   Try different values and measure their performance.\n\n. . .\n\n-   Find good values for these parameters.\n\n. . .\n\n-   Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n\n## Optimize tuning parameters\n\nThe main two strategies for optimization are:\n\n. . .\n\n-   **Grid search** üí† which tests a pre-defined set of candidate values\n\n-   **Iterative search** üåÄ which suggests/estimates new values of candidate parameters to evaluate\n\n## Choosing tuning parameters ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\nLet's take our previous recipe and add a few changes:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"13-14\"}\nglm_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  ) %>%\n  step_rm(coord_x, coord_y) %>%\n  step_zv(all_predictors()) %>%\n  step_ns(angle, deg_free = tune(\"angle\")) %>%\n  step_ns(distance, deg_free = tune(\"distance\")) %>%\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n. . .\n\nLet's `tune()` our spline terms!\n\n## Choosing tuning parameters ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\nLet's take our previous recipe and add a few changes:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nglm_spline_wflow <-\n  workflow() %>%\n  add_model(logistic_reg()) %>%\n  add_recipe(glm_rec)\n```\n:::\n\n\n\n## {background-image=\"https://www.tmwr.org/figures/ames-latitude-splines-1.png\" background-size=\"contain\"}\n\n:::notes\nSplines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.\n\nMore spline terms = more \"wiggly\", i.e. flexibly model a nonlinear relationship\n\nHow many spline terms? This is called *degrees of freedom*\n\n2 and 5 look like they underfit; 20 and 100 look like they overfit\n:::\n\n## Splines and nonlinear relationships\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::notes\nOur hockey data exhibits nonlinear relationships\n\nWe can model nonlinearity like this via a *model* (later this afternoon) or *feature engineering*\n\nHow do we decide how \"wiggly\" or flexible to make our spline features? TUNING \n:::\n\n## Grid search\n\n#### Parameters\n\n-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\n\n-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.\n\n::: fragment\n#### Grids\n\n-   Create your grid manually or automatically.\n\n-   The `grid_*()` functions can make a grid.\n:::\n\n::: notes\nMost basic (but very effective) way to tune models\n:::\n\n## Create a grid ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spline_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>       angle deg_free nparam[+]\n#>    distance deg_free nparam[+]\n```\n:::\n\n\n::: fragment\nA parameter set can be updated (e.g. to change the ranges).\n:::\n\n## Create a grid ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 23 √ó 2\n#>    angle distance\n#>    <int>    <int>\n#>  1    12        4\n#>  2    15        8\n#>  3     6       14\n#>  4    10        5\n#>  5    12       12\n#>  6     7        8\n#>  7    14        3\n#>  8    14       13\n#>  9    11       12\n#> 10     8       11\n#> # ‚Ä¶ with 13 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n::: fragment\n-   A *space-filling design* like this tends to perform better than random grids.\n-   Space-filling designs are also usually more efficient than regular grids.\n:::\n:::\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create a grid for our tunable workflow.*\n\n*Try creating a regular grid.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"make-grid\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Create a grid ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 25)\n\ngrid\n#> # A tibble: 225 √ó 2\n#>    angle distance\n#>    <int>    <int>\n#>  1     1        1\n#>  2     2        1\n#>  3     3        1\n#>  4     4        1\n#>  5     5        1\n#>  6     6        1\n#>  7     7        1\n#>  8     8        1\n#>  9     9        1\n#> 10    10        1\n#> # ‚Ä¶ with 215 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n```\n:::\n\n\n:::notes\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the `deg_free` parameters only have a range of `1->15`.\n:::\n\n## Update parameter ranges ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5-6\"}\nset.seed(2)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(angle = spline_degree(c(2L, 20L)),\n         distance = spline_degree(c(2L, 20L))) %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 24 √ó 2\n#>    angle distance\n#>    <int>    <int>\n#>  1    16        6\n#>  2    20       11\n#>  3     8       19\n#>  4    14        7\n#>  5    16       17\n#>  6    10       11\n#>  7    19        5\n#>  8    18       17\n#>  9    15       16\n#> 10    11       15\n#> # ‚Ä¶ with 14 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n```\n:::\n\n\n::: notes\nEven though `angle` is a `deg_free` parameter in `step_ns()`, we don't use the dials `deg_free()` object here. We have a special `spline_degree()` function that has better defaults for splines.\n:::\n\n## The results ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ngrid %>% \n  ggplot(aes(angle, distance)) +\n  geom_point(size = 4)\n```\n\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/show-grid-code-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Use the `tune_*()` functions to tune models\n\n## Spline grid search ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n::: {.cell hash='06-tuning-hyperparameters_cache/revealjs/tuning_c4d1d704f9edde994833e709a0f86779'}\n\n```{.r .cell-code}\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglm_spline_res <-\n  glm_spline_wflow %>%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\n\nglm_spline_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes            .predictions         \n#>   <list>              <chr>      <list>            <list>            <list>               \n#> 1 <split [7288/1822]> validation <tibble [48 √ó 6]> <tibble [24 √ó 3]> <tibble [43,728 √ó 8]>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x24: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n```\n:::\n\n\n::: notes\n-   `tune_grid()` is representative of tuning function syntax\n-   similar to `fit_resamples()`\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Tune our `glm_wflow`.*\n\n*What happens if you don't supply a `grid` argument to `tune_grid()`?*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"tune-glm\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Grid results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(glm_spline_res)\n```\n\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/autoplot-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(glm_spline_res)\n#> # A tibble: 48 √ó 8\n#>    angle distance .metric  .estimator  mean     n std_err .config              \n#>    <int>    <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1    16        6 accuracy binary     0.610     1      NA Preprocessor01_Model1\n#>  2    16        6 roc_auc  binary     0.649     1      NA Preprocessor01_Model1\n#>  3    20       11 accuracy binary     0.613     1      NA Preprocessor02_Model1\n#>  4    20       11 roc_auc  binary     0.649     1      NA Preprocessor02_Model1\n#>  5     8       19 accuracy binary     0.619     1      NA Preprocessor03_Model1\n#>  6     8       19 roc_auc  binary     0.652     1      NA Preprocessor03_Model1\n#>  7    14        7 accuracy binary     0.610     1      NA Preprocessor04_Model1\n#>  8    14        7 roc_auc  binary     0.650     1      NA Preprocessor04_Model1\n#>  9    16       17 accuracy binary     0.617     1      NA Preprocessor05_Model1\n#> 10    16       17 roc_auc  binary     0.649     1      NA Preprocessor05_Model1\n#> # ‚Ä¶ with 38 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n```\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#> # A tibble: 48 √ó 7\n#>    id         angle distance .metric  .estimator .estimate .config              \n#>    <chr>      <int>    <int> <chr>    <chr>          <dbl> <chr>                \n#>  1 validation    16        6 accuracy binary         0.610 Preprocessor01_Model1\n#>  2 validation    16        6 roc_auc  binary         0.649 Preprocessor01_Model1\n#>  3 validation    20       11 accuracy binary         0.613 Preprocessor02_Model1\n#>  4 validation    20       11 roc_auc  binary         0.649 Preprocessor02_Model1\n#>  5 validation     8       19 accuracy binary         0.619 Preprocessor03_Model1\n#>  6 validation     8       19 roc_auc  binary         0.652 Preprocessor03_Model1\n#>  7 validation    14        7 accuracy binary         0.610 Preprocessor04_Model1\n#>  8 validation    14        7 roc_auc  binary         0.650 Preprocessor04_Model1\n#>  9 validation    16       17 accuracy binary         0.617 Preprocessor05_Model1\n#> 10 validation    16       17 roc_auc  binary         0.649 Preprocessor05_Model1\n#> # ‚Ä¶ with 38 more rows\n#> # ‚Ñπ Use `print(n = ...)` to see more rows\n```\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 8\n#>   angle distance .metric .estimator  mean     n std_err .config              \n#>   <int>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1     5        8 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n#> 2     6        9 roc_auc binary     0.653     1      NA Preprocessor17_Model1\n#> 3     3       15 roc_auc binary     0.653     1      NA Preprocessor13_Model1\n#> 4     3       13 roc_auc binary     0.652     1      NA Preprocessor11_Model1\n#> 5     5       19 roc_auc binary     0.652     1      NA Preprocessor24_Model1\n```\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nCreate your own tibble for final parameters or use one of the `tune::select_*()` functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\n```\n:::\n\n\n. . .\n\nThis best result has:\n\n-   low-degree spline for `angle` (less \"wiggly\", less complex)\n-   higher-degree spline for `distance` (more \"wiggly\", more complex)\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Try an alternative selection strategy.*\n\n*Read the docs for `select_by_pct_loss()`.*\n\n*Try choosing a model that has a simpler (less \"wiggly\") relationship for `distance`.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"select-by-pct-loss\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\nselect_by_pct_loss(glm_spline_res, distance, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 10\n#>   angle distance .metric .estimator  mean     n std_err .config               .best .loss\n#>   <int>    <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 <dbl> <dbl>\n#> 1    13        4 roc_auc binary     0.646     1      NA Preprocessor20_Model1 0.653 0.984\n```\n:::\n\n\n# Boosted trees üå≥üå≤üå¥üåµüå¥üå≥üå≥üå¥üå≤üåµüå¥üå≤üå≥üå¥üå≥üåµüåµüå¥üå≤üå≤üå≥üå¥üå≥üå¥üå≤üå¥üåµüå¥üå≤üå¥üåµüå≤üåµüå¥üå≤üå≥üå¥üåµüå≥üå¥üå≥\n\n## Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\n-   Ensemble many decision tree models\n\n::: fragment\n### Review how a decision tree model works:\n\n-   Series of splits or if/then statements based on predictors\n\n-   First the tree *grows* until some condition is met (maximum depth, no more data)\n\n-   Then the tree is *pruned* to reduce its complexity\n:::\n\n## Single decision tree\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/tree-example-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\nBoosting methods fit a *sequence* of tree-based models.\n\n. . .\n\n-   Each tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\n\n-   This is like gradient-based steepest ascent methods from calculus.\n\n## Boosted tree tuning parameters  {.annotation}\n\nMost modern boosting methods have *a lot* of tuning parameters!\n\n. . .\n\n-   For tree growth and pruning (`min_n`, `max_depth`, etc)\n\n-   For boosting (`trees`, `stop_iter`, `learn_rate`)\n\n. . .\n\nWe'll use *early stopping* to stop boosting when a few iterations produce consecutively worse results.\n\n## Comparing tree ensembles\n\n::: columns\n::: {.column width=\"50%\"}\nRandom forest\n\n* Independent trees\n* Bootstrapped data\n* No pruning\n* 1000's of trees\n:::\n\n::: {.column width=\"50%\"}\nBoosting\n\n* Dependent trees\n* Different case weights\n* Tune tree parameters\n* Far fewer trees\n:::\n:::\n\nThe general consensus for tree-based models is, in terms of performance: boosting > random forest > bagging > single trees.\n\n\n## Boosted tree code {.annotation}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <-\n  boost_tree(\n    trees = 500, min_n = tune(), stop_iter = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %>%\n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\", validation = 1/10) # <- for better early stopping\n\nxgb_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_lencode_mixed(player, outcome = vars(on_goal)) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nxgb_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(xgb_rec)\n```\n:::\n\n\n:::notes\n`validation` is an argument to `parsnip::xgb_train()`, not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to `xgb.train(watchlist = list(validation = data))`.\n\nSee `translate(xgb_spec)` to see where it is passed to `parsnip::xgb_train()`.\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create your boosted tree workflow.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"xgb-wflow\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Running in parallel\n\n::: columns\n::: {.column width=\"60%\"}\n-   Grid search, combined with resampling, requires fitting a lot of models!\n\n-   These models don't depend on one another and can be run in parallel.\n\nWe can use a *parallel backend* to do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncores <- parallel::detectCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/resample-times-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Running in parallel\n\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n:::notes\nFaceted on the expensiveness of preprocessing used.\n:::\n\n## Tuning ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nThis will take some time to run ‚è≥\n\n\n::: {.cell hash='06-tuning-hyperparameters_cache/revealjs/xgboost-tune_c04416e76ad58d5b8417a7f5c4e1b4b8'}\n\n```{.r .cell-code}\nset.seed(9)\n\nxgb_res <-\n  xgb_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 15, control = ctrl) # automatic grid now!\n```\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Start tuning the boosted tree model!*\n\n*We won't wait for everyone's tuning to finish, but take this time to get it started before we move on.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"tune-xgboost\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes           .predictions          \n#>   <list>              <chr>      <list>            <list>           <list>                \n#> 1 <split [7288/1822]> validation <tibble [30 √ó 9]> <tibble [0 √ó 3]> <tibble [27,330 √ó 11]>\n```\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(xgb_res)\n```\n\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/autoplot-xgboost-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n## Again with the location features\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_rec <- \n  xgb_rec %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  ) %>% \n  step_rm(coord_x, coord_y)\n\nxgb_coord_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res <-\n  xgb_coord_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 20, control = ctrl)\n```\n:::\n\n\n## Did the machine figure it out? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 11\n#>   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#>   <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    19          2    0.311         1.46e- 4        14 roc_auc binary     0.633     1      NA Preprocessor1_Model12\n#> 2    11          3    0.0255        5.61e- 7        19 roc_auc binary     0.628     1      NA Preprocessor1_Model05\n#> 3    27          5    0.0120        6.04e- 6        12 roc_auc binary     0.627     1      NA Preprocessor1_Model07\n#> 4    25         14    0.0379        3.62e- 5         8 roc_auc binary     0.626     1      NA Preprocessor1_Model13\n#> 5    31         11    0.00585       1.02e-10         7 roc_auc binary     0.625     1      NA Preprocessor1_Model08\n\nshow_best(xgb_coord_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 11\n#>   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#>   <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1    30         13     0.0578  0.00000000141        18 roc_auc binary     0.648     1      NA Preprocessor1_Model07\n#> 2    39         12     0.0803  0.000411             10 roc_auc binary     0.643     1      NA Preprocessor1_Model12\n#> 3    14          2     0.146   0.00244              19 roc_auc binary     0.642     1      NA Preprocessor1_Model14\n#> 4    26         15     0.0365  2.51                 17 roc_auc binary     0.642     1      NA Preprocessor1_Model11\n#> 5    35          5     0.101   0.0000000784         13 roc_auc binary     0.641     1      NA Preprocessor1_Model17\n```\n:::\n\n\n\n## Compare models\n\nBest logistic regression results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n```\n:::\n\n\n::: fragment\nBest boosting results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_coord_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.648     1      NA Preprocessor1_Model07\n```\n:::\n\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Can you get better ROC results with xgboost?*\n\n*Try increasing `learn_rate` beyond the original range.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"improve-xgb\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">20</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Updating the workflow ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell hash='06-tuning-hyperparameters_cache/revealjs/final-select-best_1fed6d707db4f46a37c7e5f5fbbdcb00'}\n\n```{.r .cell-code}\nbest_auc <- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#> # A tibble: 1 √ó 3\n#>   angle distance .config              \n#>   <int>    <int> <chr>                \n#> 1     5        8 Preprocessor12_Model1\n\nglm_spline_wflow <-\n  glm_spline_wflow %>% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#> ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> 8 Recipe Steps\n#> \n#> ‚Ä¢ step_lencode_mixed()\n#> ‚Ä¢ step_dummy()\n#> ‚Ä¢ step_mutate()\n#> ‚Ä¢ step_rm()\n#> ‚Ä¢ step_zv()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_normalize()\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n:::\n\n\n## The final fit to the NHL data ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}  {.annotation}\n\n\n::: {.cell hash='06-tuning-hyperparameters_cache/revealjs/final-last-fit_9d92988e4c910b489e50e69d77ea67b4'}\n\n```{.r .cell-code}\ntest_res <- \n  glm_spline_wflow %>% \n  last_fit(split = nhl_split)\n\ntest_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 √ó 6\n#>   splits              id               .metrics         .notes           .predictions         .workflow \n#>   <list>              <chr>            <list>           <list>           <list>               <list>    \n#> 1 <split [9110/3037]> train/test split <tibble [2 √ó 4]> <tibble [1 √ó 3]> <tibble [3,037 √ó 6]> <workflow>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n```\n:::\n\n\n. . .\n\nRemember that `last_fit()` fits one time with the combined training and validation set, then evaluates one time with the testing set.\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Finalize your workflow with the best parameters.*\n\n*Create a final fit.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"finalize-xgb\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Estimates of ROC AUC ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nValidation results from tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, mean, n, std_err)\n#> # A tibble: 1 √ó 4\n#>   .metric  mean     n std_err\n#>   <chr>   <dbl> <int>   <dbl>\n#> 1 roc_auc 0.653     1      NA\n```\n:::\n\n\n::: fragment\nTest set results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_res %>% collect_metrics()\n#> # A tibble: 2 √ó 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.616 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.656 Preprocessor1_Model1\n```\n:::\n\n:::\n\n## Final fitted workflow\n\nExtract the final fitted workflow, fit using the training set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_glm_spline_wflow <- \n  test_res %>% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#> # A tibble: 3 √ó 1\n#>   .pred_class\n#>   <fct>      \n#> 1 no         \n#> 2 yes        \n#> 3 no\n```\n:::\n\n\n## Next steps\n\n\n-   [Document the model](https://vetiver.rstudio.com/learn-more/model-card.html).\n\n. . .\n\n-   [Deploy the model](https://vetiver.rstudio.com/get-started/).\n\n. . .\n\n-   Create an [applicability domain model](https://applicable.tidymodels.org/) to help monitor our data over time.\n\n. . .\n\n-   Use [explainers](https://www.tmwr.org/explain.html) to characterize the model and the predictions.\n\n\n## Explain yourself  {.annotation}\n\n\nThere are two categories of model explanations, **global** and **local**.\n\n. . .\n\n- Global model explanations provide an overall understanding aggregated over a _whole set_ of observations.\n\n- Local model explanations provide information about a prediction for a _single_ observation.\n\n\n. . .\n\nYou can also build global model explanations by aggregating local model explanations.\n\n# tidymodels integrates with model explainability frameworks\n\n![](https://dalex.drwhy.ai/misc/dalex_even.png){.absolute bottom=\"-300\" right=\"0\" width=\"300\"}\n\n## A tidymodels explainer  {.annotation}\n\nWe can build explainers using:\n\n- original, basic predictors\n- derived features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DALEXtra)\n\nglm_explainer <- explain_tidymodels(\n  final_glm_spline_wflow,\n  data = dplyr::select(nhl_train, -on_goal),\n  # DALEX required an integer for factors:\n  y = as.integer(nhl_train$on_goal),\n  verbose = FALSE\n)\n```\n:::\n\n\n## Explain the x coordinates\n\nWith our explainer, let's create [partial dependence profiles](https://ema.drwhy.ai/partialDependenceProfiles.html):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\npdp_coord_x <- model_profile(\n  glm_explainer,\n  variables = \"coord_x\",\n  N = 500,\n  groups = \"position\"\n)\n```\n:::\n\n\n. . .\n\nYou can use the default `plot()` method or create your own visualization.\n\n## Explain the x coordinates\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/pdp-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Explain the x coordinates\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/pdp-grouped-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create an explainer for our glm model.*\n\n*Try grouping by another variable, like `game_type` or `dow`.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"explainer\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.3.5/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}