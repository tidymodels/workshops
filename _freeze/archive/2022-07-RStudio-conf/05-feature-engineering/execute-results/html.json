{
  "hash": "4b56f39b21f50637b05fa4477748d125",
  "result": {
    "markdown": "---\ntitle: \"5 - Feature engineering\"\nsubtitle: \"Machine learning with tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n## Working with our predictors\n\nWe might want to modify our predictors columns for a few reasons: \n\n::: {.incremental}\n- The model requires them in a different format (e.g. dummy variables for `lm()`).\n- The model needs certain data qualities (e.g. same units for K-NN).\n- The outcome is better predicted when one or more columns are transformed in some way (a.k.a \"feature engineering\"). \n:::\n\n. . .\n\nThe first two reasons are fairly predictable ([next page](https://www.tmwr.org/pre-proc-table.html#tab:preprocessing)).\n\nThe last one depends on your modeling problem. \n\n\n##  {background-iframe=\"https://www.tmwr.org/pre-proc-table.html#tab:preprocessing\"}\n\n::: footer\n:::\n\n\n## What is feature engineering?\n\nThink of a feature as some *representation* of a predictor that will be used in a model.\n\n. . .\n\nExample representations:\n\n-   Interactions\n-   Polynomial expansions/splines\n-   PCA feature extraction\n\nThere are a lot of examples in [_Feature Engineering and Selection_](https://bookdown.org/max/FES/).\n\n\n\n## Example: Dates\n\nHow can we represent date columns for our model?\n\n. . .\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n. . .\n\nIt can be re-engineered as:\n\n-   Days since a reference date\n-   Day of the week\n-   Month\n-   Year\n-   Indicators for holidays\n\n::: notes\nThe main point is that we try to maximize performance with different versions of the predictors. \n\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model.\n:::\n\n## General definitions ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n-   *Data preprocessing* steps allow your model to fit.\n\n-   *Feature engineering* steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\n\nIn a little bit, we'll see successful (and unsuccessful) feature engineering methods for our example data. \n\n\n::: notes\nThese terms are often used interchangeably in the ML community but we want to distinguish them.\n:::\n\n## The NHL data üèí\n\n-   From Pittsburgh Penguins games, 12,147 shots\n\n-   Data from the 2015-2016 season\n\n. . .\n\nLet's predict whether a shot is on-goal (a goal or blocked by goaltender) or not.\n\n## Case study\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nglimpse(season_2015)\n#> Rows: 12,147\n#> Columns: 17\n#> $ on_goal           <fct> yes, no, no, yes, no, no, yes, no, yes, no, no, no, ‚Ä¶\n#> $ period            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n#> $ period_type       <fct> regular, regular, regular, regular, regular, regular‚Ä¶\n#> $ coord_x           <dbl> -53, 68, -42, -77, -67, 55, 77, 62, 59, 76, 44, 62, ‚Ä¶\n#> $ coord_y           <dbl> -18, -12, -18, 9, -5, -12, 13, 14, -5, -6, 7, -2, -2‚Ä¶\n#> $ game_time         <dbl> 0.300000, 0.900000, 1.250000, 1.783333, 2.050000, 3.‚Ä¶\n#> $ strength          <fct> even, even, even, even, even, even, even, even, even‚Ä¶\n#> $ player            <fct> victor_hedman, evgeni_malkin, jason_garrison, ondrej‚Ä¶\n#> $ player_diff       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#> $ offense_team      <fct> TBL, PIT, TBL, TBL, TBL, PIT, PIT, PIT, PIT, PIT, PI‚Ä¶\n#> $ defense_team      <fct> PIT, TBL, PIT, PIT, PIT, TBL, TBL, TBL, TBL, TBL, TB‚Ä¶\n#> $ offense_goal_diff <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#> $ game_type         <fct> playoff, playoff, playoff, playoff, playoff, playoff‚Ä¶\n#> $ position          <fct> defenseman, center, defenseman, left_wing, defensema‚Ä¶\n#> $ dow               <fct> Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sa‚Ä¶\n#> $ month             <fct> May, May, May, May, May, May, May, May, May, May, Ma‚Ä¶\n#> $ year              <dbl> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016‚Ä¶\n```\n:::\n\n\n## Data splitting strategy\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/whole-game-split.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Why a validation set?\n\nRecall that resampling gives us performance measures without using the test set. \n\nIt's important to get good resampling statistics (e.g. $R^2$). \n\n - That usually means having enough data to estimate performance. \n\nWhen you have \"a lot\" of data, a validation set can be an efficient way to do this. \n\n\n## Splitting the NHL data ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(23)\nnhl_split <- initial_split(season_2015, prop = 3/4)\nnhl_split\n#> <Training/Testing/Total>\n#> <9110/3037/12147>\n\nnhl_train_and_val <- training(nhl_split)\nnhl_test  <- testing(nhl_split)\n\n## not testing\nnrow(nhl_train_and_val)\n#> [1] 9110\n \n## testing\nnrow(nhl_test)\n#> [1] 3037\n```\n:::\n\n\n## Validation split ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nSince there are a lot of observations, we'll use a validation set: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nnhl_val <- validation_split(nhl_train_and_val, prop = 0.80)\nnhl_val\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 2\n#>   splits              id        \n#>   <list>              <chr>     \n#> 1 <split [7288/1822]> validation\n```\n:::\n\n\n. . .\n\nRemember that a validation split is a type of resample. \n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Let's explore the training set data.*\n\n*Use the function `plot_nhl_shots()` for nice spatial plots of the data.*\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_train <- analysis(nhl_val$splits[[1]])\n\nset.seed(100)\nnhl_train %>% \n  sample_n(200) %>%\n  plot_nhl_shots(emphasis = position)\n```\n\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/rink-code-1.svg)\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"nhl-explore\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n:::\n:::\n\n\n\n## Prepare your data for modeling ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n- The recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n. . .\n\n- Statistical parameters for the steps can be _estimated_ from an initial data set and then _applied_ to other data sets.\n\n. . .\n\n- The resulting processed output can be used as inputs for statistical or machine learning models.\n\n## A first recipe ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train)\n```\n:::\n\n\n. . .\n\n- The `recipe()` function assigns columns to roles of \"outcome\" or \"predictor\" using the formula\n\n## A first recipe ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(nhl_rec)\n#> # A tibble: 17 √ó 4\n#>    variable          type    role      source  \n#>    <chr>             <chr>   <chr>     <chr>   \n#>  1 period            numeric predictor original\n#>  2 period_type       nominal predictor original\n#>  3 coord_x           numeric predictor original\n#>  4 coord_y           numeric predictor original\n#>  5 game_time         numeric predictor original\n#>  6 strength          nominal predictor original\n#>  7 player            nominal predictor original\n#>  8 player_diff       numeric predictor original\n#>  9 offense_team      nominal predictor original\n#> 10 defense_team      nominal predictor original\n#> 11 offense_goal_diff numeric predictor original\n#> 12 game_type         nominal predictor original\n#> 13 position          nominal predictor original\n#> 14 dow               nominal predictor original\n#> 15 month             nominal predictor original\n#> 16 year              numeric predictor original\n#> 17 on_goal           nominal outcome   original\n```\n:::\n\n\n## Create indicator variables ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n. . .\n\n- For any factor or character predictors, make binary indicators.\n\n- There are *many* recipe steps that can convert categorical predictors to numeric columns.\n\n## Filter out constant columns ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n```\n:::\n\n\n. . .\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all `0`s), we can delete any *zero-variance* predictors that have a single unique value.\n\n:::notes\nNote that the selector chooses all columns with a role of \"predictor\"\n:::\n\n\n## Normalization ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n. . .\n\n- This centers and scales the numeric predictors.\n\n\n- The recipe will use the _training_ set to estimate the means and standard deviations of the data.\n\n. . .\n\n- All data the recipe is applied to will be normalized using those statistics (there is no re-estimation).\n\n## Reduce correlation ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n```\n:::\n\n\n. . .\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold.\n\n## Other possible steps ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(all_numeric_predictors())\n```\n:::\n\n\n. . . \n\nPCA feature extraction...\n\n## Other possible steps ![](hexes/embed.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  embed::step_umap(all_numeric_predictors(), outcome = on_goal)\n```\n:::\n\n\n. . . \n\nA fancy machine learning supervised dimension reduction technique...\n\n:::notes\nNote that this uses the outcome, and it is from an extension package\n:::\n\n\n## Other possible steps ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_ns(coord_y, coord_x, deg_free = 10)\n```\n:::\n\n\n. . . \n\nNonlinear transforms like natural splines, and so on!\n\n##  {background-iframe=\"https://recipes.tidymodels.org/reference/index.html\"}\n\n::: footer\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create a `recipe()` for the on-goal data to :*\n\n-   *create one-hot indicator variables*\n-   *remove zero-variance variables*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"make-recipe\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n## Minimal recipe ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_indicators <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n## Using a workflow ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell hash='05-feature-engineering_cache/revealjs/unnamed-chunk-10_d25f3f3effb02851a0cf9029710a3f56'}\n\n```{.r .cell-code}\nset.seed(9)\n\nnhl_glm_wflow <-\n  workflow() %>%\n  add_recipe(nhl_indicators) %>%\n  add_model(logistic_reg())\n \nctrl <- control_resamples(save_pred = TRUE)\nnhl_glm_res <-\n  nhl_glm_wflow %>%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#> # A tibble: 2 √ó 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.555     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.558     1      NA Preprocessor1_Model1\n```\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Use `fit_resamples()` to fit your workflow with a recipe.*\n\n*Collect the predictions from the results.*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"resample-recipe\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n## Holdout predictions ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since we used `save_pred = TRUE`\nglm_val_pred <- collect_predictions(nhl_glm_res)\nglm_val_pred %>% slice(1:7)\n#> # A tibble: 7 √ó 7\n#>   id         .pred_yes .pred_no  .row .pred_class on_goal .config             \n#>   <chr>          <dbl>    <dbl> <int> <fct>       <fct>   <chr>               \n#> 1 validation     0.198 8.02e- 1    10 no          no      Preprocessor1_Model1\n#> 2 validation     0.264 7.36e- 1    17 no          yes     Preprocessor1_Model1\n#> 3 validation     0.189 8.11e- 1    23 no          no      Preprocessor1_Model1\n#> 4 validation     1.00  8.39e-11    40 yes         yes     Preprocessor1_Model1\n#> 5 validation     0.322 6.78e- 1    41 no          yes     Preprocessor1_Model1\n#> 6 validation     1.00  8.39e-11    46 yes         no      Preprocessor1_Model1\n#> 7 validation     0.354 6.46e- 1    55 no          no      Preprocessor1_Model1\n```\n:::\n\n\n# Two class data\n\nLet's say we can define one class as the \"event\", like a shot being on goal.\n\n. . .\n\n-   The **sensitivity** is the *true positive rate* (accuracy on actual events).\n\n-   The **specificity** is the *true negative rate* (accuracy on actual non-events, or 1 - *false positive rate*).\n\n## Two class data\n\nThese definitions assume that we know the threshold for converting \"soft\" probability predictions into \"hard\" class predictions.\n\n. . .\n\nIs a 50% threshold good? \n\nWhat happens if we say that we need to be 80% sure to declare an event?\n\n-   sensitivity ‚¨áÔ∏è, specificity ‚¨ÜÔ∏è\n\n. . .\n\nWhat happens for a 20% threshold?\n\n-   sensitivity ‚¨ÜÔ∏è, specificity ‚¨áÔ∏è\n\n## ROC curves\n\nTo make an ROC (receiver operator characteristic) curve, we:\n\n- calculate the sensitivity and specificity for all possible thresholds\n\n- plot false positive rate (x-axis) versus true positive rate (y-axis)\n\n. . .\n\nWe can use the area under the ROC curve as a classification metric: \n\n- ROC AUC = 1 üíØ \n- ROC AUC = 1/2 üò¢\n\n:::notes\nROC curves are insensitive to class imbalance.\n:::\n\n## ROC curves ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumes _first_ factor level is event; there are options to change that\nroc_curve_points <- glm_val_pred %>% roc_curve(truth = on_goal, estimate = .pred_yes)\nroc_curve_points %>% slice(1, 50, 100)\n#> # A tibble: 3 √ó 3\n#>   .threshold specificity sensitivity\n#>        <dbl>       <dbl>       <dbl>\n#> 1   -Inf          0            1    \n#> 2      0.139      0.0303       0.977\n#> 3      0.272      0.0642       0.955\n\nglm_val_pred %>% roc_auc(truth = on_goal, estimate = .pred_yes)\n#> # A tibble: 1 √ó 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.558\n```\n:::\n\n\n## ROC curve plot ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(roc_curve_points)\n```\n\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/roc-curve-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Compute and plot an ROC curve for your current model.*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"roc-curve\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*What data are being used for this ROC curve plot?*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"roc-curve-data\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n\n## What do we do with the player data? üèí\n\nThere are 597 unique player values in our training set. How can we include this information in our model?\n\n. . .\n\nWe could:\n\n-   make the full set of indicator variables üò≥\n\n-   use [feature hashing](https://www.tmwr.org/categorical.html#feature-hashing) to create a smaller set of indicator variables\n\n-   use effect encoding to replace the `player` column with the estimated effect of that predictor\n\n\n. . .\n\nLet's use an _effect encoding_.\n\n\n\n\n\n\n## What is an effect encoding?\n\nWe replace the qualitative‚Äôs predictor data with their _effect on the outcome_. \n\n::: columns\n::: {.column width=\"50%\"}\nData before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbefore\n#> # A tibble: 7 √ó 3\n#>   on_goal player            .row\n#>   <fct>   <fct>            <int>\n#> 1 yes     brian_dumoulin       1\n#> 2 yes     patric_hornqvist     2\n#> 3 yes     nikita_nesterov      3\n#> 4 yes     jack_eichel          4\n#> 5 yes     justin_williams      5\n#> 6 yes     seth_jones           6\n#> 7 yes     kris_letang          7\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nData after:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafter\n#> # A tibble: 7 √ó 3\n#>   on_goal  player  .row\n#>   <fct>     <dbl> <int>\n#> 1 yes     -0.114      1\n#> 2 yes      0.631      2\n#> 3 yes      0.142      3\n#> 4 yes      0.220      4\n#> 5 yes      0.248      5\n#> 6 yes      0.0733     6\n#> 7 yes      0.0774     7\n```\n:::\n\n\n:::\n:::\n\nThe `player` column is replaced with the log-odds of being on goal. \n\n:::notes\nAs a reminder:\n\n\n$$\\text{log-odds} = log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)$$ \n\n\nwhere $\\hat{p}$ is the on goal rate estimate. \n\nFor logistic regression, this is what the predictors are modeling. The log-odds are more likely to be linear with the outcome. \n\n:::\n\n\n## Per-player statistics {.annotation}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/effects-1.svg){fig-align='center' width=90%}\n:::\n\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/effects-2.svg){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n- Good statistical methods for estimating these rates use *partial pooling*.\n\n\n- Pooling borrows strength across players and shrinks extreme values (e.g. zero or one) towards the mean for players with very few shots.\n\n\n- The embed package has recipe steps for effect encodings.\n\n:::\n:::\n\n\n:::notes\nPartial pooling gives better estimates for players with fewer shots by shrinking the estimate to the overall on-goal rate (55.2%)\n\n\n:::\n\n## Partial pooling\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/effect-compare-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Player effects ![](hexes/embed.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1,5\"}\nlibrary(embed)\n\nnhl_effect_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n. . .\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting.\n\n## Recipes are estimated ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nPreprocessing steps in a recipe use the *training set* to compute quantities.\n\n. . .\n\nWhat kind of quantities are computed for preprocessing?\n\n-   Levels of a factor\n-   Whether a column has zero variance\n-   Normalization\n-   Feature extraction\n-   Effect encodings\n\n. . .\n\nWhen a recipe is part of a workflow, this estimation occurs when `fit()` is called.\n\n## Effect encoding results ![](hexes/embed.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3|\"}\nnhl_effect_wflow <-\n  nhl_glm_wflow %>%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res <-\n  nhl_effect_wflow %>%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_effect_res)\n#> # A tibble: 2 √ó 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.540     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.551     1      NA Preprocessor1_Model1\n```\n:::\n\n\n# Where is the shot coming from? üèíüßê \n\n## Angle\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_angle_rec <-\n  nhl_indicators %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi))\n  )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/angle-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n:::notes\nNote the danger of using `step_mutate()` -- easy to have data leakage \n\n`coord_x` is \"distance from goal\". We subtract it from `89` to get the distance from the center of the ice. The `abs()` calls account for the fact that the goals might be on either side of `(0, 0)`. The rest of it is the formula for going from `(x, y)` to angle in degrees.\n:::\n\n## Distance {.annotation}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_distance_rec <-\n  nhl_angle_rec %>%\n  step_mutate(\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance)\n  )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/distance-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n## Behind goal line\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_behind_rec <-\n  nhl_distance_rec %>%\n  step_mutate(\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/goal-line-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n## Fit different recipes ![](hexes/embed.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\nA workflow set can cross models and/or preprocessors and then resample them *en masse*. \n\n\n::: {.cell hash='05-feature-engineering_cache/revealjs/nhl-feature-sets_66f2052b57a2e56db808c69ad028522a'}\n\n```{.r .cell-code}\nset.seed(9)\n\nnhl_glm_set_res <-\n  workflow_set(\n    list(`1_dummy` = nhl_indicators, `2_angle` = nhl_angle_rec, \n         `3_dist` = nhl_distance_rec, `4_bgl` = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %>%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val, verbose = TRUE, control = ctrl)\n```\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create a workflow set with 2 or 3 recipes.*\n\n*(Consider using recipes we've already created.)*\n\n*Use `workflow_map()` to resample the workflow set.* \n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"hockey-wfset\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n## Compare recipes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %>%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point(size = 3) +\n  labs(y = NULL, x = \"ROC AUC (validation set)\")\n```\n:::\n\n\n## Compare recipes\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/unnamed-chunk-21-1.svg)\n:::\n:::\n\n\n## Debugging a recipe\n\n- Typically, you will want to use a workflow to estimate and apply a recipe.\n\n. . .\n\n- If you have an error and need to debug your recipe, the original recipe object (e.g. `encoded_players`) can be estimated manually with a function called `prep()`. It is analogous to `fit()`.\n\n. . .\n\n- Another function (`bake()`) is analogous to `predict()`, and gives you the processed data back.\n\n## More on recipes\n\n-   Once `fit()` is called on a workflow, changing the model does not re-fit the recipe.\n\n. . .\n\n-   A list of all known steps is at <https://www.tidymodels.org/find/recipes/>.\n\n. . .\n\n-   Some steps can be [skipped](https://recipes.tidymodels.org/articles/Skipping.html) when using `predict()`.\n\n. . .\n\n-   The [order](https://recipes.tidymodels.org/articles/Ordering.html) of the steps matters.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.3.5/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}