{
  "hash": "c0b22c6fb586f0f890a5b44feac9f4a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"4 - Evaluating models\"\nsubtitle: \"Introduction to tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.path: \"figures/\"\n---\n\n\n\n\n\n## Looking at predictions\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  relocate(tip, .pred_class, .pred_yes, .pred_no)\n#> # A tibble: 8,000 Ã— 10\n#>    tip   .pred_class .pred_yes .pred_no distance company local dow   month  hour\n#>    <fct> <fct>           <dbl>    <dbl>    <dbl> <fct>   <fct> <fct> <fct> <int>\n#>  1 yes   yes             0.967   0.0333    17.2  Chicagâ€¦ no    Thu   Feb      16\n#>  2 yes   yes             0.935   0.0646     0.88 City Sâ€¦ yes   Thu   Mar       8\n#>  3 yes   yes             0.967   0.0333    18.1  other   no    Mon   Feb      18\n#>  4 yes   yes             0.949   0.0507    12.2  Chicagâ€¦ no    Sun   Mar      21\n#>  5 yes   yes             0.821   0.179      0.94 Sun Taâ€¦ yes   Sat   Apr      23\n#>  6 yes   yes             0.967   0.0333    17.5  Flash â€¦ no    Fri   Mar      12\n#>  7 yes   yes             0.967   0.0333    17.7  other   no    Sun   Jan       6\n#>  8 yes   yes             0.938   0.0616     1.85 Taxicaâ€¦ no    Fri   Apr      12\n#>  9 yes   yes             0.938   0.0616     0.53 Sun Taâ€¦ no    Tue   Mar      18\n#> 10 yes   yes             0.931   0.0694     6.65 Taxicaâ€¦ no    Sun   Apr      11\n#> # â„¹ 7,990 more rows\n```\n:::\n\n\n\n## Confusion matrix ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n![](images/confusion-matrix.png)\n\n## Confusion matrix ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  conf_mat(truth = tip, estimate = .pred_class)\n#>           Truth\n#> Prediction  yes   no\n#>        yes 7341  536\n#>        no    43   80\n```\n:::\n\n\n\n## Confusion matrix ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  conf_mat(truth = tip, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](figures/conf-mat-plot-1.svg)\n:::\n:::\n\n\n\n## Metrics for model performance ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  accuracy(truth = tip, estimate = .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.928\n```\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n![](images/confusion-matrix-accuracy.png)\n:::\n:::\n\n## Dangers of accuracy ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWe need to be careful of using `accuracy()` since it can give \"good\" performance by only predicting one way with imbalanced data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  mutate(.pred_class = factor(\"yes\", levels = c(\"yes\", \"no\"))) %>%\n  accuracy(truth = tip, estimate = .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.923\n```\n:::\n\n\n\n## Metrics for model performance ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  sensitivity(truth = tip, estimate = .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 sensitivity binary         0.994\n```\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n![](images/confusion-matrix-sensitivity.png)\n:::\n:::\n\n\n## Metrics for model performance ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3-6\"}\naugment(taxi_fit, new_data = taxi_train) %>%\n  sensitivity(truth = tip, estimate = .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 sensitivity binary         0.994\n```\n:::\n\n\n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>%\n  specificity(truth = tip, estimate = .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 specificity binary         0.130\n```\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n![](images/confusion-matrix-specificity.png)\n:::\n:::\n\n## Metrics for model performance ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWe can use `metric_set()` to combine multiple calculations into one\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_metrics <- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %>%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#> # A tibble: 3 Ã— 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 accuracy    binary         0.928\n#> 2 specificity binary         0.130\n#> 3 sensitivity binary         0.994\n```\n:::\n\n\n\n## Metrics for model performance ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_metrics <- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %>%\n  group_by(local) %>%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#> # A tibble: 6 Ã— 4\n#>   local .metric     .estimator .estimate\n#>   <fct> <chr>       <chr>          <dbl>\n#> 1 yes   accuracy    binary         0.898\n#> 2 no    accuracy    binary         0.935\n#> 3 yes   specificity binary         0.169\n#> 4 no    specificity binary         0.116\n#> 5 yes   sensitivity binary         0.987\n#> 6 no    sensitivity binary         0.996\n```\n:::\n\n\n\n## Two class data\n\nThese metrics assume that we know the threshold for converting \"soft\" probability predictions into \"hard\" class predictions.\n\n. . .\n\nIs a 50% threshold good? \n\nWhat happens if we say that we need to be 80% sure to declare an event?\n\n-   sensitivity â¬‡ï¸, specificity â¬†ï¸\n\n. . .\n\nWhat happens for a 20% threshold?\n\n-   sensitivity â¬†ï¸, specificity â¬‡ï¸\n\n## Varying the threshold\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/thresholds-1.svg)\n:::\n:::\n\n\n\n## ROC curves\n\nTo make an ROC (receiver operator characteristic) curve, we:\n\n- calculate the sensitivity and specificity for all possible thresholds\n\n- plot false positive rate (x-axis) versus true positive rate (y-axis)\n\ngiven that sensitivity is the true positive rate, and specificity is the true negative rate. Hence `1 - specificity` is the false positive rate.\n\n. . .\n\nWe can use the area under the ROC curve as a classification metric: \n\n- ROC AUC = 1 ğŸ’¯ \n- ROC AUC = 1/2 ğŸ˜¢\n\n:::notes\nROC curves are insensitive to class imbalance.\n:::\n\n## ROC curves ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumes _first_ factor level is event; there are options to change that\naugment(taxi_fit, new_data = taxi_train) %>% \n  roc_curve(truth = tip, .pred_yes) %>%\n  slice(1, 20, 50)\n#> # A tibble: 3 Ã— 3\n#>   .threshold specificity sensitivity\n#>        <dbl>       <dbl>       <dbl>\n#> 1   -Inf           0         1      \n#> 2      0.783       0.209     0.981  \n#> 3      1           1         0.00135\n\naugment(taxi_fit, new_data = taxi_train) %>% \n  roc_auc(truth = tip, .pred_yes)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.691\n```\n:::\n\n\n\n## ROC curve plot ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\naugment(taxi_fit, new_data = taxi_train) %>% \n  roc_curve(truth = tip, .pred_yes) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](figures/roc-curve-1.svg)\n:::\n:::\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Compute and plot an ROC curve for your current model.*\n\n*What data are being used for this ROC curve plot?*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"roc-curve\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n##  {background-iframe=\"https://yardstick.tidymodels.org/reference/index.html\"}\n\n::: footer\n:::\n\n# âš ï¸ DANGERS OF OVERFITTING âš ï¸\n\n## Dangers of overfitting âš ï¸\n\n![](https://raw.githubusercontent.com/topepo/2022-nyr-workshop/main/images/tuning-overfitting-train-1.svg)\n\n## Dangers of overfitting âš ï¸\n\n![](https://raw.githubusercontent.com/topepo/2022-nyr-workshop/main/images/tuning-overfitting-test-1.svg)\n\n## Dangers of overfitting âš ï¸ ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_train)\n#> # A tibble: 8,000 Ã— 10\n#>    .pred_class .pred_yes .pred_no tip   distance company local dow   month  hour\n#>    <fct>           <dbl>    <dbl> <fct>    <dbl> <fct>   <fct> <fct> <fct> <int>\n#>  1 yes             0.967   0.0333 yes      17.2  Chicagâ€¦ no    Thu   Feb      16\n#>  2 yes             0.935   0.0646 yes       0.88 City Sâ€¦ yes   Thu   Mar       8\n#>  3 yes             0.967   0.0333 yes      18.1  other   no    Mon   Feb      18\n#>  4 yes             0.949   0.0507 yes      12.2  Chicagâ€¦ no    Sun   Mar      21\n#>  5 yes             0.821   0.179  yes       0.94 Sun Taâ€¦ yes   Sat   Apr      23\n#>  6 yes             0.967   0.0333 yes      17.5  Flash â€¦ no    Fri   Mar      12\n#>  7 yes             0.967   0.0333 yes      17.7  other   no    Sun   Jan       6\n#>  8 yes             0.938   0.0616 yes       1.85 Taxicaâ€¦ no    Fri   Apr      12\n#>  9 yes             0.938   0.0616 yes       0.53 Sun Taâ€¦ no    Tue   Mar      18\n#> 10 yes             0.931   0.0694 yes       6.65 Taxicaâ€¦ no    Sun   Apr      11\n#> # â„¹ 7,990 more rows\n```\n:::\n\n\n\nWe call this \"resubstitution\" or \"repredicting the training set\"\n\n## Dangers of overfitting âš ï¸ ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_train) %>%\n  accuracy(tip, .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.928\n```\n:::\n\n\n\nWe call this a \"resubstitution estimate\"\n\n## Dangers of overfitting âš ï¸ ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_train) %>%\n  accuracy(tip, .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.928\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n:::\n:::\n\n## Dangers of overfitting âš ï¸ ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_train) %>%\n  accuracy(tip, .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.928\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_test) %>%\n  accuracy(tip, .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.908\n```\n:::\n\n\n:::\n:::\n\n. . .\n\nâš ï¸ Remember that we're demonstrating overfitting \n\n. . .\n\nâš ï¸ Don't use the test set until the *end* of your modeling analysis\n\n\n##  {background-image=\"https://media.giphy.com/media/55itGuoAJiZEEen9gg/giphy.gif\" background-size=\"70%\"}\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute bottom=\"0\" left=\"0\" width=\"150\" height=\"150\"}\n\n*Use `augment()` and a metric function to compute a classification metric like `brier_class()`.*\n\n*Compute the metrics for both training and testing data to demonstrate overfitting!*\n\n*Notice the evidence of overfitting!* âš ï¸\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"augment-metrics\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Dangers of overfitting âš ï¸ ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_train) %>%\n  brier_class(tip, .pred_yes)\n#> # A tibble: 1 Ã— 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 brier_class binary        0.0632\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit %>%\n  augment(taxi_test) %>%\n  brier_class(tip, .pred_yes)\n#> # A tibble: 1 Ã— 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 brier_class binary        0.0782\n```\n:::\n\n\n:::\n:::\n\n. . .\n\nWhat if we want to compare more models?\n\n. . .\n\nAnd/or more model configurations?\n\n. . .\n\nAnd we want to understand if these are important differences?\n\n# The testing data are precious ğŸ’\n\n# How can we use the *training* data to compare and evaluate different models? ğŸ¤”\n\n##  {background-color=\"white\" background-image=\"https://www.tmwr.org/premade/resampling.svg\" background-size=\"80%\"}\n\n## Cross-validation\n\n![](https://www.tmwr.org/premade/three-CV.svg)\n\n## Cross-validation\n\n![](https://www.tmwr.org/premade/three-CV-iter.svg)\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*If we use 10 folds, what percent of the training data*\n\n-   *ends up in analysis*\n-   *ends up in assessment*\n\n*for* **each** *fold?*\n\n![](images/taxi_spinning.svg){width=\"300\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"percent-in-folds\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Cross-validation ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvfold_cv(taxi_train) # v = 10 is default\n#> #  10-fold cross-validation \n#> # A tibble: 10 Ã— 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [7200/800]> Fold01\n#>  2 <split [7200/800]> Fold02\n#>  3 <split [7200/800]> Fold03\n#>  4 <split [7200/800]> Fold04\n#>  5 <split [7200/800]> Fold05\n#>  6 <split [7200/800]> Fold06\n#>  7 <split [7200/800]> Fold07\n#>  8 <split [7200/800]> Fold08\n#>  9 <split [7200/800]> Fold09\n#> 10 <split [7200/800]> Fold10\n```\n:::\n\n\n\n## Cross-validation ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWhat is in this?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_folds <- vfold_cv(taxi_train)\ntaxi_folds$splits[1:3]\n#> [[1]]\n#> <Analysis/Assess/Total>\n#> <7200/800/8000>\n#> \n#> [[2]]\n#> <Analysis/Assess/Total>\n#> <7200/800/8000>\n#> \n#> [[3]]\n#> <Analysis/Assess/Total>\n#> <7200/800/8000>\n```\n:::\n\n\n\n::: notes\nTalk about a list column, storing non-atomic types in dataframe\n:::\n\n## Cross-validation ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvfold_cv(taxi_train, v = 5)\n#> #  5-fold cross-validation \n#> # A tibble: 5 Ã— 2\n#>   splits              id   \n#>   <list>              <chr>\n#> 1 <split [6400/1600]> Fold1\n#> 2 <split [6400/1600]> Fold2\n#> 3 <split [6400/1600]> Fold3\n#> 4 <split [6400/1600]> Fold4\n#> 5 <split [6400/1600]> Fold5\n```\n:::\n\n\n\n## Cross-validation ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvfold_cv(taxi_train, strata = tip)\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [7200/800]> Fold01\n#>  2 <split [7200/800]> Fold02\n#>  3 <split [7200/800]> Fold03\n#>  4 <split [7200/800]> Fold04\n#>  5 <split [7200/800]> Fold05\n#>  6 <split [7200/800]> Fold06\n#>  7 <split [7200/800]> Fold07\n#>  8 <split [7200/800]> Fold08\n#>  9 <split [7200/800]> Fold09\n#> 10 <split [7200/800]> Fold10\n```\n:::\n\n\n\n. . .\n\nStratification often helps, with very little downside\n\n## Cross-validation ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWe'll use this setup:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntaxi_folds <- vfold_cv(taxi_train, v = 10, strata = tip)\ntaxi_folds\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [7200/800]> Fold01\n#>  2 <split [7200/800]> Fold02\n#>  3 <split [7200/800]> Fold03\n#>  4 <split [7200/800]> Fold04\n#>  5 <split [7200/800]> Fold05\n#>  6 <split [7200/800]> Fold06\n#>  7 <split [7200/800]> Fold07\n#>  8 <split [7200/800]> Fold08\n#>  9 <split [7200/800]> Fold09\n#> 10 <split [7200/800]> Fold10\n```\n:::\n\n\n\n. . .\n\nSet the seed when creating resamples\n\n# We are equipped with metrics and resamples!\n\n## Fit our model to the resamples\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_res <- fit_resamples(taxi_wflow, taxi_folds)\ntaxi_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 4\n#>    splits             id     .metrics         .notes          \n#>    <list>             <chr>  <list>           <list>          \n#>  1 <split [7200/800]> Fold01 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  2 <split [7200/800]> Fold02 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  3 <split [7200/800]> Fold03 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  4 <split [7200/800]> Fold04 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  5 <split [7200/800]> Fold05 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  6 <split [7200/800]> Fold06 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  7 <split [7200/800]> Fold07 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  8 <split [7200/800]> Fold08 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  9 <split [7200/800]> Fold09 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#> 10 <split [7200/800]> Fold10 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n```\n:::\n\n\n\n## Evaluating model performance ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_res %>%\n  collect_metrics()\n#> # A tibble: 2 Ã— 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.915    10 0.00309 Preprocessor1_Model1\n#> 2 roc_auc  binary     0.624    10 0.0105  Preprocessor1_Model1\n```\n:::\n\n\n\n::: notes\n`collect_metrics()` is one of a suite of `collect_*()` functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with `.` have a corresponding `collect_*()` function with options for common summaries.\n:::\n\n. . .\n\nWe can reliably measure performance using only the **training** data ğŸ‰\n\n## Comparing metrics ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_res %>%\n  collect_metrics() %>% \n  select(.metric, mean, n)\n#> # A tibble: 2 Ã— 3\n#>   .metric   mean     n\n#>   <chr>    <dbl> <int>\n#> 1 accuracy 0.915    10\n#> 2 roc_auc  0.624    10\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nThe ROC AUC previously was\n\n- 0.69 for the training set\n- 0.64 for test set\n:::\n:::\n\n. . .\n\nRemember that:\n\nâš ï¸ the training set gives you overly optimistic metrics\n\nâš ï¸ the test set is precious\n\n## Evaluating model performance ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the assessment set results\nctrl_taxi <- control_resamples(save_pred = TRUE)\ntaxi_res <- fit_resamples(taxi_wflow, taxi_folds, control = ctrl_taxi)\n\ntaxi_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 5\n#>    splits             id     .metrics         .notes           .predictions\n#>    <list>             <chr>  <list>           <list>           <list>      \n#>  1 <split [7200/800]> Fold01 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  2 <split [7200/800]> Fold02 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  3 <split [7200/800]> Fold03 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  4 <split [7200/800]> Fold04 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  5 <split [7200/800]> Fold05 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  6 <split [7200/800]> Fold06 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  7 <split [7200/800]> Fold07 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  8 <split [7200/800]> Fold08 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  9 <split [7200/800]> Fold09 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#> 10 <split [7200/800]> Fold10 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>\n```\n:::\n\n\n\n## Evaluating model performance ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the assessment set results\ntaxi_preds <- collect_predictions(taxi_res)\ntaxi_preds\n#> # A tibble: 8,000 Ã— 7\n#>    id     .pred_yes .pred_no  .row .pred_class tip   .config             \n#>    <chr>      <dbl>    <dbl> <int> <fct>       <fct> <chr>               \n#>  1 Fold01     0.938   0.0615    14 yes         yes   Preprocessor1_Model1\n#>  2 Fold01     0.946   0.0544    19 yes         yes   Preprocessor1_Model1\n#>  3 Fold01     0.973   0.0269    33 yes         yes   Preprocessor1_Model1\n#>  4 Fold01     0.903   0.0971    43 yes         yes   Preprocessor1_Model1\n#>  5 Fold01     0.973   0.0269    74 yes         yes   Preprocessor1_Model1\n#>  6 Fold01     0.903   0.0971   103 yes         yes   Preprocessor1_Model1\n#>  7 Fold01     0.915   0.0851   104 yes         no    Preprocessor1_Model1\n#>  8 Fold01     0.903   0.0971   124 yes         yes   Preprocessor1_Model1\n#>  9 Fold01     0.667   0.333    126 yes         yes   Preprocessor1_Model1\n#> 10 Fold01     0.949   0.0510   128 yes         yes   Preprocessor1_Model1\n#> # â„¹ 7,990 more rows\n```\n:::\n\n\n\n## Evaluating model performance ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_preds %>% \n  group_by(id) %>%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#> # A tibble: 30 Ã— 4\n#>    id     .metric  .estimator .estimate\n#>    <chr>  <chr>    <chr>          <dbl>\n#>  1 Fold01 accuracy binary         0.905\n#>  2 Fold02 accuracy binary         0.925\n#>  3 Fold03 accuracy binary         0.926\n#>  4 Fold04 accuracy binary         0.915\n#>  5 Fold05 accuracy binary         0.902\n#>  6 Fold06 accuracy binary         0.912\n#>  7 Fold07 accuracy binary         0.906\n#>  8 Fold08 accuracy binary         0.91 \n#>  9 Fold09 accuracy binary         0.918\n#> 10 Fold10 accuracy binary         0.931\n#> # â„¹ 20 more rows\n```\n:::\n\n\n\n## Where are the fitted models? ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}  {.annotation}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 5\n#>    splits             id     .metrics         .notes           .predictions\n#>    <list>             <chr>  <list>           <list>           <list>      \n#>  1 <split [7200/800]> Fold01 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  2 <split [7200/800]> Fold02 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  3 <split [7200/800]> Fold03 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  4 <split [7200/800]> Fold04 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  5 <split [7200/800]> Fold05 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  6 <split [7200/800]> Fold06 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  7 <split [7200/800]> Fold07 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  8 <split [7200/800]> Fold08 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#>  9 <split [7200/800]> Fold09 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>    \n#> 10 <split [7200/800]> Fold10 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble>\n```\n:::\n\n\n\n. . .\n\nğŸ—‘ï¸\n\n# Alternate resampling schemes\n\n## Bootstrapping\n\n![](https://www.tmwr.org/premade/bootstraps.svg)\n\n## Bootstrapping ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3214)\nbootstraps(taxi_train)\n#> # Bootstrap sampling \n#> # A tibble: 25 Ã— 2\n#>    splits              id         \n#>    <list>              <chr>      \n#>  1 <split [8000/2902]> Bootstrap01\n#>  2 <split [8000/2916]> Bootstrap02\n#>  3 <split [8000/3004]> Bootstrap03\n#>  4 <split [8000/2979]> Bootstrap04\n#>  5 <split [8000/2961]> Bootstrap05\n#>  6 <split [8000/2962]> Bootstrap06\n#>  7 <split [8000/3026]> Bootstrap07\n#>  8 <split [8000/2926]> Bootstrap08\n#>  9 <split [8000/2972]> Bootstrap09\n#> 10 <split [8000/2972]> Bootstrap10\n#> # â„¹ 15 more rows\n```\n:::\n\n\n\n##  {background-iframe=\"https://rsample.tidymodels.org/reference/index.html\"}\n\n::: footer\n:::\n\n## The whole game - status update\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/whole-game-transparent-resamples.jpg){fig-align='center' width=3543}\n:::\n:::\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create:*\n\n-   *Monte Carlo Cross-Validation sets*\n-   *validation set*\n\n(use the reference guide to find the functions)\n\n*Don't forget to set a seed when you resample!*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"try-rsample\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Monte Carlo Cross-Validation ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(322)\nmc_cv(taxi_train, times = 10)\n#> # Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n#> # A tibble: 10 Ã— 2\n#>    splits              id        \n#>    <list>              <chr>     \n#>  1 <split [6000/2000]> Resample01\n#>  2 <split [6000/2000]> Resample02\n#>  3 <split [6000/2000]> Resample03\n#>  4 <split [6000/2000]> Resample04\n#>  5 <split [6000/2000]> Resample05\n#>  6 <split [6000/2000]> Resample06\n#>  7 <split [6000/2000]> Resample07\n#>  8 <split [6000/2000]> Resample08\n#>  9 <split [6000/2000]> Resample09\n#> 10 <split [6000/2000]> Resample10\n```\n:::\n\n\n\n## Validation set ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(853)\ntaxi_val_split <- initial_validation_split(taxi, strata = tip)\nvalidation_set(taxi_val_split)\n#> # A tibble: 1 Ã— 2\n#>   splits              id        \n#>   <list>              <chr>     \n#> 1 <split [6000/2000]> validation\n```\n:::\n\n\n\n. . .\n\nA validation set is just another type of resample\n\n# Decision tree ğŸŒ³\n\n# Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ´ğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒ³ğŸŒµğŸŒµğŸŒ´ğŸŒ²ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒ´ğŸŒµğŸŒ´ğŸŒ²ğŸŒ´ğŸŒµğŸŒ²ğŸŒµğŸŒ´ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒµğŸŒ³ğŸŒ´ğŸŒ³\n\n## Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\n- Ensemble many decision tree models\n\n- All the trees vote! ğŸ—³ï¸\n\n- Bootstrap aggregating + random predictor sampling\n\n. . .\n\n- Often works well without tuning hyperparameters (more on this in Advanced tidymodels!), as long as there are enough trees\n\n## Create a random forest model ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#> Random Forest Model Specification (classification)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger\n```\n:::\n\n\n\n## Create a random forest model ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_wflow <- workflow(tip ~ ., rf_spec)\nrf_wflow\n#> â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> tip ~ .\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> Random Forest Model Specification (classification)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger\n```\n:::\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Use `fit_resamples()` and `rf_wflow` to:*\n\n-   *keep predictions*\n-   *compute metrics*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"try-fit-resamples\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Evaluating model performance ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl_taxi <- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res <- fit_resamples(rf_wflow, taxi_folds, control = ctrl_taxi)\ncollect_metrics(rf_res)\n#> # A tibble: 2 Ã— 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.923    10 0.00317 Preprocessor1_Model1\n#> 2 roc_auc  binary     0.616    10 0.0147  Preprocessor1_Model1\n```\n:::\n\n\n\n## The whole game - status update\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/whole-game-transparent-select.jpg){fig-align='center' width=3543}\n:::\n:::\n\n\n\n## The final fit ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} \n\nSuppose that we are happy with our random forest model.\n\nLet's fit the model on the training set and verify our performance using the test set.\n\n. . .\n\nWe've shown you `fit()` and `predict()` (+ `augment()`) but there is a shortcut:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# taxi_split has train + test info\nfinal_fit <- last_fit(rf_wflow, taxi_split) \n\nfinal_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 Ã— 6\n#>   splits              id               .metrics .notes   .predictions .workflow \n#>   <list>              <chr>            <list>   <list>   <list>       <list>    \n#> 1 <split [8000/2000]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n\n\n## What is in `final_fit`? ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(final_fit)\n#> # A tibble: 2 Ã— 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.914 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.638 Preprocessor1_Model1\n```\n:::\n\n\n\n. . .\n\nThese are metrics computed with the **test** set\n\n## What is in `final_fit`? ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_predictions(final_fit)\n#> # A tibble: 2,000 Ã— 7\n#>    id               .pred_yes .pred_no  .row .pred_class tip   .config          \n#>    <chr>                <dbl>    <dbl> <int> <fct>       <fct> <chr>            \n#>  1 train/test split     0.957   0.0426     4 yes         yes   Preprocessor1_Moâ€¦\n#>  2 train/test split     0.938   0.0621    10 yes         yes   Preprocessor1_Moâ€¦\n#>  3 train/test split     0.958   0.0416    19 yes         yes   Preprocessor1_Moâ€¦\n#>  4 train/test split     0.894   0.106     23 yes         yes   Preprocessor1_Moâ€¦\n#>  5 train/test split     0.943   0.0573    28 yes         yes   Preprocessor1_Moâ€¦\n#>  6 train/test split     0.979   0.0213    34 yes         yes   Preprocessor1_Moâ€¦\n#>  7 train/test split     0.954   0.0463    35 yes         yes   Preprocessor1_Moâ€¦\n#>  8 train/test split     0.928   0.0722    38 yes         yes   Preprocessor1_Moâ€¦\n#>  9 train/test split     0.985   0.0147    40 yes         yes   Preprocessor1_Moâ€¦\n#> 10 train/test split     0.948   0.0523    42 yes         no    Preprocessor1_Moâ€¦\n#> # â„¹ 1,990 more rows\n```\n:::\n\n\n\n## What is in `final_fit`? ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_workflow(final_fit)\n#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> tip ~ .\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  1000 \n#> Sample size:                      8000 \n#> Number of independent variables:  6 \n#> Mtry:                             2 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.07069778\n```\n:::\n\n\n\n. . .\n\nUse this for **prediction** on new data, like for deploying\n\n## The whole game\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/whole-game-final-performance.jpg){fig-align='center' width=3543}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}