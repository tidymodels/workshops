{
  "hash": "bbfc56edec294a8271b98e16fc4d47b6",
  "result": {
    "markdown": "---\ntitle: \"Annotations\"\n---\n\n\n\n\n# 01 - Introduction\n\n## ðŸ‘€\n\nThis page contains _annotations_ for selected slides. \n\nThere's a lot that we want to tell you. We don't want people to have to frantically scribble down things that we say that are not on the slides. \n\nWe've added sections to this document with longer explanations and links to other resources. \n\n\n\n## Finalize and verify\n\nThis is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\n\nThe important point here is that: **tidymodels does most of this work for you**. In other words, you don't have to directly specify which data are being used where. \n\nIn a later section, we will talk about methods of [resampling](https://www.tmwr.org/resampling.html). These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are [special cases of resampling](https://www.tmwr.org/resampling.html#validation) where there is a single \"resample\". \n\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/whole-game-final-resamples.svg){fig-align='center'}\n:::\n:::\n\n\nIn this case there is just \"testing\" and \"training\". Once the final model is determined, the entire training set is used for the last fit. \n\nThis is the process that will be used for the tree frog data. \n\n# 02 - Data Budget\n\n## Data splitting and spending\n\nWhat does `set.seed()` do? \n\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random). \n\nThink of PRN as a box that takes a starting value (the \"seed\") that produces random numbers using that starting value as an input into its process. \n\nIf we know a seed value, we can reproduce our \"random\" numbers. To use a different set of random numbers, choose a different seed value. \n\nFor example: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nrunif(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.2655087 0.3721239 0.5728534\n```\n:::\n\n```{.r .cell-code}\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.1848823 0.7023740 0.5733263\n```\n:::\n\n```{.r .cell-code}\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.2655087 0.3721239 0.5728534\n```\n:::\n:::\n\n\nIf we _donâ€™t_ set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible. \n\nSince we want our code to be reproducible, we set the seeds before random numbers are used. \n\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results. \n\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to \"spread the randomness around\". It is basically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> set.seed(9725)\n#> set.seed(8462)\n#> set.seed(4050)\n#> set.seed(8789)\n#> set.seed(1301)\n```\n:::\n:::\n\n\n# 03 - What Makes A Model?\n\n## What is wrong with this? \n\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand. \n\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\n* Your performance metrics are slightly-to-moderately optimistic (e.g. you might think your accuracy is 85% when it is actually 75%)\n* A consequential component of the analysis is not right and the model just doesnâ€™t work. \n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set. \n\nA really good example of this is in [â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™](https://pubmed.ncbi.nlm.nih.gov/11983868/). The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors. \n\nGenerally speaking, this problem is referred to as [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)). Some other references: \n\n * [Overfitting to Predictors and External Validation](https://bookdown.org/max/FES/selection-overfitting.html)\n * [Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract-round2.html)\n * [Navigating the pitfalls of applying machine learning in genomics](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Navigating+the+pitfalls+of+applying+machine+learning+in+genomics&btnG=)\n * [A review of feature selection techniques in bioinformatics](https://academic.oup.com/bioinformatics/article/23/19/2507/185254)\n * [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n\n\n# 04 - Evaluating Models\n\n## Where are the fitted models?\n\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again. \n\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we don't keep them. \n\nFor more advanced use cases, you can extract and save them. See:\n\n * <https://www.tmwr.org/resampling.html#extract>\n * <https://www.tidymodels.org/learn/models/coefficients/> (an example)\n\n\n## The final fit\n\nSince our data spending scheme created the resamples from the training set, `last_fit()` will use all of the training data to fit the final workflow. \n\nAs shown in the Whole Game slides, there is a slightly different scheme used when we have a validation set (instead of multiple resamples like 10-fold CV). \n\n\n# 05 - Feature Engineering\n\n\n## Using a workflow\n\nWhat's going on with the \n\n> prediction from a rank-deficient fit may be misleading\n\nwarnings? \n\nFor linear regression, a computation is used called _matrix inversion_. The matrix in question is called the \"model matrix\" and it contains the predictor set for the training data. \n\nMatrix inversion can fail if two or more columns: \n\n * are identical, or \n * add up to some other column. \n \nThese situations are called _linear dependencies_.  \n\nWhen this happens, `lm()` is pretty tolerant. It does not fail but does not compute regression coefficients for a minimal number of predictors involved in the dependency (and issues the warning above).\n\nFor these data, there is this dependency:\n\n```\nshooter_type_{level} = shooter_{level}\n```\n\nHere is what is happening: since the player shooting only ever plays a single position, their indicators sum up (row-wide) to the same data as the sum of the shooter type indicators. \n\nIn other words, if you know the player's name, you know their position too. This is a perfect redundency in the data. \n\nThe way to avoid this problem is to use `step_lincomb(all_numeric_predictors())` in the recipe. [This step](https://recipes.tidymodels.org/reference/step_lincomb.html) removes the minimum number of columns to avoid the issue. \n\nJust in case you ever want to figure out what the specific issues are, this code might help:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the exact data set used to fit the model. For a recipe, that is:\nprocessed_data <- \n  nhl_indicators %>% \n  prep() %>% \n  bake(new_data = NULL, all_predictors())\n\n# Let's capture their names:\npred_names <- names(processed_data)\n\n# caret has a function to determine the variables involved in each dependency\nissues <- caret::findLinearCombos(processed_data)\n# Convert the column index to names:\nissues_vars <- purrr::map(issues$linearCombos, ~ pred_names[.x])\n\n# caret proposes removing these columns to get rid of the issue. The choice\n# is pretty arbitrary: \nremoved_names <- pred_names[issues$remove]\n```\n:::\n\n\n**tl;dr**\n\nLinear regression detects some redundancies in the predictor set. We can ignore the warnings since `lm()` can deal with it or use [`step_lincomb()`](https://recipes.tidymodels.org/reference/step_lincomb.html) to avoid the warnings. \n\n\n## Per-player statistics\n\nThe effect encoding method essentially takes the effect of a variable, like player, and makes a data column for that effect. In our example, the ability of a player to have an on-goal shot is quantified by a model and then added as a data column to be used in the model. \n\nSuppose NHL rookie Max has a single shot in the data and it was on goal. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should have a 100% chance of being on goal. Thatâ€™s a very poor estimate since it is from a single data point. \n\nContrast this with seasoned player Davis, who has taken 250 shots and 75% of these were on goal. Davisâ€™s proportion is more predictive because it is estimated with better data (i.e., more total shots). \nPartial pooling leverages the entire data set and can borrow strength from all of the players. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a playerâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw proportion. Maxâ€™s data is not great and is \"shrunk\" towards the center of the overall on goal proportion. Since there is so little known about Maxâ€™s shot history, this is a better effect estimate (until more data is available for him). \n\nThe Stan documentation has a pretty good vignette on this:  <https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html>\n\nAlso, _Bayes Rules!_ has a nice section on this: <https://www.bayesrulesbook.com/chapter-15.html>\n\nIf the outcome were numeric, the effect would be the mean of the outcome per player. In this case, partial pooling is very similar to the Jamesâ€“Stein estimator: <https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator>\n\n## Player effects\n\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set). \n\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for players with small sample sizes. It canâ€™t correct for improper data usage or data leakage though. \n\n## Angle\n\nAbout geometry... \n\nThe coordinates for the rink are centered at `(0, 0)` and the goal lines are both 89 ft from center. The center of the goal on the left is at `(-89, 0)` and the right-hand goal is centered at `(89, 0)`. \n\nFor angle to center of the goal mouth, the formula is \n\n$$a = \\tan^{-1}\\left(\\frac{y}{x}\\right)$$\nThis is in radian units and we can convert to degrees using \n\n$$a = \\frac{180}{\\pi}\\tan^{-1}\\left(\\frac{y}{x}\\right)$$\nFor the angle to the goal, we need to alter $x$ and use `x* = (89 - abs(coord_x))` instead. \n\n# 06 - Tuning Hyperparameters\n\n## Update parameter ranges\n\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the `mtry` parameter in a random forests model, the code would look like\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameter_object %>% \n  update(mtry = mtry(c(1, 100)))\n```\n:::\n\n\nIn our case, the argument name is `deg_free` but we update it with `spline_degree()`. \n\n`deg_free` represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a $t$ distribution, we would call that argument `deg_free`. \n\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called `spline_degree()` to be used in these cases. \n\nHow can you tell when this happens? There is a helper function called `tunable()` and that gives information on how we make the default ranges for parameters. There is a column in these objects names `call_info`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nns_tunable <- \n  recipe(mpg ~ ., data = mtcars) %>% \n  step_ns(dis, deg_free = tune()) %>% \n  tunable()\n\nns_tunable\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 1 Ã— 5\n#>   name     call_info        source component component_id\n#>   <chr>    <list>           <chr>  <chr>     <chr>       \n#> 1 deg_free <named list [3]> recipe step_ns   ns_P1Tjg\n```\n:::\n\n```{.r .cell-code}\nns_tunable$call_info\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [[1]]\n#> [[1]]$pkg\n#> [1] \"dials\"\n#> \n#> [[1]]$fun\n#> [1] \"spline_degree\"\n#> \n#> [[1]]$range\n#> [1]  1 15\n```\n:::\n:::\n\n\n\n## Boosted tree tuning parameters\n\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\n * Directly tune it (`trees = tune()`)\n \n * Set it to one value and tune the number of early stopping iterations (`trees = 500`, `stop_iter = tune()`).\n\nEarly stopping is when we monitor the performance of the model. If the model doesn't make any improvements for `stop_iter` iterations, training stops. \n\nHere's an example where, after eleven iterations, performance starts to get worse. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](annotations_files/figure-html/early-stopping-1.svg)\n:::\n:::\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.  \n\nEarly stopping usually has good results and takes far less time. \n\nWe _could_ an engine argument called `validation` here. That's not an argument to any function in the xgboost package. \n\nparsnip has its own wrapper around (`xgboost::xgb.train()`) called `xgb_train()`. We use that here and it has a `validation` argument.\n\nHow would you know that? There are a few different ways:\n\n * Look at the documentation in `?boost_tree` and click on the `xgboost` entry in the engine list. \n * Check out the pkgdown reference website <https://parsnip.tidymodels.org/reference/index.html>\n * Run the `translate()` function on the parsnip specification object. \n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for xgboost). \n\n\n## The final fit to the NHL data\n\nRecall that `last_fit()` uses the objects produced by `initial_split()` to determine what data are used for the final model fit and which are used as the test set. \n\nFor the validation set, `last_fit()` will use the non-testing data to create the final model fit. This includes the training and validation set. \n\nThere is no agreement in the community on whether this is the best approach or if we should just use the training set. There are good arguments either way. \n\nIf you only want to use the training set for the final model, you can do this via: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining_data <- nhl_val$splits[[1]] %>% analysis()\n\n# Use `fit()` to train the model on just the training set\nfinal_glm_spline_wflow <- \n  glm_spline_wflow %>% \n  fit(data = training_data)\n\n# Create test set predictions\ntest_set_pred <- augment(final_glm_spline_wflow, nhl_test)\n\n# Setup and compute the test set metrics\ncls_metrics <- metric_set(roc_auc, accuracy)\n\ntest_res <- \n  test_set_pred %>% \n  cls_metrics(on_goal, estimate = .pred_class, .pred_yes)\ntest_res\n```\n:::\n\n\n## Explain yourself\n\nSome other resources: \n\n * [TMwR chapter _Explaining Models and Predictions_](https://www.tmwr.org/explain.html)\n * [_Explanatory Model Analysis_ book](https://ema.drwhy.ai)\n * [_Interpretable Machine Learning_ book](https://christophm.github.io/interpretable-ml-book/)\n * [_Definitions, methods, and applications in interpretable machine learning_](https://www.pnas.org/doi/10.1073/pnas.1900654116)\n \n\n## A tidymodels explainer\n\nFor our example, the angle was an original predictor. Recall that we made spline terms from this predictor, so there are derived features such as `angle_ns_1` and so on.\n\nOriginal versus derived doesn't affect local explainers since we are focused on a single prediction. \n\nFor global explainers, we should decide between: \n\n * explaining the overall affect of angle (lumping all its features into one importance score), or\n * explaining the effect of each term in the model (including `angle_ns_1` and so on).\n \nThe choice depends on what you want. For example, if we have an original date predictor and make features for month and year, is it more informative to know if date is important (overall) or exactly _how_ the date is important? You might want to look at it both ways. \n\n",
    "supporting": [
      "annotations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}