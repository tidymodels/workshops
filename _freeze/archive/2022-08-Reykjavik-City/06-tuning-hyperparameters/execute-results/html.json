{
  "hash": "5c33d206e30312184211ca3b8c74ee0c",
  "result": {
    "markdown": "---\ntitle: \"6 - Tuning Hyperparameters\"\nsubtitle: \"Machine learning with tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n\n## Tuning parameters\n\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\n. . .\n\nSome examples:\n\n-   Tree depth in decision trees\n-   Number of neighbors in a K-nearest neighbor model\n\n# Activation function in neural networks?\n\nSigmoidal functions, ReLu, etc.\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Number of PCA components to retain?\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Bayesian priors for model parameters?\n\n::: fragment\nHmmmm, probably not.\nThese are based on prior belief.\n‚ùå\n:::\n\n# Covariance/correlation matrix structure in mixed models?\n\n::: fragment\nYes, but it is unlikely to affect performance.\n:::\n\n::: fragment\nIt will impact inference though.\nü§î\n:::\n\n\n\n# Is the random seed a tuning parameter?\n\n::: fragment\nNope. It is not. \n‚ùå\n:::\n\n## Optimize tuning parameters\n\n-   Try different values and measure their performance.\n\n. . .\n\n-   Find good values for these parameters.\n\n. . .\n\n-   Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n\n## Optimize tuning parameters\n\nThe main two strategies for optimization are:\n\n. . .\n\n-   **Grid search** üí† which tests a pre-defined set of candidate values\n\n-   **Iterative search** üåÄ which suggests/estimates new values of candidate parameters to evaluate\n\n## Choosing tuning parameters ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\nLet's take our previous recipe and add a few changes:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"11-12\"}\nglm_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) ),\n    defensive_zone = ifelse(coord_x <= -25.5, 1, 0),\n    behind_goal_line = ifelse(coord_x >= 89, 1, 0)\n  ) %>%\n  step_zv(all_predictors()) %>%\n  step_ns(angle, deg_free = tune(\"angle\")) %>%\n  step_ns(coord_x, deg_free = tune(\"coord_x\")) %>%\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n. . .\n\nLet's `tune()` our spline terms!\n\n## Choosing tuning parameters ![](hexes/parsnip.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\nLet's take our previous recipe and add a few changes:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nglm_spline_wflow <-\n  workflow() %>%\n  add_model(logistic_reg()) %>%\n  add_recipe(glm_rec)\n```\n:::\n\n\n\n## {background-image=\"https://www.tmwr.org/figures/ames-latitude-splines-1.png\" background-size=\"contain\"}\n\n:::notes\nSplines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.\n\nMore spline terms = more \"wiggly\", i.e. flexibly model a nonlinear relationship\n\nHow many spline terms? This is called *degrees of freedom*\n\n2 and 5 look like they underfit; 20 and 100 look like they overfit\n:::\n\n## Splines and nonlinear relationships\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n:::notes\nOur hockey data exhibits nonlinear relationships\n\nWe can model nonlinearity like this via a *model* (later this afternoon) or *feature engineering*\n\nHow do we decide how \"wiggly\" or flexible to make our spline features? TUNING \n:::\n\n## Grid search\n\n#### Parameters\n\n-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\n\n-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.\n\n::: fragment\n#### Grids\n\n-   Create your grid manually or automatically.\n\n-   The `grid_*()` functions can make a grid.\n:::\n\n::: notes\nMost basic (but very effective) way to tune models\n:::\n\n## Create a grid ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spline_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>       angle deg_free nparam[+]\n#>     coord_x deg_free nparam[+]\n```\n:::\n\n\n::: fragment\nA parameter set can be updated (e.g. to change the ranges).\n:::\n\n## Create a grid ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 2\n#>    angle coord_x\n#>    <int>   <int>\n#>  1     5       8\n#>  2     1      13\n#>  3     8       6\n#>  4    11       5\n#>  5     4       1\n#>  6     6      10\n#>  7    10      15\n#>  8    12      13\n#>  9     8      13\n#> 10    10       6\n#> # ‚Ñπ 15 more rows\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n::: fragment\n-   A *space-filling design* like this tends to perform better than random grids.\n-   Space-filling designs are also usually more efficient than regular grids.\n:::\n:::\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create a grid for our tunable workflow.*\n\n*Try creating a regular grid.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"make-grid\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Create a grid ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nset.seed(12)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 25)\n\ngrid\n#> # A tibble: 225 √ó 2\n#>    angle coord_x\n#>    <int>   <int>\n#>  1     1       1\n#>  2     2       1\n#>  3     3       1\n#>  4     4       1\n#>  5     5       1\n#>  6     6       1\n#>  7     7       1\n#>  8     8       1\n#>  9     9       1\n#> 10    10       1\n#> # ‚Ñπ 215 more rows\n```\n:::\n\n\n:::notes\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the `deg_free` parameters only have a range of `1->15`.\n:::\n\n## Update parameter ranges ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5-6\"}\nset.seed(12)\ngrid <- \n  glm_spline_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(angle = spline_degree(c(2L, 50L)),\n         coord_x = spline_degree(c(2L, 50L))) %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 2\n#>    angle coord_x\n#>    <int>   <int>\n#>  1    14      27\n#>  2     4      42\n#>  3    26      20\n#>  4    36      16\n#>  5    13       3\n#>  6    20      33\n#>  7    31      49\n#>  8    40      44\n#>  9    24      45\n#> 10    34      18\n#> # ‚Ñπ 15 more rows\n```\n:::\n\n\n::: notes\nEven though `angle` is a `deg_free` parameter in `step_ns()`, we don't use the dials `deg_free()` object here. We have a special `spline_degree()` function that has better defaults for splines.\n:::\n\n## The results ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ngrid %>% \n  ggplot(aes(angle, coord_x)) +\n  geom_point(size = 4)\n```\n\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/show-grid-code-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Use the `tune_*()` functions to tune models\n\n## Spline grid search ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglm_spline_res <-\n  glm_spline_wflow %>%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\n\nglm_spline_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes           .predictions         \n#>   <list>              <chr>      <list>            <list>           <list>               \n#> 1 <split [5348/1338]> validation <tibble [50 √ó 6]> <tibble [2 √ó 3]> <tibble [33,450 √ó 8]>\n#> \n#> There were issues with some computations:\n#> \n#>   - Warning(s) x1: glm.fit: algorithm did not converge, glm.fit: fitted probabilities numerically 0 or 1 occurred\n#>   - Warning(s) x1: glm.fit: fitted probabilities numerically 0 or 1 occurred\n#> \n#> Run `show_notes(.Last.tune.result)` for more information.\n```\n:::\n\n\n::: notes\n-   `tune_grid()` is representative of tuning function syntax\n-   similar to `fit_resamples()`\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Tune our `glm_wflow`.*\n\n*What happens if you don't supply a `grid` argument to `tune_grid()`?*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"tune-glm\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Grid results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(glm_spline_res)\n```\n\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/autoplot-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(glm_spline_res)\n#> # A tibble: 50 √ó 8\n#>    angle coord_x .metric  .estimator  mean     n std_err .config              \n#>    <int>   <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1    14      27 accuracy binary     0.804     1      NA Preprocessor01_Model1\n#>  2    14      27 roc_auc  binary     0.815     1      NA Preprocessor01_Model1\n#>  3     4      42 accuracy binary     0.808     1      NA Preprocessor02_Model1\n#>  4     4      42 roc_auc  binary     0.820     1      NA Preprocessor02_Model1\n#>  5    26      20 accuracy binary     0.805     1      NA Preprocessor03_Model1\n#>  6    26      20 roc_auc  binary     0.819     1      NA Preprocessor03_Model1\n#>  7    36      16 accuracy binary     0.800     1      NA Preprocessor04_Model1\n#>  8    36      16 roc_auc  binary     0.817     1      NA Preprocessor04_Model1\n#>  9    13       3 accuracy binary     0.803     1      NA Preprocessor05_Model1\n#> 10    13       3 roc_auc  binary     0.807     1      NA Preprocessor05_Model1\n#> # ‚Ñπ 40 more rows\n```\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#> # A tibble: 50 √ó 7\n#>    id         angle coord_x .metric  .estimator .estimate .config              \n#>    <chr>      <int>   <int> <chr>    <chr>          <dbl> <chr>                \n#>  1 validation    14      27 accuracy binary         0.804 Preprocessor01_Model1\n#>  2 validation    14      27 roc_auc  binary         0.815 Preprocessor01_Model1\n#>  3 validation     4      42 accuracy binary         0.808 Preprocessor02_Model1\n#>  4 validation     4      42 roc_auc  binary         0.820 Preprocessor02_Model1\n#>  5 validation    26      20 accuracy binary         0.805 Preprocessor03_Model1\n#>  6 validation    26      20 roc_auc  binary         0.819 Preprocessor03_Model1\n#>  7 validation    36      16 accuracy binary         0.800 Preprocessor04_Model1\n#>  8 validation    36      16 roc_auc  binary         0.817 Preprocessor04_Model1\n#>  9 validation    13       3 accuracy binary         0.803 Preprocessor05_Model1\n#> 10 validation    13       3 roc_auc  binary         0.807 Preprocessor05_Model1\n#> # ‚Ñπ 40 more rows\n```\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 5 √ó 8\n#>   angle coord_x .metric .estimator  mean     n std_err .config              \n#>   <int>   <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1     4      42 roc_auc binary     0.820     1      NA Preprocessor02_Model1\n#> 2    26      20 roc_auc binary     0.819     1      NA Preprocessor03_Model1\n#> 3    36      16 roc_auc binary     0.817     1      NA Preprocessor04_Model1\n#> 4    40      44 roc_auc binary     0.817     1      NA Preprocessor08_Model1\n#> 5    37      27 roc_auc binary     0.816     1      NA Preprocessor15_Model1\n```\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nCreate your own tibble for final parameters or use one of the `tune::select_*()` functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#> # A tibble: 1 √ó 3\n#>   angle coord_x .config              \n#>   <int>   <int> <chr>                \n#> 1     4      42 Preprocessor02_Model1\n```\n:::\n\n\n. . .\n\nThis best result has:\n\n-   low-degree spline for `angle` (less \"wiggly\", less complex)\n-   higher-degree spline for `coord_x` (more \"wiggly\", more complex)\n\n\n# Boosted trees üå≥üå≤üå¥üåµüå¥üå≥üå≥üå¥üå≤üåµüå¥üå≤üå≥üå¥üå≥üåµüåµüå¥üå≤üå≤üå≥üå¥üå≥üå¥üå≤üå¥üåµüå¥üå≤üå¥üåµüå≤üåµüå¥üå≤üå≥üå¥üåµüå≥üå¥üå≥\n\n## Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\n-   Ensemble many decision tree models\n\n::: fragment\n### Review how a decision tree model works:\n\n-   Series of splits or if/then statements based on predictors\n\n-   First the tree *grows* until some condition is met (maximum depth, no more data)\n\n-   Then the tree is *pruned* to reduce its complexity\n:::\n\n## Single decision tree\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/tree-example-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Boosted trees üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ\n\nBoosting methods fit a *sequence* of tree-based models.\n\n. . .\n\n-   Each tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\n\n-   This is like gradient-based steepest ascent methods from calculus.\n\n## Boosted tree tuning parameters  {.annotation}\n\nMost modern boosting methods have *a lot* of tuning parameters!\n\n. . .\n\n-   For tree growth and pruning (`min_n`, `max_depth`, etc)\n\n-   For boosting (`trees`, `stop_iter`, `learn_rate`)\n\n. . .\n\nWe'll use *early stopping* to stop boosting when a few iterations produce consecutively worse results.\n\n## Comparing tree ensembles\n\n::: columns\n::: {.column width=\"50%\"}\nRandom forest\n\n* Independent trees\n* Bootstrapped data\n* No pruning\n* 1000's of trees\n:::\n\n::: {.column width=\"50%\"}\nBoosting\n\n* Dependent trees\n* Different case weights\n* Tune tree parameters\n* Far fewer trees\n:::\n:::\n\nThe general consensus for tree-based models is, in terms of performance: boosting > random forest > bagging > single trees.\n\n\n## Boosted tree code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <-\n  boost_tree(\n    trees = tune(), min_n = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %>%\n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n\nxgb_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nxgb_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(xgb_rec)\n```\n:::\n\n\n:::notes\n`validation` is an argument to `parsnip::xgb_train()`, not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to `xgb.train(watchlist = list(validation = data))`.\n\nSee `translate(xgb_spec)` to see where it is passed to `parsnip::xgb_train()`.\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create your boosted tree workflow.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"xgb-wflow\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Running in parallel\n\n::: columns\n::: {.column width=\"60%\"}\n-   Grid search, combined with resampling, requires fitting a lot of models!\n\n-   These models don't depend on one another and can be run in parallel.\n\nWe can use a *parallel backend* to do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncores <- parallelly::availableCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/resample-times-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Running in parallel\n\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/unnamed-chunk-12-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n:::notes\nFaceted on the expensiveness of preprocessing used.\n:::\n\n## Tuning ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nThis will take some time to run ‚è≥\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\n\nxgb_res <-\n  xgb_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 30, control = ctrl) # automatic grid now!\n```\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Start tuning the boosted tree model!*\n\n*We won't wait for everyone's tuning to finish, but take this time to get it started before we move on.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"tune-xgboost\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res\n#> # Tuning results\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 5\n#>   splits              id         .metrics          .notes           .predictions          \n#>   <list>              <chr>      <list>            <list>           <list>                \n#> 1 <split [5348/1338]> validation <tibble [60 √ó 9]> <tibble [0 √ó 3]> <tibble [40,140 √ó 11]>\n```\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(xgb_res)\n```\n\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/autoplot-xgboost-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n## Again with the location features\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_rec <- \n  xgb_rec %>%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) ),\n    defensive_zone = ifelse(coord_x <= -25.5, 1, 0),\n    behind_goal_line = ifelse(coord_x >= 89, 1, 0)\n  )\n\nxgb_coord_wflow <- \n  workflow() %>% \n  add_model(xgb_spec) %>% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res <-\n  xgb_coord_wflow %>%\n  tune_grid(resamples = nhl_val, grid = 30, control = ctrl)\n```\n:::\n\n\n## Did the machine figure it out? \n\nNo extra features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_res, metric = \"roc_auc\", n = 3)\n#> # A tibble: 3 √ó 11\n#>   trees min_n tree_depth learn_rate loss_reduction .metric .estimator  mean     n std_err .config              \n#>   <int> <int>      <int>      <dbl>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1  1697     3          3    0.0116       0.674     roc_auc binary     0.804     1      NA Preprocessor1_Model02\n#> 2  1264    16          2    0.00732      0.000340  roc_auc binary     0.803     1      NA Preprocessor1_Model12\n#> 3   569    39          4    0.0272       0.0000288 roc_auc binary     0.800     1      NA Preprocessor1_Model30\n```\n:::\n\n\nWith additional coordinate features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_coord_res, metric = \"roc_auc\", n = 3)\n#> # A tibble: 3 √ó 11\n#>   trees min_n tree_depth learn_rate loss_reduction .metric .estimator  mean     n std_err .config              \n#>   <int> <int>      <int>      <dbl>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1  1697     3          3    0.0116    0.674        roc_auc binary     0.807     1      NA Preprocessor1_Model02\n#> 2  1264    16          2    0.00732   0.000340     roc_auc binary     0.804     1      NA Preprocessor1_Model12\n#> 3   427    31          2    0.249     0.0000000773 roc_auc binary     0.803     1      NA Preprocessor1_Model23\n```\n:::\n\n\n\n## Compare models\n\nBest logistic regression results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.820     1      NA Preprocessor02_Model1\n```\n:::\n\n\n::: fragment\nBest boosting results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#> # A tibble: 1 √ó 6\n#>   .metric .estimator  mean     n std_err .config              \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 roc_auc binary     0.804     1      NA Preprocessor1_Model02\n```\n:::\n\n:::\n\n## Updating the workflow ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_auc <- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#> # A tibble: 1 √ó 3\n#>   angle coord_x .config              \n#>   <int>   <int> <chr>                \n#> 1     4      42 Preprocessor02_Model1\n\nglm_spline_wflow <-\n  glm_spline_wflow %>% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#> ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> 7 Recipe Steps\n#> \n#> ‚Ä¢ step_lencode_mixed()\n#> ‚Ä¢ step_dummy()\n#> ‚Ä¢ step_mutate()\n#> ‚Ä¢ step_zv()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_ns()\n#> ‚Ä¢ step_normalize()\n#> \n#> ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n:::\n\n\n## The final fit to the NHL data ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/workflows.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}  {.annotation}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_res <- \n  glm_spline_wflow %>% \n  last_fit(split = nhl_split)\n\ntest_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 √ó 6\n#>   splits              id               .metrics         .notes           .predictions         .workflow \n#>   <list>              <chr>            <list>           <list>           <list>               <list>    \n#> 1 <split [6686/2229]> train/test split <tibble [2 √ó 4]> <tibble [0 √ó 3]> <tibble [2,229 √ó 6]> <workflow>\n```\n:::\n\n\n. . .\n\nRemember that `last_fit()` fits one time with the combined training and validation set, then evaluates one time with the testing set.\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Finalize your workflow with the best parameters.*\n\n*Create a final fit.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"finalize-xgb\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Estimates of ROC AUC ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nValidation results from tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spline_res %>% \n  show_best(metric = \"roc_auc\", n = 1) %>% \n  select(.metric, mean, n, std_err)\n#> # A tibble: 1 √ó 4\n#>   .metric  mean     n std_err\n#>   <chr>   <dbl> <int>   <dbl>\n#> 1 roc_auc 0.820     1      NA\n```\n:::\n\n\n::: fragment\nTest set results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_res %>% collect_metrics()\n#> # A tibble: 2 √ó 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.800 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.807 Preprocessor1_Model1\n```\n:::\n\n:::\n\n## Final fitted workflow\n\nExtract the final fitted workflow, fit using the training set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_glm_spline_wflow <- \n  test_res %>% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#> # A tibble: 3 √ó 1\n#>   .pred_class\n#>   <fct>      \n#> 1 yes        \n#> 2 yes        \n#> 3 no\n```\n:::\n\n\n## Next steps\n\n\n-   [Document the model](https://vetiver.rstudio.com/learn-more/model-card.html).\n\n. . .\n\n-   [Deploy the model](https://vetiver.rstudio.com/get-started/).\n\n. . .\n\n-   Create an [applicability domain model](https://applicable.tidymodels.org/) to help monitor our data over time.\n\n. . .\n\n-   Use [explainers](https://www.tmwr.org/explain.html) to characterize the model and the predictions.\n\n\n## Explain yourself  {.annotation}\n\n\nThere are two categories of model explanations, **global** and **local**.\n\n. . .\n\n- Global model explanations provide an overall understanding aggregated over a _whole set_ of observations.\n\n- Local model explanations provide information about a prediction for a _single_ observation.\n\n\n. . .\n\nYou can also build global model explanations by aggregating local model explanations.\n\n# tidymodels integrates with model explainability frameworks\n\n![](https://dalex.drwhy.ai/misc/dalex_even.png){.absolute bottom=\"-300\" right=\"0\" width=\"300\"}\n\n## A tidymodels explainer  {.annotation}\n\nWe can build explainers using:\n\n- original, basic predictors\n- derived features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DALEXtra)\n\nglm_explainer <- explain_tidymodels(\n  final_glm_spline_wflow,\n  data = dplyr::select(nhl_train, -on_goal),\n  # DALEX required an integer for factors:\n  y = as.integer(nhl_train$on_goal) - 1,\n  verbose = FALSE\n)\n```\n:::\n\n\n## Explain the x coordinates\n\nWith our explainer, let's create [partial dependence profiles](https://ema.drwhy.ai/partialDependenceProfiles.html):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\npdp_coord_x <- model_profile(\n  glm_explainer,\n  variables = \"coord_x\",\n  N = 500,\n  groups = \"strength\"\n)\n```\n:::\n\n\n. . .\n\nYou can use the default `plot()` method or create your own visualization.\n\n## Explain the x coordinates\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/pdp-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Explain the x coordinates\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-tuning-hyperparameters_files/figure-revealjs/pdp-grouped-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create an explainer for our glm model.*\n\n*Try using other variables, like `extra_attacker` or `game_seconds`.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"explainer\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "06-tuning-hyperparameters_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.3.5/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}