{
  "hash": "31d20d9aedae2ca8f48a1788daabe492",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3 - Tuning Hyperparameters\"\nsubtitle: \"Advanced tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.path: \"figures/\"\n---\n\n\n\n\n\n\n\n\n## Previously - Setup  ![](hexes/tidymodels.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics <- metric_set(mae, rsq)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(hotel_rates)\nset.seed(295)\nhotel_rates <- \n  hotel_rates %>% \n  sample_n(5000) %>% \n  arrange(arrival_date) %>% \n  select(-arrival_date) %>% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n```\n:::\n\n\n\n\n:::\n\n::::\n\n\n## Previously - Data Usage  ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4028)\nhotel_split <- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train <- training(hotel_split)\nhotel_test <- testing(hotel_split)\n\nset.seed(472)\nhotel_rs <- vfold_cv(hotel_train, strata = avg_price_per_room)\n```\n:::\n\n\n\n\n## Previously - Feature engineering  ![](hexes/textrecipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(textrecipes)\n\nhash_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %>%\n  step_YeoJohnson(lead_time) %>%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %>%\n  step_dummy_hash(company) %>%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n```\n:::\n\n\n\n# Optimizing Models via Tuning Parameters\n\n## Tuning parameters\n\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\n. . .\n\nSome examples:\n\n- Tree depth in decision trees\n- Number of neighbors in a K-nearest neighbor model\n\n# Activation function in neural networks?\n\nSigmoidal functions, ReLu, etc.\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Number of feature hashing columns to generate?\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Bayesian priors for model parameters?\n\n::: fragment\nHmmmm, probably not.\nThese are based on prior belief.\n‚ùå\n:::\n\n\n# The random seed?\n\n::: fragment\nNope. It is not. \n‚ùå\n:::\n\n## Optimize tuning parameters\n\n- Try different values and measure their performance.\n\n. . .\n\n- Find good values for these parameters.\n\n. . .\n\n- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n\n\n## Tagging parameters for tuning  ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWith tidymodels, you can mark the parameters that you want to optimize with a value of `tune()`. \n\n<br>\n\nThe function itself just returns... itself: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune()\n#> tune()\nstr(tune())\n#>  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#> tune(\"I hope that the workshop is going well\")\n```\n:::\n\n\n\n. . . \n\nFor example...\n\n## Optimizing the hash features ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/textrecipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\nOur new recipe is: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4-5|\"}\nhash_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %>%\n  step_YeoJohnson(lead_time) %>%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %>%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n\n<br>\n\nWe will be using a tree-based model in a minute. \n\n - The other categorical predictors are left as-is.\n - That's why there is no `step_dummy()`. \n\n\n## Boosted Trees\n\nThese are popular ensemble methods that build a _sequence_ of tree models. \n\n<br>\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted. \n\n<br>\n\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble. \n\n<br>\n\nWe'll focus on the popular lightgbm implementation. \n\n## Boosted Tree Tuning Parameters\n\nSome _possible_ parameters: \n\n* `mtry`: The number of predictors randomly sampled at each split (in $[1, ncol(x)]$ or $(0, 1]$).\n* `trees`: The number of trees ($[1, \\infty]$, but usually up to thousands)\n* `min_n`: The number of samples needed to further split ($[1, n]$).\n* `learn_rate`: The rate that each tree adapts from previous iterations ($(0, \\infty]$, usual maximum is 0.1).\n* `stop_iter`: The number of iterations of boosting where _no improvement_ was shown before stopping ($[1, trees]$)\n\n## Boosted Tree Tuning Parameters\n\nTBH it is usually not difficult to optimize these models. \n\n<br>\n\nOften, there are multiple _candidate_ tuning parameter combinations that have very good results. \n\n<br>\n\nTo demonstrate simple concepts, we'll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate ($10^{-5}$ to $10^{-1}$).\n\n## Boosted Tree Tuning Parameters ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\nWe'll need to load the bonsai package. This has the information needed to use lightgbm\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bonsai)\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow <- workflow(hash_rec, lgbm_spec)\n```\n:::\n\n\n\n\n\n\n## Optimize tuning parameters\n\nThe main two strategies for optimization are:\n\n. . .\n\n- **Grid search** üí† which tests a pre-defined set of candidate values\n\n- **Iterative search** üåÄ which suggests/estimates new values of candidate parameters to evaluate\n\n\n## Grid search\n\nA small grid of points trying to minimize the error via learning rate: \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/small_init.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n## Grid search\n\nIn reality we would probably sample the space more densely: \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/grid_points.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n## Iterative Search\n\nWe could start with a few points and search the space:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](animations/anime_seq.gif){fig-align='center' width=60%}\n:::\n:::\n\n\n\n# Grid Search\n\n## Parameters\n\n-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\n\n-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.\n\n::: fragment\n#### Grids\n\n-   Create your grid manually or automatically.\n\n-   The `grid_*()` functions can make a grid.\n:::\n\n::: notes\nMost basic (but very effective) way to tune models\n:::\n\n## Different types of grids ![](hexes/dials.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}  {.annotation}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/grid-types-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nSpace-filling designs (SFD) attempt to cover the parameter space without redundant candidates. We recommend these the most.\n\n## Create a grid ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 4 parameters for tuning\n#> \n#>    identifier       type    object\n#>         trees      trees nparam[+]\n#>    learn_rate learn_rate nparam[+]\n#>    agent hash  num_terms nparam[+]\n#>  company hash  num_terms nparam[+]\n\n# Individual functions: \ntrees()\n#> # Trees (quantitative)\n#> Range: [1, 2000]\nlearn_rate()\n#> Learning Rate (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-10, -1]\n```\n:::\n\n\n\n::: fragment\nA parameter set can be updated (e.g. to change the ranges).\n:::\n\n## Create a grid ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\ngrid <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_space_filling(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 4\n#>    trees learn_rate `agent hash` `company hash`\n#>    <int>      <dbl>        <int>          <int>\n#>  1     1   7.50e- 6          574            574\n#>  2    84   1.78e- 5         2048           2298\n#>  3   167   5.62e-10         1824            912\n#>  4   250   4.22e- 5         3250            512\n#>  5   334   1.78e- 8          512           2896\n#>  6   417   1.33e- 3          322           1625\n#>  7   500   1   e- 1         1448           1149\n#>  8   584   1   e- 7         1290            256\n#>  9   667   2.37e-10          456            724\n#> 10   750   1.78e- 2          645            322\n#> # ‚Ñπ 15 more rows\n```\n:::\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create a grid for our tunable workflow.*\n\n*Try creating a regular grid.*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"make-grid\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Create a regular grid ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nset.seed(12)\ngrid <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 4)\n\ngrid\n#> # A tibble: 256 √ó 4\n#>    trees   learn_rate `agent hash` `company hash`\n#>    <int>        <dbl>        <int>          <int>\n#>  1     1 0.0000000001          256            256\n#>  2   667 0.0000000001          256            256\n#>  3  1333 0.0000000001          256            256\n#>  4  2000 0.0000000001          256            256\n#>  5     1 0.0000001             256            256\n#>  6   667 0.0000001             256            256\n#>  7  1333 0.0000001             256            256\n#>  8  2000 0.0000001             256            256\n#>  9     1 0.0001                256            256\n#> 10   667 0.0001                256            256\n#> # ‚Ñπ 246 more rows\n```\n:::\n\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n<br>\n\n*What advantage would a regular grid have?* \n\n\n\n## Update parameter ranges ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4-5|\"}\nlgbm_param <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\ngrid <- \n  lgbm_param %>% \n  grid_space_filling(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 4\n#>    trees learn_rate `agent hash` `company hash`\n#>    <int>      <dbl>        <int>          <int>\n#>  1     1  0.00147            574            574\n#>  2     5  0.00215           2048           2298\n#>  3     9  0.0000215         1824            912\n#>  4    13  0.00316           3250            512\n#>  5    17  0.0001             512           2896\n#>  6    21  0.0147             322           1625\n#>  7    25  0.1               1448           1149\n#>  8    29  0.000215          1290            256\n#>  9    34  0.0000147          456            724\n#> 10    38  0.0464             645            322\n#> # ‚Ñπ 15 more rows\n```\n:::\n\n\n\n\n## The results ![](hexes/ggplot2.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ngrid %>% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](figures/sfd-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n\nNote that the learning rates are uniform on the log-10 scale and this shows 2 of 4 dimensions.\n\n\n# Use the `tune_*()` functions to tune models\n\n\n## Choosing tuning parameters ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=256 width=\"64\" height=\"74.24\"}\n\nLet's take our previous model and tune more parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2,12-13|\"}\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow <- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param <-\n  lgbm_wflow %>%\n  extract_parameter_set_dials() %>%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))\n```\n:::\n\n\n\n\n## Grid Search ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2|\"}\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE)\n\nlgbm_res <-\n  lgbm_wflow %>%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n```\n:::\n\n\n\n::: notes\n-   `tune_grid()` is representative of tuning function syntax\n-   similar to `fit_resamples()`\n:::\n\n\n\n## Grid Search ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_res \n#> # Tuning results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 5\n#>    splits             id     .metrics          .notes           .predictions        \n#>    <list>             <chr>  <list>            <list>           <list>              \n#>  1 <split [3372/377]> Fold01 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,425 √ó 9]>\n#>  2 <split [3373/376]> Fold02 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,400 √ó 9]>\n#>  3 <split [3373/376]> Fold03 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,400 √ó 9]>\n#>  4 <split [3373/376]> Fold04 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,400 √ó 9]>\n#>  5 <split [3373/376]> Fold05 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,400 √ó 9]>\n#>  6 <split [3374/375]> Fold06 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,375 √ó 9]>\n#>  7 <split [3375/374]> Fold07 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,350 √ó 9]>\n#>  8 <split [3376/373]> Fold08 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,325 √ó 9]>\n#>  9 <split [3376/373]> Fold09 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,325 √ó 9]>\n#> 10 <split [3376/373]> Fold10 <tibble [50 √ó 9]> <tibble [0 √ó 4]> <tibble [9,325 √ó 9]>\n```\n:::\n\n\n\n\n## Grid results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(lgbm_res)\n```\n\n::: {.cell-output-display}\n![](figures/autoplot-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lgbm_res)\n#> # A tibble: 50 √ó 11\n#>    trees min_n learn_rate `agent hash` `company hash` .metric .estimator   mean     n std_err .config              \n#>    <int> <int>      <dbl>        <int>          <int> <chr>   <chr>       <dbl> <int>   <dbl> <chr>                \n#>  1   298    19   4.15e- 9          222             36 mae     standard   53.2      10 0.427   Preprocessor01_Model1\n#>  2   298    19   4.15e- 9          222             36 rsq     standard    0.810    10 0.00686 Preprocessor01_Model1\n#>  3  1394     5   5.82e- 6           28             21 mae     standard   52.9      10 0.424   Preprocessor02_Model1\n#>  4  1394     5   5.82e- 6           28             21 rsq     standard    0.810    10 0.00800 Preprocessor02_Model1\n#>  5   774    12   4.41e- 2           27             95 mae     standard    9.77     10 0.155   Preprocessor03_Model1\n#>  6   774    12   4.41e- 2           27             95 rsq     standard    0.946    10 0.00341 Preprocessor03_Model1\n#>  7  1342     7   6.84e-10           71             17 mae     standard   53.2      10 0.427   Preprocessor04_Model1\n#>  8  1342     7   6.84e-10           71             17 rsq     standard    0.811    10 0.00785 Preprocessor04_Model1\n#>  9   669    39   8.62e- 7          141            145 mae     standard   53.2      10 0.426   Preprocessor05_Model1\n#> 10   669    39   8.62e- 7          141            145 rsq     standard    0.807    10 0.00639 Preprocessor05_Model1\n#> # ‚Ñπ 40 more rows\n```\n:::\n\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lgbm_res, summarize = FALSE)\n#> # A tibble: 500 √ó 10\n#>    id     trees min_n    learn_rate `agent hash` `company hash` .metric .estimator .estimate .config              \n#>    <chr>  <int> <int>         <dbl>        <int>          <int> <chr>   <chr>          <dbl> <chr>                \n#>  1 Fold01   298    19 0.00000000415          222             36 mae     standard      51.8   Preprocessor01_Model1\n#>  2 Fold01   298    19 0.00000000415          222             36 rsq     standard       0.821 Preprocessor01_Model1\n#>  3 Fold02   298    19 0.00000000415          222             36 mae     standard      52.1   Preprocessor01_Model1\n#>  4 Fold02   298    19 0.00000000415          222             36 rsq     standard       0.804 Preprocessor01_Model1\n#>  5 Fold03   298    19 0.00000000415          222             36 mae     standard      52.2   Preprocessor01_Model1\n#>  6 Fold03   298    19 0.00000000415          222             36 rsq     standard       0.786 Preprocessor01_Model1\n#>  7 Fold04   298    19 0.00000000415          222             36 mae     standard      51.7   Preprocessor01_Model1\n#>  8 Fold04   298    19 0.00000000415          222             36 rsq     standard       0.826 Preprocessor01_Model1\n#>  9 Fold05   298    19 0.00000000415          222             36 mae     standard      55.2   Preprocessor01_Model1\n#> 10 Fold05   298    19 0.00000000415          222             36 rsq     standard       0.845 Preprocessor01_Model1\n#> # ‚Ñπ 490 more rows\n```\n:::\n\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(lgbm_res, metric = \"rsq\")\n#> # A tibble: 5 √ó 11\n#>   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#>   <int> <int>      <dbl>        <int>          <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1  1890    10    0.0159           115            174 rsq     standard   0.948    10 0.00334 Preprocessor12_Model1\n#> 2   774    12    0.0441            27             95 rsq     standard   0.946    10 0.00341 Preprocessor03_Model1\n#> 3  1638    36    0.0409            15            120 rsq     standard   0.945    10 0.00384 Preprocessor16_Model1\n#> 4   963    23    0.00556          157             13 rsq     standard   0.937    10 0.00320 Preprocessor06_Model1\n#> 5   590     5    0.00320           85             73 rsq     standard   0.908    10 0.00465 Preprocessor24_Model1\n```\n:::\n\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nCreate your own tibble for final parameters or use one of the `tune::select_*()` functions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_best <- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#> # A tibble: 1 √ó 6\n#>   trees min_n learn_rate `agent hash` `company hash` .config              \n#>   <int> <int>      <dbl>        <int>          <int> <chr>                \n#> 1  1890    10     0.0159          115            174 Preprocessor12_Model1\n```\n:::\n\n\n\n## Checking Calibration ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/probably.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nlibrary(probably)\nlgbm_res %>%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %>%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred\n  )\n```\n\n::: {.cell-output-display}\n![](figures/lgb-cal-plot-1.svg){width=90%}\n:::\n:::\n\n\n\n\n## Running in parallel\n\n::: columns\n::: {.column width=\"60%\"}\n-   Grid search, combined with resampling, requires fitting a lot of models!\n\n-   These models don't depend on one another and can be run in parallel.\n\nWe can use a *parallel backend* to do this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncores <- parallelly::availableCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)\n```\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/resample-times-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Running in parallel\n\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/parallel-speedup-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n\n:::notes\nFaceted on the expensiveness of preprocessing used.\n:::\n\n## The 'future' of parallel processing\n\nWe have relied on the foreach package for parallel processing. \n\nWe will start the transition to using the future package in the upcoming version of the tune package (version 1.3.0). \n\nThere will be a period of backward compatibility where you can still use foreach with future via the doFuture package. After that, the transition to future will occur. \n\nOverall, there will be minimal changes to your code. \n\n\n## Early stopping for boosted trees {.annotation}\n\nWe have directly optimized the number of trees as a tuning parameter. \n\nInstead we could \n \n - Set the number of trees to a single large number.\n - Stop adding trees when performance gets worse. \n \nThis is known as \"early stopping\" and there is a parameter for that: `stop_iter`.\n\nEarly stopping has a potential to decrease the tuning time. \n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n<br>\n\n\n*Set `trees = 2000` and tune the `stop_iter` parameter.* \n\nNote that you will need to regenerate `lgbm_param` with your new workflow!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"lgbm-stop\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}