{
  "hash": "af229cb88c1852c275c9aa1719579617",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"5 - Iterative Search\"\nsubtitle: \"Advanced tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.path: \"figures/\"\n---\n\n\n\n\n\n\n\n## Previously - Setup\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(hotel_rates)\nset.seed(295)\nhotel_rates <- \n  hotel_rates %>% \n  sample_n(5000) %>% \n  arrange(arrival_date) %>% \n  select(-arrival_date) %>%  \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n```\n:::\n\n\n\n\n:::\n\n::::\n\n\n## Previously - Data Usage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4028)\nhotel_split <- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train <- training(hotel_split)\nhotel_test <- testing(hotel_split)\n\nset.seed(472)\nhotel_rs <- vfold_cv(hotel_train, strata = avg_price_per_room)\n```\n:::\n\n\n\n## Our Boosting Model\n\nWe used feature hashing to generate a smaller set of indicator columns to deal with the large number of levels for the agent and country predictors. \n\n<br>\n\nTree-based models (and a few others) don't require indicators for categorical predictors. They can split on these variables as-is. \n\n<br>\n\nWe'll keep all categorical predictors as factors and focus on optimizing additional boosting parameters. \n\n## Our Boosting Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_spec <- \n  boost_tree(trees = 1000, learn_rate = tune(), min_n = tune(), \n             tree_depth = tune(), loss_reduction = tune(), \n             stop_iter = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow <- workflow(avg_price_per_room ~ ., lgbm_spec)\n\nlgbm_param <- \n  lgbm_wflow %>%\n    extract_parameter_set_dials() %>%\n    update(learn_rate = learn_rate(c(-5, -1)))\n```\n:::\n\n\n\n## Iterative Search\n\nInstead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be. \n\n<br>\n\nSuppose that we are only tuning the learning rate in our boosted tree. \n\n<br>\n\nWe could do something like: \n\n```r\nmae_pred <- lm(mae ~ learn_rate, data = resample_results)\n```\n\nand use this to predict and rank new learning rate candidates. \n\n\n## Iterative Search\n\nA linear model probably isn't the best choice though (more in a minute). \n\nTo illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between MAE and learning rate. \n\nNow suppose that we used a grid of three points in the parameter range for learning rate...\n\n\n## A Large Grid\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/grid-large-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## A Three Point Grid\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/grid-large-sampled-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n## Gaussian Processes and Optimization  {.annotation}\n\nWe can make a \"meta-model\" with a small set of historical performance results. \n\n[Gaussian Processes](https://gaussianprocess.org/gpml/) (GP) models are a good choice to model performance. \n\n- It is a Bayesian model so we are using **Bayesian Optimization (BO)**.\n- For regression, we can assume that our data are multivariate normal. \n- We also define a _covariance_ function for the variance relationship between data points. A common one is:\n\n$$\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}$$\n\n\n:::notes\nGPs are good because \n\n- they are flexible regression models (in the sense that splines are flexible). \n- we need to get mean and variance predictions (and they are Bayesian)\n- their variability is based on spatial distances.\n\nSome people use random forests (with conformal variance estimates) or other methods but GPs are most popular.\n:::\n\n\n## Predicting Candidates\n\nThe GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g. MAE)\n\n - The _mean_ performance\n - The _variance_ of performance \n \nThe variance is mostly driven by spatial variability (the previous equation). \n\nThe predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data. \n\n\n## Your turn {transition=\"slide-in\"}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n*Your GP makes predictions on two new candidate tuning parameters.*  \n\n*We want to minimize MAE.* \n\n*Which should we choose?*\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/two-candidates-1.svg){width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"mean-var-trade\" data-update-every=\"1\" tabindex=\"0\" style=\"bottom:0;left:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n\n\n## GP Fit (ribbon is mean +/- 1SD)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/gp-iter-0-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## Choosing New Candidates\n\nThis isn't a very good fit but we can still use it.\n\nHow can we use the outputs to choose the next point to measure?\n\n<br> \n\n[_Acquisition functions_](https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html) take the predicted mean and variance and use them to balance: \n\n - _exploration_:  new candidates should explore new areas.\n - _exploitation_: new candidates must stay near existing values. \n\nExploration focuses on the variance, exploitation is about the mean. \n\n## Acquisition Functions\n\nWe'll use an acquisition function to select a new candidate.\n\nThe most popular method appears to be _expected improvement_ ([EI](https://arxiv.org/pdf/1911.12809.pdf)) above the current best results. \n \n  - Zero at existing data points. \n  - The _expected_ improvement is integrated over all possible improvement (\"expected\" in the probability sense). \n\nWe would probably pick the point with the largest EI as the next point. \n\n(There are other functions beyond EI.)\n\n## Expected Improvement\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/gp-iter-0-ei-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n## Iteration\n\nOnce we pick the candidate point, we measure performance for it (e.g. resampling). \n\n<br> \n\nAnother GP is fit, EI is recomputed, and so on. \n\n<br> \n\nWe stop when we have completed the allowed number of iterations _or_ if we don't see any improvement after a pre-set number of attempts. \n\n\n## GP Fit with four points\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/gp-iter-1-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## Expected Improvement\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/gp-iter-1-ei-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## GP Evolution\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](animations/anime_gp.gif){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## Expected Improvement Evolution\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](animations/anime_improvement.gif){fig-align='center' width=50%}\n:::\n:::\n\n\n\n## BO in tidymodels\n\nWe'll use a function called `tune_bayes()` that has very similar syntax to `tune_grid()`. \n\n<br> \n\nIt has an additional `initial` argument for the initial set of performance estimates and parameter combinations for the GP model. \n\n## Initial grid points\n\n`initial` can be the results of another `tune_*()` function or an integer (in which case `tune_grid()` is used under to hood to make such an initial set of results).\n \n - We'll run the optimization more than once, so let's make an initial grid of results to serve as the substrate for the BO. \n\n - I suggest at least the number of tuning parameters plus two as the initial grid for BO. \n\n## An Initial Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_metrics <- metric_set(mae, rsq)\n\nset.seed(12)\ninit_res <-\n  lgbm_wflow %>%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = nrow(lgbm_param) + 2,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(init_res, metric = \"mae\") %>% select(-.metric, -.estimator)\n#> # A tibble: 5 × 9\n#>   min_n tree_depth learn_rate loss_reduction stop_iter  mean     n std_err .config             \n#>   <int>      <int>      <dbl>          <dbl>     <int> <dbl> <int>   <dbl> <chr>               \n#> 1    16         12   0.0136         1.91e- 3         9  10.1    10   0.196 Preprocessor1_Model4\n#> 2     9          4   0.0415         5.21e- 9        13  10.2    10   0.167 Preprocessor1_Model1\n#> 3    25          8   0.00256        9.58e-10         7  14.1    10   0.278 Preprocessor1_Model7\n#> 4    22          9   0.00154        5.77e- 6         5  19.3    10   0.326 Preprocessor1_Model5\n#> 5    32          3   0.000144       3.02e+ 1        18  47.6    10   0.387 Preprocessor1_Model6\n```\n:::\n\n\n\n## BO using tidymodels\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1|6,8-11|\"}\nctrl_bo <- control_bayes(verbose_iter = TRUE) # <- for demonstration\n\nset.seed(15)\nlgbm_bayes_res <-\n  lgbm_wflow %>%\n  tune_bayes(\n    resamples = hotel_rs,\n    initial = init_res,     # <- initial results\n    iter = 20,\n    param_info = lgbm_param,\n    control = ctrl_bo,\n    metrics = reg_metrics\n  )\n#> Optimizing mae using the expected improvement\n#> \n#> ── Iteration 1 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=10.13 (@iter 0)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=32, tree_depth=12, learn_rate=0.0178, loss_reduction=1.03e-10, stop_iter=12\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tmae=10.08 (+/-0.175)\n#> \n#> ── Iteration 2 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=10.08 (@iter 1)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=15, tree_depth=14, learn_rate=0.0977, loss_reduction=0.00535, stop_iter=4\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tmae=9.719 (+/-0.187)\n#> \n#> ── Iteration 3 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.719 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=38, tree_depth=1, learn_rate=0.1, loss_reduction=0.0809, stop_iter=10\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=15.45 (+/-0.253)\n#> \n#> ── Iteration 4 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.719 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=32, tree_depth=1, learn_rate=0.00833, loss_reduction=1.31e-06, stop_iter=8\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=19.44 (+/-0.33)\n#> \n#> ── Iteration 5 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.719 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=18, tree_depth=8, learn_rate=0.0495, loss_reduction=1.4e-06, stop_iter=5\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.757 (+/-0.146)\n#> \n#> ── Iteration 6 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.719 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=3, tree_depth=14, learn_rate=0.0319, loss_reduction=4.02e-09, stop_iter=17\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.76 (+/-0.163)\n#> \n#> ── Iteration 7 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.719 (@iter 2)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=6, tree_depth=8, learn_rate=0.0883, loss_reduction=1.94e-08, stop_iter=4\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tmae=9.712 (+/-0.17)\n#> \n#> ── Iteration 8 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.712 (@iter 7)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=6, tree_depth=8, learn_rate=0.025, loss_reduction=7.82e-05, stop_iter=19\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.838 (+/-0.17)\n#> \n#> ── Iteration 9 ───────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.712 (@iter 7)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=32, tree_depth=6, learn_rate=0.0737, loss_reduction=2.15e-07, stop_iter=8\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=10.06 (+/-0.2)\n#> \n#> ── Iteration 10 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.712 (@iter 7)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=5, tree_depth=11, learn_rate=0.0451, loss_reduction=3.45e-10, stop_iter=7\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tmae=9.637 (+/-0.156)\n#> \n#> ── Iteration 11 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=2, tree_depth=7, learn_rate=0.0372, loss_reduction=2.44e-09, stop_iter=11\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.761 (+/-0.171)\n#> \n#> ── Iteration 12 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=26, tree_depth=15, learn_rate=0.00626, loss_reduction=0.00554, stop_iter=16\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=10.79 (+/-0.198)\n#> \n#> ── Iteration 13 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=29, tree_depth=10, learn_rate=0.0996, loss_reduction=4.5e-05, stop_iter=16\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.838 (+/-0.169)\n#> \n#> ── Iteration 14 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=12, tree_depth=13, learn_rate=0.085, loss_reduction=2.16, stop_iter=9\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.795 (+/-0.16)\n#> \n#> ── Iteration 15 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=4, tree_depth=9, learn_rate=0.0418, loss_reduction=0.00293, stop_iter=7\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.75 (+/-0.168)\n#> \n#> ── Iteration 16 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=6, tree_depth=15, learn_rate=0.0703, loss_reduction=5.15e-10, stop_iter=13\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.672 (+/-0.134)\n#> \n#> ── Iteration 17 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=27, tree_depth=15, learn_rate=0.0956, loss_reduction=3.74e-10, stop_iter=17\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.861 (+/-0.197)\n#> \n#> ── Iteration 18 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.637 (@iter 10)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=2, tree_depth=11, learn_rate=0.0871, loss_reduction=0.00196, stop_iter=18\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ♥ Newest results:\tmae=9.601 (+/-0.147)\n#> \n#> ── Iteration 19 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.601 (@iter 18)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=2, tree_depth=12, learn_rate=0.0991, loss_reduction=8.45e-06, stop_iter=14\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.61 (+/-0.17)\n#> \n#> ── Iteration 20 ──────────────────────────────────────────────────────\n#> \n#> i Current best:\t\tmae=9.601 (@iter 18)\n#> i Gaussian process model\n#> ✓ Gaussian process model\n#> i Generating 5000 candidates\n#> i Predicted candidates\n#> i min_n=4, tree_depth=15, learn_rate=0.0206, loss_reduction=1.46e-06, stop_iter=15\n#> i Estimating performance\n#> ✓ Estimating performance\n#> ⓧ Newest results:\tmae=9.881 (+/-0.177)\n```\n:::\n\n\n\n## Best results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(lgbm_bayes_res, metric = \"mae\") %>% select(-.metric, -.estimator)\n#> # A tibble: 5 × 10\n#>   min_n tree_depth learn_rate loss_reduction stop_iter  mean     n std_err .config .iter\n#>   <int>      <int>      <dbl>          <dbl>     <int> <dbl> <int>   <dbl> <chr>   <int>\n#> 1     2         11     0.0871       1.96e- 3        18  9.60    10   0.147 Iter18     18\n#> 2     2         12     0.0991       8.45e- 6        14  9.61    10   0.170 Iter19     19\n#> 3     5         11     0.0451       3.45e-10         7  9.64    10   0.156 Iter10     10\n#> 4     6         15     0.0703       5.15e-10        13  9.67    10   0.134 Iter16     16\n#> 5     6          8     0.0883       1.94e- 8         4  9.71    10   0.170 Iter7       7\n```\n:::\n\n\n\n\n## Plotting BO Results\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\")\n```\n\n::: {.cell-output-display}\n![](figures/autoplot-marginals-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## Plotting BO Results\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](figures/autoplot-param-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## Plotting BO Results\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](figures/autoplot-perf-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## ENHANCE\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\") +\n  ylim(c(9, 14))\n```\n\n::: {.cell-output-display}\n![](figures/autoplot-perf-zoomed-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n*Let's try a different acquisition function: `conf_bound(kappa)`.*\n\n*We'll use the `objective` argument to set it.*\n\n*Choose your own `kappa` value:*\n\n - *Larger values will explore the space more.* \n - *\"Large\" values are usually less than one.*\n\n**Bonus points**: Before the optimization is done, press `<esc>` and see what happens.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"conf-bound\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Notes\n\n- Stopping `tune_bayes()` will return the current results. \n\n- Parallel processing can still be used to more efficiently measure each candidate point. \n\n- There are [a lot of other iterative methods](https://github.com/topepo/Optimization-Methods-for-Tuning-Predictive-Models) that you can use. \n\n- The finetune package also has functions for [simulated annealing](https://www.tmwr.org/iterative-search.html#simulated-annealing) search. \n\n## Finalizing the Model\n\nLet's say that we've tried a lot of different models and we like our lightgbm model the most. \n\nWhat do we do now? \n\n  * Finalize the workflow by choosing the values for the tuning parameters. \n  * Fit the model on the entire training set. \n  * Verify performance using the test set. \n  * Document and publish the model(?)\n  \n## Locking Down the Tuning Parameters\n\nWe can take the results of the Bayesian optimization and accept the best results: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(lgbm_bayes_res, metric = \"mae\")\nfinal_wflow <- \n  lgbm_wflow %>% \n  finalize_workflow(best_param)\nfinal_wflow\n#> ══ Workflow ══════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: boost_tree()\n#> \n#> ── Preprocessor ──────────────────────────────────────────────────────\n#> avg_price_per_room ~ .\n#> \n#> ── Model ─────────────────────────────────────────────────────────────\n#> Boosted Tree Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#>   min_n = 2\n#>   tree_depth = 11\n#>   learn_rate = 0.0871075826616985\n#>   loss_reduction = 0.00195652467829182\n#>   stop_iter = 18\n#> \n#> Engine-Specific Arguments:\n#>   num_threads = 1\n#> \n#> Computational engine: lightgbm\n```\n:::\n\n\n\n## The Final Fit\n\nWe can use individual functions: \n\n```r\nfinal_fit <- final_wflow %>% fit(data = hotel_train)\n\n# then predict() or augment() \n# then compute metrics\n```\n\n<br> \n\nRemember that there is also a convenience function to do all of this: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3893)\nfinal_res <- final_wflow %>% last_fit(hotel_split, metrics = reg_metrics)\nfinal_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 × 6\n#>   splits              id               .metrics         .notes           .predictions         .workflow \n#>   <list>              <chr>            <list>           <list>           <list>               <list>    \n#> 1 <split [3749/1251]> train/test split <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1,251 × 4]> <workflow>\n```\n:::\n\n\n\n## Test Set Results\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>% \n  collect_predictions() %>% \n  cal_plot_regression(\n    truth = avg_price_per_room, \n    estimate = .pred)\n```\n:::\n\n\n\nTest set performance: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>% collect_metrics()\n#> # A tibble: 2 × 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 mae     standard       9.60  Preprocessor1_Model1\n#> 2 rsq     standard       0.949 Preprocessor1_Model1\n```\n:::\n\n\n:::\n\n::: {.column width=\"35%\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/test-cal-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n::::\n\nRecall that resampling predicted the MAE to be 9.601.\n\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}