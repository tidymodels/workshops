{
  "hash": "0e183a03b8c38cce1cf1df5e00d08e01",
  "result": {
    "markdown": "---\ntitle: \"2 - Tuning Hyperparameters\"\nsubtitle: \"Advanced tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.path: \"figures/\"\n---\n\n\n\n\n\n\n\n## Previously - Setup  ![](hexes/tidymodels.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(modeldatatoo)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics <- metric_set(mae, rsq)\n```\n:::\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(295)\nhotel_rates <- \n  data_hotel_rates() %>% \n  sample_n(5000) %>% \n  arrange(arrival_date) %>% \n  select(-arrival_date_num, -arrival_date) %>% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n```\n:::\n\n\n\n:::\n\n::::\n\n\n## Previously - Data Usage  ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4028)\nhotel_split <- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr <- training(hotel_split)\nhotel_te <- testing(hotel_split)\n\nset.seed(472)\nhotel_rs <- vfold_cv(hotel_tr, strata = avg_price_per_room)\n```\n:::\n\n\n\n## Previously - Feature engineering  ![](hexes/textrecipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(textrecipes)\n\nhash_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %>%\n  step_YeoJohnson(lead_time) %>%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %>%\n  step_dummy_hash(company) %>%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n```\n:::\n\n\n# Optimizing Models via Tuning Parameters\n\n## Tuning parameters\n\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\n. . .\n\nSome examples:\n\n- Tree depth in decision trees\n- Number of neighbors in a K-nearest neighbor model\n\n# Activation function in neural networks?\n\nSigmoidal functions, ReLu, etc.\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Number of feature hashing columns to generate?\n\n::: fragment\nYes, it is a tuning parameter.\n‚úÖ\n:::\n\n# Bayesian priors for model parameters?\n\n::: fragment\nHmmmm, probably not.\nThese are based on prior belief.\n‚ùå\n:::\n\n# Covariance/correlation matrix structure in mixed models?\n\n::: fragment\nYes, but it is unlikely to affect performance.\n:::\n\n::: fragment\nIt will impact inference though.\nü§î\n:::\n\n\n\n# Is the random seed a tuning parameter?\n\n::: fragment\nNope. It is not. \n‚ùå\n:::\n\n## Optimize tuning parameters\n\n- Try different values and measure their performance.\n\n. . .\n\n- Find good values for these parameters.\n\n. . .\n\n- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n\n\n## Tagging parameters for tuning  ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWith tidymodels, you can mark the parameters that you want to optimize with a value of `tune()`. \n\n<br>\n\nThe function itself just returns... itself: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune()\n#> tune()\nstr(tune())\n#>  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#> tune(\"I hope that the workshop is going well\")\n```\n:::\n\n\n. . . \n\nFor example...\n\n## Optimizing the hash features ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/textrecipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\nOur new recipe is: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4-5|\"}\nhash_rec <-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %>%\n  step_YeoJohnson(lead_time) %>%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %>%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n<br>\n\nWe will be using a tree-based model in a minute. \n\n - The other categorical predictors are left as-is.\n - That's why there is no `step_dummy()`. \n\n\n## Boosted Trees\n\nThese are popular ensemble methods that build a _sequence_ of tree models. \n\n<br>\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted. \n\n<br>\n\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble. \n\n<br>\n\nWe'll focus on the popular lightgbm implementation. \n\n## Boosted Tree Tuning Parameters\n\nSome _possible_ parameters: \n\n* `mtry`: The number of predictors randomly sampled at each split (in $[1, ncol(x)]$ or $(0, 1]$).\n* `trees`: The number of trees ($[1, \\infty]$, but usually up to thousands)\n* `min_n`: The number of samples needed to further split ($[1, n]$).\n* `learn_rate`: The rate that each tree adapts from previous iterations ($(0, \\infty]$, usual maximum is 0.1).\n* `stop_iter`: The number of iterations of boosting where _no improvement_ was shown before stopping ($[1, trees]$)\n\n## Boosted Tree Tuning Parameters\n\nTBH it is usually not difficult to optimize these models. \n\n<br>\n\nOften, there are multiple _candidate_ tuning parameter combinations that have very good results. \n\n<br>\n\nTo demonstrate simple concepts, we'll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate ($10^{-5}$ to $10^{-1}$).\n\n## Boosted Tree Tuning Parameters ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\nWe'll need to load the bonsai package. This has the information needed to use lightgbm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bonsai)\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow <- workflow(hash_rec, lgbm_spec)\n```\n:::\n\n\n\n\n\n## Optimize tuning parameters\n\nThe main two strategies for optimization are:\n\n. . .\n\n- **Grid search** üí† which tests a pre-defined set of candidate values\n\n- **Iterative search** üåÄ which suggests/estimates new values of candidate parameters to evaluate\n\n\n## Grid search\n\nA small grid of points trying to minimize the error via learning rate: \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/small_init.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## Grid search\n\nIn reality we would probably sample the space more densely: \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/grid_points.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## Iterative Search\n\nWe could start with a few points and search the space:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](animations/anime_seq.gif){fig-align='center' width=60%}\n:::\n:::\n\n\n# Grid Search\n\n## Parameters\n\n-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\n\n-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.\n\n::: fragment\n#### Grids\n\n-   Create your grid manually or automatically.\n\n-   The `grid_*()` functions can make a grid.\n:::\n\n::: notes\nMost basic (but very effective) way to tune models\n:::\n\n## Create a grid ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 4 parameters for tuning\n#> \n#>    identifier       type    object\n#>         trees      trees nparam[+]\n#>    learn_rate learn_rate nparam[+]\n#>    agent hash  num_terms nparam[+]\n#>  company hash  num_terms nparam[+]\n\n# Individual functions: \ntrees()\n#> # Trees (quantitative)\n#> Range: [1, 2000]\nlearn_rate()\n#> Learning Rate (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-10, -1]\n```\n:::\n\n\n::: fragment\nA parameter set can be updated (e.g. to change the ranges).\n:::\n\n## Create a grid ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n::: columns\n::: {.column width=\"65%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\ngrid <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 4\n#>    trees    learn_rate `agent hash` `company hash`\n#>    <int>         <dbl>        <int>          <int>\n#>  1  1629 0.00000440             524           1454\n#>  2  1746 0.0000000751          1009           2865\n#>  3    53 0.0000180             2313            367\n#>  4   442 0.000000445            347            460\n#>  5  1413 0.0000000208          3232            553\n#>  6  1488 0.0000578             3692            639\n#>  7   906 0.000385               602            332\n#>  8  1884 0.00000000101         1127            567\n#>  9  1812 0.0239                 961           1183\n#> 10   393 0.000000117            487           1783\n#> # ‚Ñπ 15 more rows\n```\n:::\n\n:::\n\n::: {.column width=\"35%\"}\n::: fragment\n-   A *space-filling design*  tends to perform better than random grids.\n-   Space-filling designs are also usually more efficient than regular grids.\n:::\n:::\n:::\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Create a grid for our tunable workflow.*\n\n*Try creating a regular grid.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"make-grid\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Create a regular grid ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nset.seed(12)\ngrid <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = 4)\n\ngrid\n#> # A tibble: 256 √ó 4\n#>    trees   learn_rate `agent hash` `company hash`\n#>    <int>        <dbl>        <int>          <int>\n#>  1     1 0.0000000001          256            256\n#>  2   667 0.0000000001          256            256\n#>  3  1333 0.0000000001          256            256\n#>  4  2000 0.0000000001          256            256\n#>  5     1 0.0000001             256            256\n#>  6   667 0.0000001             256            256\n#>  7  1333 0.0000001             256            256\n#>  8  2000 0.0000001             256            256\n#>  9     1 0.0001                256            256\n#> 10   667 0.0001                256            256\n#> # ‚Ñπ 246 more rows\n```\n:::\n\n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n<br>\n\n*What advantage would a regular grid have?* \n\n\n\n## Update parameter ranges ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} {.annotation}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4-5|\"}\nlgbm_param <- \n  lgbm_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\ngrid <- \n  lgbm_param %>% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#> # A tibble: 25 √ó 4\n#>    trees learn_rate `agent hash` `company hash`\n#>    <int>      <dbl>        <int>          <int>\n#>  1    75  0.000312          2991           1250\n#>  2     4  0.0000337          899           3088\n#>  3    15  0.0295             520           1578\n#>  4     8  0.0997            1256           3592\n#>  5    80  0.000622           419            258\n#>  6    70  0.000474          2499           1089\n#>  7    35  0.000165           287           2376\n#>  8    64  0.00137            389            359\n#>  9    58  0.0000250          616            881\n#> 10    84  0.0639            2311           2635\n#> # ‚Ñπ 15 more rows\n```\n:::\n\n\n\n## The results ![](hexes/ggplot2.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ngrid %>% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](figures/sfd-1.svg){fig-align='center'}\n:::\n:::\n\n\nNote that the learning rates are uniform on the log-10 scale. \n\n\n# Use the `tune_*()` functions to tune models\n\n\n## Choosing tuning parameters ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=256 width=\"64\" height=\"74.24\"}\n\nLet's take our previous model and tune more parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2,12-13|\"}\nlgbm_spec <- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow <- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param <-\n  lgbm_wflow %>%\n  extract_parameter_set_dials() %>%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))\n```\n:::\n\n\n\n## Grid Search ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2|\"}\nset.seed(9)\nctrl <- control_grid(save_pred = TRUE)\n\nlgbm_res <-\n  lgbm_wflow %>%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n```\n:::\n\n\n::: notes\n-   `tune_grid()` is representative of tuning function syntax\n-   similar to `fit_resamples()`\n:::\n\n\n\n## Grid Search ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_res \n#> # Tuning results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 √ó 5\n#>    splits             id     .metrics          .notes           .predictions        \n#>    <list>             <chr>  <list>            <list>           <list>              \n#>  1 <split [3372/377]> Fold01 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,425 √ó 9]>\n#>  2 <split [3373/376]> Fold02 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,400 √ó 9]>\n#>  3 <split [3373/376]> Fold03 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,400 √ó 9]>\n#>  4 <split [3373/376]> Fold04 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,400 √ó 9]>\n#>  5 <split [3373/376]> Fold05 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,400 √ó 9]>\n#>  6 <split [3374/375]> Fold06 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,375 √ó 9]>\n#>  7 <split [3375/374]> Fold07 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,350 √ó 9]>\n#>  8 <split [3376/373]> Fold08 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,325 √ó 9]>\n#>  9 <split [3376/373]> Fold09 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,325 √ó 9]>\n#> 10 <split [3376/373]> Fold10 <tibble [50 √ó 9]> <tibble [0 √ó 3]> <tibble [9,325 √ó 9]>\n```\n:::\n\n\n\n## Grid results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(lgbm_res)\n```\n\n::: {.cell-output-display}\n![](figures/autoplot-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lgbm_res)\n#> # A tibble: 50 √ó 11\n#>    trees min_n learn_rate `agent hash` `company hash` .metric .estimator   mean     n std_err .config              \n#>    <int> <int>      <dbl>        <int>          <int> <chr>   <chr>       <dbl> <int>   <dbl> <chr>                \n#>  1   298    19   4.15e- 9          222             36 mae     standard   53.2      10 0.427   Preprocessor01_Model1\n#>  2   298    19   4.15e- 9          222             36 rsq     standard    0.811    10 0.00785 Preprocessor01_Model1\n#>  3  1394     5   5.82e- 6           28             21 mae     standard   52.9      10 0.424   Preprocessor02_Model1\n#>  4  1394     5   5.82e- 6           28             21 rsq     standard    0.810    10 0.00857 Preprocessor02_Model1\n#>  5   774    12   4.41e- 2           27             95 mae     standard   10.5      10 0.175   Preprocessor03_Model1\n#>  6   774    12   4.41e- 2           27             95 rsq     standard    0.939    10 0.00381 Preprocessor03_Model1\n#>  7  1342     7   6.84e-10           71             17 mae     standard   53.2      10 0.427   Preprocessor04_Model1\n#>  8  1342     7   6.84e-10           71             17 rsq     standard    0.810    10 0.00903 Preprocessor04_Model1\n#>  9   669    39   8.62e- 7          141            145 mae     standard   53.2      10 0.426   Preprocessor05_Model1\n#> 10   669    39   8.62e- 7          141            145 rsq     standard    0.808    10 0.00661 Preprocessor05_Model1\n#> # ‚Ñπ 40 more rows\n```\n:::\n\n\n## Tuning results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lgbm_res, summarize = FALSE)\n#> # A tibble: 500 √ó 10\n#>    id     trees min_n    learn_rate `agent hash` `company hash` .metric .estimator .estimate .config              \n#>    <chr>  <int> <int>         <dbl>        <int>          <int> <chr>   <chr>          <dbl> <chr>                \n#>  1 Fold01   298    19 0.00000000415          222             36 mae     standard      51.8   Preprocessor01_Model1\n#>  2 Fold01   298    19 0.00000000415          222             36 rsq     standard       0.834 Preprocessor01_Model1\n#>  3 Fold02   298    19 0.00000000415          222             36 mae     standard      52.1   Preprocessor01_Model1\n#>  4 Fold02   298    19 0.00000000415          222             36 rsq     standard       0.801 Preprocessor01_Model1\n#>  5 Fold03   298    19 0.00000000415          222             36 mae     standard      52.2   Preprocessor01_Model1\n#>  6 Fold03   298    19 0.00000000415          222             36 rsq     standard       0.784 Preprocessor01_Model1\n#>  7 Fold04   298    19 0.00000000415          222             36 mae     standard      51.7   Preprocessor01_Model1\n#>  8 Fold04   298    19 0.00000000415          222             36 rsq     standard       0.828 Preprocessor01_Model1\n#>  9 Fold05   298    19 0.00000000415          222             36 mae     standard      55.2   Preprocessor01_Model1\n#> 10 Fold05   298    19 0.00000000415          222             36 rsq     standard       0.850 Preprocessor01_Model1\n#> # ‚Ñπ 490 more rows\n```\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(lgbm_res, metric = \"rsq\")\n#> # A tibble: 5 √ó 11\n#>   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#>   <int> <int>      <dbl>        <int>          <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1  1890    10    0.0159           115            174 rsq     standard   0.940    10 0.00369 Preprocessor12_Model1\n#> 2   774    12    0.0441            27             95 rsq     standard   0.939    10 0.00381 Preprocessor03_Model1\n#> 3  1638    36    0.0409            15            120 rsq     standard   0.938    10 0.00346 Preprocessor16_Model1\n#> 4   963    23    0.00556          157             13 rsq     standard   0.930    10 0.00358 Preprocessor06_Model1\n#> 5   590     5    0.00320           85             73 rsq     standard   0.905    10 0.00505 Preprocessor24_Model1\n```\n:::\n\n\n## Choose a parameter combination ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nCreate your own tibble for final parameters or use one of the `tune::select_*()` functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgbm_best <- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#> # A tibble: 1 √ó 6\n#>   trees min_n learn_rate `agent hash` `company hash` .config              \n#>   <int> <int>      <dbl>        <int>          <int> <chr>                \n#> 1   774    12     0.0441           27             95 Preprocessor03_Model1\n```\n:::\n\n\n## Checking Calibration ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/probably.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nlibrary(probably)\nlgbm_res %>%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %>%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred,\n    alpha = 1 / 3\n  )\n```\n\n::: {.cell-output-display}\n![](figures/lgb-cal-plot-1.svg){width=90%}\n:::\n:::\n\n\n\n## Running in parallel\n\n::: columns\n::: {.column width=\"60%\"}\n-   Grid search, combined with resampling, requires fitting a lot of models!\n\n-   These models don't depend on one another and can be run in parallel.\n\nWe can use a *parallel backend* to do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncores <- parallelly::availableCores(logical = FALSE)\ncl <- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/resample-times-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Running in parallel\n\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/parallel-speedup-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n:::notes\nFaceted on the expensiveness of preprocessing used.\n:::\n\n\n## Early stopping for boosted trees {.annotation}\n\nWe have directly optimized the number of trees as a tuning parameter. \n\nInstead we could \n \n - Set the number of trees to a single large number.\n - Stop adding trees when performance gets worse. \n \nThis is known as \"early stopping\" and there is a parameter for that: `stop_iter`.\n\nEarly stopping has a potential to decrease the tuning time. \n\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n<br>\n\n\n*Set `trees = 2000` and tune the `stop_iter` parameter.* \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"lgbm-stop\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}