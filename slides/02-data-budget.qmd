---
title: "2 - Your data budget"
subtitle: "Machine learning with tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    theme: [default, tidymodels.scss]
knitr:
  opts_chunk: 
    echo: true
---

```{r}
#| include: false
#| file: setup.R
```

##  {background-image="https://media.giphy.com/media/Lr3UeH9tYu3qJtsSUg/giphy.gif" background-size="600px"}

## Data on tree frog hatching

```{r}
library(tidyverse)

data("tree_frogs", package = "stacks")
tree_frogs <- tree_frogs %>%
  filter(!is.na(latency)) %>%
  select(-c(clutch, hatched))
```

-   Red-eyed tree frog embryos can hatch earlier than their normal \~7 days if they detect potential predator threat!
-   Type `?stacks::tree_frogs` to learn more about this dataset, including references.

## Data on tree frog hatching

```{r}
glimpse(tree_frogs)
```

## Data on tree frog hatching

-   N = 572
-   a numeric outcome `latency`
-   4 other variables
    -   `treatment`, `reflex`, and `t_o_d` are **nominal** predictors
    -   `age` is a **numeric** predictor

## Data splitting and spending

For machine learning, we typically split data into training and test sets:

. . .

-   The **training set** is used to estimate model parameters.
-   The **test set** is used to find an independent assessment of model performance.

. . .

Do not ðŸš« use the test set during training.

## 

```{r}
#| echo: false
#| fig.width: 12
#| fig.height: 3
#| 
set.seed(123)
library(tidymodels)
one_split <- slice(tree_frogs, 1:30) %>% 
  initial_split() %>% 
  tidy() %>% 
  add_row(Row = 1:30, Data = "Original") %>% 
  mutate(Data = case_when(
    Data == "Analysis" ~ "Training",
    Data == "Assessment" ~ "Testing",
    TRUE ~ Data
  )) %>% 
  mutate(Data = factor(Data, levels = c("Original", "Training", "Testing")))
all_split <-
  ggplot(one_split, aes(x = Row, y = fct_rev(Data), fill = Data)) + 
  geom_tile(color = "white",
            size = 1) + 
  scale_fill_manual(values = splits_pal, guide = "none") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = rel(2)),
        axis.text.x = element_blank(),
        legend.position = "top",
        panel.grid = element_blank()) +
  coord_equal(ratio = 1) +
  labs(x = NULL, y = NULL)
all_split
```

## 

::: r-fit-text
**The more data we spend** ðŸ¤‘
:::

::: r-fit-text
**the better estimates we'll get.**
:::

## Data splitting and spending

-   Spending too much data in training prevents us from a good assessment of predictive **performance**.

-   Spending too much in testing keeps us from finding a good estimate of model **parameters**.

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*When is a good time to split your data?*

```{r}
#| echo: false
countdown(minutes = 3)
```

# The testing data is precious ðŸ’Ž

## Data splitting and spending `r hexes("rsample")`

```{r}
library(tidymodels)

set.seed(123)
frog_split <- initial_split(tree_frogs, strata = latency)
frog_split
```

## Data splitting and spending `r hexes("rsample")`

```{r}
frog_train <- training(frog_split)
frog_test <- testing(frog_split)
```

## Data splitting and spending `r hexes("rsample")`

```{r}
glimpse(frog_train)
```

## Data splitting and spending `r hexes("rsample")`

```{r}
glimpse(frog_test)
```

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Split your data so 20% is held out for the test set.*

```{r}
#| echo: false
countdown(minutes = 5)
```

## Data splitting and spending `r hexes("rsample")`

```{r}
set.seed(123)
frog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)
frog_train <- training(frog_split)
frog_test <- testing(frog_split)

nrow(frog_train)
nrow(frog_test)
```

# What about a validation set?

## {background-color="white" background-image="https://www.tmwr.org/premade/validation.svg" background-size="800px"}

## {background-color="white" background-image="https://www.tmwr.org/premade/validation-alt.svg" background-size="800px"}

# Exploratory data analysis for ML ðŸ§

## 

```{r}
ggplot(frog_train, aes(latency)) +
  geom_histogram(bins = 20)
```

. . .

Remember how we used `strata = latency`?

## 

```{r}
quartiles <- quantile(frog_train$latency, probs = c(1:3)/4)
ggplot(frog_train, aes(latency)) +
  geom_histogram(bins = 20) +
  geom_vline(xintercept = quartiles, color = train_color, 
             size = 1.5, lty = 2)
```

. . .

Stratification often helps, with very little downside

## 

```{r}
ggplot(frog_train, aes(latency, treatment, fill = treatment)) +
  geom_boxplot(alpha = 0.5, show.legend = FALSE)
```

## 

```{r}
frog_train %>%
  ggplot(aes(latency, reflex, fill = reflex)) +
  geom_boxplot(alpha = 0.3, show.legend = FALSE)
```

## 

```{r}
ggplot(frog_train, aes(reflex, age)) +
  stat_summary_2d(aes(z = latency), 
                  alpha = 0.7, binwidth = c(1, 5e3)) +
  scale_fill_viridis_c() +
  labs(fill = "mean latency")
```

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Explore the `frog_train` data on your own!*

```{r}
#| echo: false
countdown(minutes = 5)
```

::: notes
Make a plot or summary and then share with neighbor
:::
