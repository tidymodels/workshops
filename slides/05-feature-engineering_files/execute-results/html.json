{
  "hash": "f5796e598ce8095467a48ceff75f97b6",
  "result": {
    "markdown": "---\ntitle: \"5 - Feature engineering\"\nsubtitle: \"Machine learning with tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    theme: [default, tidymodels.scss]\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n# What is feature engineering?\n\nFirst thing's first: what's a feature? \n\nI tend to think of a feature as some representation of a predictor that will be used in a model. \n\nOld-school features: \n\n * Interactions\n * Polynomial expansions/splines\n * PCA feature extraction\n \n\"Feature engineering\" sounds pretty cool, but let's take a minute to talk about _preprocessing_ data.  \n\n\n# Two types of preprocessing\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/fe_venn.svg){fig-align='center' width=75%}\n:::\n:::\n\n\n\n# Two types of preprocessing\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/fe_venn_info.svg){fig-align='center' width=75%}\n:::\n:::\n\n\n\n# Easy examples\n\nFor example, centering and scaling are definitely not feature engineering.\n\nConsider the `date` field in the data. If given as a raw predictor, it is converted to an integer. \n\nIt can be re-encoded as:\n\n* Days since a reference date \n* Day of the week\n* Month\n* Year\n* Indicators for holidays\n\n\n\n# Original column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/steve.gif){fig-align='center' width=35%}\n:::\n:::\n\n\n# Features\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/cap.png){fig-align='center' width=75%}\n:::\n:::\n\n\n(At least that's what we hope the difference looks like.)\n\n\n# General definitions\n\n* _Data preprocessing_ are the steps that you take to make your model successful. \n\n* _Feature engineering_ are what you do to the original predictors to make the model do the least work to predict the outcome as well as possible. \n\nWe'll demonstrate the recipes package for all of your data needs. \n\n\n# Recipes prepare your data for modeling\n\nThe package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data. \n    \nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets. \n    \nThe resulting processed output can then be used as inputs for statistical or machine learning models.\n\n\n## Case study\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nnames(season_2015)\n#>  [1] \"on_goal\"       \"period\"        \"period_type\"   \"coord_x\"      \n#>  [5] \"coord_y\"       \"game_time\"     \"strength\"      \"player\"       \n#>  [9] \"offense_goals\" \"defense_goals\" \"player_diff\"   \"offense_team\" \n#> [13] \"defence_team\"  \"game_type\"     \"position\"      \"dow\"          \n#> [17] \"month\"         \"year\"\n```\n:::\n\n\n## Splitting the NHL data ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(23)\nnhl_split <- initial_split(season_2015, prop = 3/4)\nnhl_split\n#> <Analysis/Assess/Total>\n#> <9110/3037/12147>\n\nnhl_train <- training(nhl_split)\nnhl_test  <- testing(nhl_split)\n\nc(training = nrow(nhl_train), testing = nrow(nhl_test))\n#> training  testing \n#>     9110     3037\n```\n:::\n\n\n# Validation split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nnhl_val <- validation_split(nhl_train, prop = 0.80)\nnhl_val\n#> # Validation Set Split (0.8/0.2)  \n#> # A tibble: 1 √ó 2\n#>   splits              id        \n#>   <list>              <chr>     \n#> 1 <split [7288/1822]> validation\n```\n:::\n\n\n# A first recipe ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train)\n\n# If ncol(data) is large, you can use\n# recipe(data = nhl_train)\n```\n:::\n\n\nBased on the formula, the function assigns columns to roles of \"outcome\" or \"predictor\"\n\n# A first recipe ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(nhl_rec)\n#> # A tibble: 18 √ó 4\n#>    variable      type    role      source  \n#>    <chr>         <chr>   <chr>     <chr>   \n#>  1 period        numeric predictor original\n#>  2 period_type   nominal predictor original\n#>  3 coord_x       numeric predictor original\n#>  4 coord_y       numeric predictor original\n#>  5 game_time     numeric predictor original\n#>  6 strength      nominal predictor original\n#>  7 player        nominal predictor original\n#>  8 offense_goals numeric predictor original\n#>  9 defense_goals numeric predictor original\n#> 10 player_diff   numeric predictor original\n#> 11 offense_team  nominal predictor original\n#> 12 defence_team  nominal predictor original\n#> 13 game_type     nominal predictor original\n#> 14 position      nominal predictor original\n#> 15 dow           nominal predictor original\n#> 16 month         nominal predictor original\n#> 17 year          numeric predictor original\n#> 18 on_goal       nominal outcome   original\n```\n:::\n\n\n# Create indicator variables ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\nFor any factor or character predictors, make binary indicators.\n\nThere are *many* recipe steps that can convert categorical predictors to numeric columns.\n\n# Filter out constant columns ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n```\n:::\n\n\nIn case there is factor level that never was observed in the trainnig data, we can delete any *zero-variance* predictors that have a single unique value.\n\n<!--Note that the selector chooses all columns with a role of \"predictor\".-->\n\n# Normalization ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\nThis centers and scales the numeric predictors.\n\nNote that this will use the training set to estimate the means and standard deviations of the data.\n\nAll data put through the recipe will be normalized using those statistics (there is no re-estimation).\n\n# Reduce correlation ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n```\n:::\n\n\nTo deal with highly correlated predictors, find the minimum predictor set to remove to make the pairwise correlations less than 0.9.\n\n# Other possible steps ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_pca(all_numeric_predictors())\n```\n:::\n\n\nPCA feature extraction...\n\n# Other possible steps ![](hexes/embed.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nlibrary(embed)\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_umap(all_numeric_predictors(), outcome = on_goal)\n```\n:::\n\n\nA fancy machine learning supervised dimension reduction technique\n\n# Other possible steps ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nnhl_rec <- \n  recipe(on_goal ~ ., data = nhl_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_ns(coord_y, coord_x, deg_free = 10)\n```\n:::\n\n\nNonlinear transforms like *natural splines* and so on.\n\n\n# Minimal recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n# Using a workflow\n\n::: columns\n::: {.column width=\"42%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nnhl_glm_wflow <-\n  workflow() %>%\n  add_recipe(nhl_rec) %>%\n  add_model(logistic_reg())\n \nctrl <- control_resamples(save_pred = TRUE)\nnhl_glm_res <-\n  nhl_glm_wflow %>%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#> # A tibble: 2 √ó 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.578     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.581     1      NA Preprocessor1_Model1\n```\n:::\n\n\n\n# Classification metrics\n\nplaceholder to talk about AUC ROC\n\n\n\n\n\n\n# What do we do with the player data?\n\nThere are 619 unique player values in our training set.\n\n-   We *could* make the full set of indicator variables...\n-   Or using [feature hashing](https://www.tmwr.org/categorical.html#feature-hashing) to make a subset.\n\nInstead, we will be using effect encoding to replace the `player` column with the estimated effect of that predictor.\n\n\n# Per-player statistics\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/effects-1.svg){fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/effects-2.svg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nThere are good statistical methods for estimating these rates that use *partial pooling*.\n\nThis borrows strength across players and shrinks extreme values (e.g. zero or one) towards the mean for players with very few shots.\n\nThe embed package has recipes steps for effect encodings.\n:::\n:::\n\n# Partial pooling\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/effect-compare-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n# Player effects ![](hexes/embed.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nlibrary(embed)\n\nnhl_effect_rec <-\n  recipe(on_goal ~ ., data = nhl_train) %>%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting.\n\n# Recipes are estimated\n\n*Every* preprocessing step in a recipe that involved calculations uses the *training set*.\nFor example:\n\n-   Levels of a factor\n-   Determination of zero-variance\n-   Normalization\n-   Feature extraction\n-   Effect encodings\n\nand so on.\n\nOnce a recipe is added to a workflow, this occurs when `fit()` is called.\n\n\n# Effect encoding results\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nnhl_effect_wflow <-\n  nhl_glm_wflow %>%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res <-\n  nhl_effect_wflow %>%\n  fit_resamples(nhl_val)\n\ncollect_metrics(nhl_effect_res)\n#> # A tibble: 2 √ó 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.549     1      NA Preprocessor1_Model1\n#> 2 roc_auc  binary     0.552     1      NA Preprocessor1_Model1\n```\n:::\n\n\n# Where is the shot coming from?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_angle_rec <-\n  nhl_rec %>%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi))\n  )\n```\n:::\n\n\n\n# \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_distance_rec <-\n  nhl_angle_rec %>%\n  step_mutate(\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2)\n  )\n```\n:::\n\n\n#\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_behind_rec <-\n  nhl_distance_rec %>%\n  step_mutate(\n    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)\n  )\n```\n:::\n\n\n\n# Fit different recipes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhl_glm_set_res <-\n  workflow_set(\n    list(dummy = nhl_rec, encoded = nhl_effect_rec,\n         angle = nhl_angle_rec, dist = nhl_distance_rec, \n         bgl = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %>%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val)\n```\n:::\n\n\n\n# Compare recipes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %>%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point()\n```\n:::\n\n\n# Compare recipes\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-feature-engineering_files/figure-revealjs/unnamed-chunk-14-1.svg)\n:::\n:::\n\n\n\n# Debugging a recipe\n\n90% of the time, you will want to use a workflow to estimate and apply a recipe.\n\nIf you have an error, the original recipe object (e.g. `encoded_players`) can be estimated manually with a function called `prep()`.\nIt is analogous to `fit()`.\n\nThis returns the fitted recipe.\nThis can help debug any issues.\n\nAnother function (`bake()`) is analogous to `predict()` and gives you the processed data back.\n\n# Fun facts about recipes\n\n-   Once `fit()` is called on a workflow, changing the model does not re-fit the recipe.\n-   A list of all known steps is [here](https://www.tidymodels.org/find/recipes/).\n-   Some steps can be [skipped](https://recipes.tidymodels.org/articles/Skipping.html) when using `predict()`.\n-   The [order](https://recipes.tidymodels.org/articles/Ordering.html) of the steps matters.\n-   There are recipes-adjacent packages with more steps: embed, timetk, textrecipes, and others.\n    -   If you do any text processing, textrecipes is üÜí<sup>‚ôæÔ∏è</sup>.\n        -   Julia and Emil have written an amazing text processing book: [*Supervised Machine Learning for Text Analysis in R*](https://smltar.com/)\n-   There are a lot of ways to handle [categorical predictors](https://recipes.tidymodels.org/articles/Dummies.html), even those with novel levels.\n-   Several dplyr steps exist, such as `step_mutate()`.\n\n",
    "supporting": [
      "05-feature-engineering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}