---
title: "1 - Tuning Hyperparameters"
subtitle: "Advanced tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```

```{r more-setup}
#| include: false
library(modeldatatoo)
library(probably)
library(countdown)

cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)

options(width = 200)

ggplot2::theme_set(ggplot2::theme_bw())
```

## Hotel Data

We'll use a version of the [hotel data](https://www.sciencedirect.com/science/article/pii/S2352340918315191). We'll use [a version](https://modeldatatoo.tidymodels.org/dev/reference/data_hotel_rates.html) where we try to predict the cost of a room. 

The data are in the modeldatatoo package. We'll sample down the data and refactor some columns: 

:::: {.columns}

::: {.column width="40%"}

```{r}
#| label: tune-startup
library(tidymodels)
library(modeldatatoo)

# Max's usual settings: 
tidymodels_prefer()
theme_set(theme_bw())
options(
  pillar.advice = FALSE, 
  pillar.min_title_chars = Inf
)
```

:::

::: {.column width="60%"}

```{r}
#| label: data-import
set.seed(295)
hotel_rates <- 
  data_hotel_rates() %>% 
  sample_n(5000) %>% 
  arrange(arrival_date) %>% 
  select(-arrival_date_num, -arrival_date) %>% 
  mutate(
    company = factor(as.character(company)),
    country = factor(as.character(country)),
    agent = factor(as.character(agent))
  )
```


:::

::::


## Data Spending

Let's split the data into a training set (75%) and testing set (25%):

```{r}
#| label: hotel-split
set.seed(4028)
hotel_split <-
  initial_split(hotel_rates, strata = avg_price_per_room)

hotel_tr <- training(hotel_split)
hotel_te <- testing(hotel_split)
```



## Your turn {transition="slide-in"}

Let's take some time and investigate the _training data_. The outcome is `avg_price_per_room`. 

Are there any characteristic of the data that are interesting?

```{r}
#| label: hotel-investigation-exercise
#| echo: false
countdown(minutes = 10, id = "hotel-investigation")
```


## Feature Engineering

How should we represent our predictors? There are: 

 * Categorical predictors, some with many levels
 * Some skewed numeric predictor(s)
 * Interactions? 

. . .

Let's start a basic recipe: 

```{r}
#| label: hotel-rec-start

hotel_rec <- 
  recipe(avg_price_per_room ~ ., data = hotel_tr) %>% 
  step_YeoJohnson(lead_time)
```

. . .

We'll add to this for different types of models. 


# Optimizing Models via Tuning Parameters



## Tuning parameters

Some model or preprocessing parameters cannot be estimated directly from the data.

. . .

Some examples:

- Tree depth in decision trees
- Number of neighbors in a K-nearest neighbor model

# Activation function in neural networks?

Sigmoidal functions, ReLu, etc.

::: fragment
Yes, it is a tuning parameter.
âœ…
:::

# Number of PCA components to retain?

::: fragment
Yes, it is a tuning parameter.
âœ…
:::

# Bayesian priors for model parameters?

::: fragment
Hmmmm, probably not.
These are based on prior belief.
âŒ
:::

# Covariance/correlation matrix structure in mixed models?

::: fragment
Yes, but it is unlikely to affect performance.
:::

::: fragment
It will impact inference though.
ğŸ¤”
:::



# Is the random seed a tuning parameter?

::: fragment
Nope. It is not. 
âŒ
:::

## Optimize tuning parameters

- Try different values and measure their performance.

. . .

- Find good values for these parameters.

. . .

- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.

## Optimize tuning parameters

The main two strategies for optimization are:

. . .

- **Grid search** ğŸ’  which tests a pre-defined set of candidate values

- **Iterative search** ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate

