---
title: "1 - Tuning Hyperparameters"
subtitle: "Advanced tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```

```{r more-setup}
#| include: false
library(modeldatatoo)
library(probably)
library(countdown)

cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)

options(width = 200)

ggplot2::theme_set(ggplot2::theme_bw())
```

## Hotel Data

We'll use a version of the [hotel data](https://www.sciencedirect.com/science/article/pii/S2352340918315191). We'll use [a version](https://modeldatatoo.tidymodels.org/dev/reference/data_hotel_rates.html) where we try to predict the cost of a room. 

The data are in the modeldatatoo package. We'll sample down the data and refactor some columns: 

:::: {.columns}

::: {.column width="40%"}

```{r}
#| label: tune-startup
library(tidymodels)
library(modeldatatoo)

# Max's usual settings: 
tidymodels_prefer()
theme_set(theme_bw())
options(
  pillar.advice = FALSE, 
  pillar.min_title_chars = Inf
)
```

:::

::: {.column width="60%"}

```{r}
#| label: data-import
set.seed(295)
hotel_rates <- 
  data_hotel_rates() %>% 
  sample_n(5000) %>% 
  arrange(arrival_date) %>% 
  select(-arrival_date_num, -arrival_date) %>% 
  mutate(
    company = factor(as.character(company)),
    country = factor(as.character(country)),
    agent = factor(as.character(agent))
  )
```


:::

::::


## Data Spending

Let's split the data into a training set (75%) and testing set (25%):

```{r}
#| label: hotel-split
set.seed(4028)
hotel_split <-
  initial_split(hotel_rates, strata = avg_price_per_room)

hotel_tr <- training(hotel_split)
hotel_te <- testing(hotel_split)
```



## Your turn {transition="slide-in"}

Let's take some time and investigate the _training data_. The outcome is `avg_price_per_room`. 

Are there any characteristic of the data that are interesting?

```{r}
#| label: hotel-investigation-exercise
#| echo: false
countdown(minutes = 10, id = "hotel-investigation")
```


## Feature Engineering

How should we represent our predictors? There are: 

 * Categorical predictors, some with many levels
 * Some skewed numeric predictor(s)
 * Interactions? 

. . .

Let's start a basic recipe: 

```{r}
#| label: hotel-rec-start

hotel_rec <- 
  recipe(avg_price_per_room ~ ., data = hotel_tr) %>% 
  step_YeoJohnson(lead_time)
```

. . .

We'll add to this for different types of models. 

## A Baseline Model

A simple linear model with main effects

```{r}
#| label: hotel-base-model
set.seed(472)
hotel_rs <- vfold_cv(hotel_tr, strata = avg_price_per_room)

base_rec <- 
  hotel_rec %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())

base_wflow <- workflow(base_rec, linear_reg())
```


## A Baseline Model

We'll try to optimize the mean absolute error: 

```{r}
#| label: hotel-base-res
#| cache: true
reg_metrics <- metric_set(mae, rsq)
ctrl_rs <- control_resamples(save_pred = TRUE)

base_res <- base_wflow %>% fit_resamples(hotel_rs, metrics = reg_metrics)

collect_metrics(base_res)
```


# Optimizing Models via Tuning Parameters



## Tuning parameters

Some model or preprocessing parameters cannot be estimated directly from the data.

. . .

Some examples:

- Tree depth in decision trees
- Number of neighbors in a K-nearest neighbor model

# Activation function in neural networks?

Sigmoidal functions, ReLu, etc.

::: fragment
Yes, it is a tuning parameter.
‚úÖ
:::

# Number of PCA components to retain?

::: fragment
Yes, it is a tuning parameter.
‚úÖ
:::

# Bayesian priors for model parameters?

::: fragment
Hmmmm, probably not.
These are based on prior belief.
‚ùå
:::

# Covariance/correlation matrix structure in mixed models?

::: fragment
Yes, but it is unlikely to affect performance.
:::

::: fragment
It will impact inference though.
ü§î
:::



# Is the random seed a tuning parameter?

::: fragment
Nope. It is not. 
‚ùå
:::

## Optimize tuning parameters

- Try different values and measure their performance.

. . .

- Find good values for these parameters.

. . .

- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.


## Boosted Trees

These are popular ensemble methods that build a _sequence_ of tree models. 

<br>

Each tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted. 

<br>

Each tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble. 

<br>

We'll focus on the popular lightgbm implementation. 

## Boosted Tree Tuning Parameters

* The number of predictors randomly sampled at each split (aka `mtry` in $[1, ncol(x)]$ or $(0, 1]$).
* The number of trees (`trees` in $[1, \infty]$, but usually up to thousands)
* The number of samples needed to further split (`min_n` in $[1, n]$).
* The rate that each tree adapts form previous iterations (`learn_rate` in $(0, \infty]$, usual maximum is 0.1).
* The number of iterations of boosting where _no improvement_ was shown (`stop_iter`)

## Boosted Tree Tuning Parameters

TBH it is usually not difficult to optimize these models. 

<br>

Often, there are multiple _candidate_ tuning parameter combinations that have very good results. 

<br>

To demonstrate simple concepts, we'll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate ($10^{-5}$ to $10^{-1}$).

## Boosted Tree Tuning Parameters `r hexes(c("workflows", "parsnip", "bonsai"))`

We'll need to load the bonsai package. This has the information needed to use lightgbm

```{r}
#| label: boot-spec

library(bonsai)
lgbm_spec <- 
  boost_tree(trees = tune(), learn_rate = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("lightgbm")

lgbm_wflow <- workflow(hotel_rec, lgbm_spec)
```


## Optimize tuning parameters

The main two strategies for optimization are:

. . .

- **Grid search** üí† which tests a pre-defined set of candidate values

- **Iterative search** üåÄ which suggests/estimates new values of candidate parameters to evaluate

We'll focus on grid search for a while. 

## Grid search

A small grid of points trying to minimize the error via learning rate: 

```{r}
#| label: small-grid-demo
#| echo: false
#| fig-align: center
#| out-width: 60%

knitr::include_graphics("images/small_init.svg")
```


## Grid search

In reality we would probably sample the space more densely: 

```{r}
#| label: grid-demo
#| echo: false
#| fig-align: center
#| out-width: 60%

knitr::include_graphics("images/grid_points.svg")
```


## Sequential Search

We could start with a few points and search the space:

```{r}
#| label: seq-demo
#| echo: false
#| fig-align: center
#| out-width: 60%

knitr::include_graphics("images/anime_seq.gif")
```

# Grid Search

## Parameters

-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).

-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.

::: fragment
#### Grids

-   Create your grid manually or automatically.

-   The `grid_*()` functions can make a grid.
:::

::: notes
Most basic (but very effective) way to tune models
:::

## Create a grid `r hexes(c("dials", "workflows"))`

```{r get-param}
#| tidy: false
lgbm_wflow %>% 
  extract_parameter_set_dials()

# Individual functions: 
trees()
learn_rate()
```

::: fragment
A parameter set can be updated (e.g. to change the ranges).
:::

## Create a grid `r hexes(c("dials", "workflows"))`

::: columns
::: {.column width="50%"}
```{r}
#| label: get-grid 
set.seed(12)
grid <- 
  lgbm_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

grid
```
:::

::: {.column width="50%"}
::: fragment
-   A *space-filling design* like this tends to perform better than random grids.
-   Space-filling designs are also usually more efficient than regular grids.
:::
:::
:::

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Create a grid for our tunable workflow.*

*Try creating a regular grid.*

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "make-grid")
```

## Create a grid `r hexes(c("dials", "workflows"))`

```{r get-regular-grid} 
#| label: reg-grid
#| code-line-numbers: "5"
set.seed(12)
grid <- 
  lgbm_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_regular(levels = 25)

grid
```

:::notes
Note that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the `deg_free` parameters only have a range of `1->15`.
:::

## Update parameter ranges `r hexes(c("dials", "workflows"))` {.annotation}


```{r mod-grid-code} 
#| label: update-param
#| code-line-numbers: "4-5"
lgbm_param <- 
  lgbm_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(trees = trees(c(1L, 100L)),
         learn_rate = learn_rate(c(-5, -1)))

set.seed(121)
grid <- 
  lgbm_param %>% 
  grid_latin_hypercube(size = 25)

grid
```


## The results `r hexes(c("dials", "workflows"))`

```{r show-grid-code} 
#| label: sfd
#| output-location: column
#| fig-width: 5
#| fig-height: 5.1
#| fig-align: 'center'
grid %>% 
  ggplot(aes(trees, learn_rate)) +
  geom_point(size = 4) +
  scale_y_log10()
```

Note that the learning rates are uniform on the log-10 scale. 


# Use the `tune_*()` functions to tune models


## Choosing tuning parameters `r hexes("recipes","parsnip", "workflows", "tune", "bonsai")`

Let's take our previous model and add more parameters:

```{r} 
#| label: lgm-more-tune
#| code-line-numbers: "2-3"
lgbm_spec <- 
  boost_tree(trees = tune(), learn_rate = tune(), mtry = tune(), 
             min_n = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("lightgbm")

lgbm_wflow <- workflow(hotel_rec, lgbm_spec)
```


## Grid Search `r hexes(c("dials", "workflows", "tune"))` 

```{r tuning} 
#| label: lgb-grid-tune
#| cache: true
set.seed(9)
ctrl <- control_grid(save_pred = TRUE, parallel_over = "everything")

lgbm_res <-
  lgbm_wflow %>%
  tune_grid(
    resamples = hotel_rs,
    grid = 25,
    control = ctrl,
    metrics = reg_metrics
  )
```

::: notes
-   `tune_grid()` is representative of tuning function syntax
-   similar to `fit_resamples()`
:::



## Grid Search `r hexes(c("dials", "workflows", "tune"))` 

```{r tuning} 
#| label: lgb-grid-tune-res
lgbm_res
```


## Grid results `r hexes(c("tune"))`

```{r autoplot}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
#| out-width: "80%"
#| dev-args: list(bg = "transparent")
autoplot(lgbm_res)
```

## Tuning results `r hexes(c("tune"))`

```{r}
collect_metrics(lgbm_res)
```

## Tuning results `r hexes(c("tune"))`

```{r}
collect_metrics(lgbm_res, summarize = FALSE)
```

## Choose a parameter combination `r hexes(c("tune"))`

```{r}
show_best(lgbm_res, metric = "rsq")
```

## Choose a parameter combination `r hexes(c("tune"))`

Create your own tibble for final parameters or use one of the `tune::select_*()` functions:

```{r}
lgbm_best <- select_best(lgbm_res, metric = "mae")
lgbm_best
```

## Checking Calibration `r hexes(c("tune", "probably"))`

```{r}
#| label: lgb-cal-plot
#| output-location: column
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5

library(probably)
lgbm_res %>%
  collect_predictions(
    parameters = lgbm_best
  ) %>%
  cal_plot_regression(
    truth = avg_price_per_room,
    estimate = .pred,
    alpha = 1 / 3
  )
```

## Updating the Workflow

If we like this model, we might want to splice the best tuning parameter values into our `lgbm_wflow` object. 

There is a function for that: 

```{r}
#| label: lgb-finalize

lgbm_wflow <- lgbm_wflow %>% finalize_workflow(lgbm_best)
lgbm_wflow
```


## Running in parallel

::: columns
::: {.column width="60%"}
-   Grid search, combined with resampling, requires fitting a lot of models!

-   These models don't depend on one another and can be run in parallel.

We can use a *parallel backend* to do this:

```{r, eval= FALSE}
#| eval: false
#| label: parallel-methods
cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)

# Now call `tune_grid()`!

# Shut it down with:
foreach::registerDoSEQ()
parallel::stopCluster(cl)
```
:::

::: {.column width="40%"}
```{r}
#| label: resample-times
#| echo: false
#| out-width: '100%'
#| fig-width: 6
#| fig-height: 6
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
load("resamples_times.RData")
resamples_times %>%
  dplyr::rename(operation = label) %>% 
  ggplot(aes(y = id_alt, x = duration, fill = operation)) +
  geom_bar(stat = "identity", color = "black") +
  labs(y = NULL, x = "Elapsed Time") + 
  scale_fill_brewer(palette = "Paired") +
  theme(legend.position = "top")
```
:::
:::

## Running in parallel

Speed-ups are fairly linear up to the number of physical cores (10 here).

```{r}
#| label: parallel-speedup
#| echo: false
#| out-width: '90%'
#| fig-width: 9
#| fig-height: 4
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
load("xgb_times.RData")
ggplot(times, aes(x = num_cores, y = speed_up, color = parallel_over, shape = parallel_over)) + 
  geom_abline(lty = 1) + 
  geom_point(size = 2) + 
  geom_line() +
  facet_wrap(~ preprocessing) + 
  coord_obs_pred() + 
  scale_color_manual(values = c("#7FC97F", "#386CB0")) +
  labs(x = "Number of Workers", y = "Speed-up")  +
  theme(legend.position = "top")
```

:::notes
Faceted on the expensiveness of preprocessing used.
:::

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

<br>

What if we used early stopping instead of optimizing the number of trees? 

We could set `trees = 1000` and use tune the `stop_iter` parameter. 

```{r}
#| echo: false
countdown::countdown(minutes = 10, id = "lgbm-stop")
```

