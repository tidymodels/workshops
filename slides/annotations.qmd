---
title: "Annotations"
---

```{r startup}
#| include: false

# To get `hexes()`
source("setup.R")
```

# 01 - Introduction

## `r emo::ji("eyes")`

This page contains _annotations_ for selected slides. 

There's a lot that we want to tell you. We don't want people to have to frantically scribble down things that we say that are not on the slides. 

We'll add sections to this document with longer explanations and links to other resources. 



## Finalize and verify

This is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.

In a later section, we will talk about methods of [resampling](https://www.tmwr.org/resampling.html). These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are [special cases of resampling](https://www.tmwr.org/resampling.html#validation) where there is a single "resample". 

Most types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like 

```{r resampling-diagram}
#| echo: false
#| fig-align: 'center'
#| fig-width: 50
knitr::include_graphics("images/whole-game-final-resamples.svg")
```

In this case there is just "testing" and "training". Once the final model is determined, the entire training set is used for the last fit. 

# 02 - Data Budget

## Data splitting and spending `r hexes("rsample")` 

What does `set.seed()` do? 

We’ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random). 

Think of PRN as a box that takes a starting value (the “seed”) that produces random numbers using that starting value as an input into its process. 

If we know a seed value, we can reproduce our “random” numbers. To use a different set of random numbers, choose a different seed value. 

For example: 

```{r}
set.seed(1)
runif(3)

# Get a new set or random numbers:
set.seed(2)
runif(3)

# We can reproduce the old ones with the same seed
set.seed(1)
runif(3)
```

If we _don’t_ set the seeds, programs use the clock time as the seed. This isn’t reproducible. 

Since we want our code to be reproducible, we set the seeds before random numbers are used. 

In theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we don’t get reproducible results. 

The value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to "spread the randomness around". It is basically:

```{r}
cat(paste0("set.seed(", sample.int(10000, 5), ")", collapse = "\n"))
```


# 06 - Tuning Hyperparameters

## Spline grid search 

What's going on with the 

> prediction from a rank-deficient fit may be misleading

warnings? 

For linear regression, a computation is used called _matrix inversion_. The matrix in question is called the "model matrix" and it contains the predictor set for the training data. 

Matrix inverse can fail if two or model columns: 

 * are identical, or 
 * add up to some other column. 
 
These situations are called _linear dependencies_.  

When this happens, `lm()` is pretty tolerant. It does not fail but does not compute regression coefficients for a minimal number of predictors involved in the dependency (and issues the warning above).

For these data, there are three dependencies between:

 * `defense_team_PIT` and `offense_team_PIT`
 * `strength_short_handed`, `player_diff`, and `strength_power_play`
 * `year`, `month_Oct`, `month_Nov`, and `month_Dec`

The first one is easy to explain. For each row, when one these two `PIT` column has a one, the other must have a zero. The linear regression intercept is represented in the model matrix as a column of all ones. The dependency is 

```r
(Intercept) = defense_team_PIT + offense_team_PIT
```

The way to avoid this problem is to use `step_lincomb(all_numeric_predictors())` in the recipe. [This step](https://recipes.tidymodels.org/reference/step_lincomb.html) removes the minimum number of columns to avoid the issue. 

**tl;dr**

Linear regression detects some redundancies in the predictor set. We can ignore the warnings since `lm()` can deal with it or use [`step_lincomb()`](https://recipes.tidymodels.org/reference/step_lincomb.html) to avoid the warnings. 



