---
title: "6 - Tuning hyperparameters"
subtitle: "Machine learning with tidymodels"
height: 900
width: 1600
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    theme: [default, tidymodels.scss]
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```


```{r more-setup}
#| include: false
library(rpart)
library(partykit)
library(doParallel)

cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(cores)
registerDoParallel(cl)

options(width = 200)

ggplot2::theme_set(ggplot2::theme_bw())
```


```{r previously}
#| include: false
library(tidymodels)
library(embed)
library(ongoal)

tidymodels_prefer()

set.seed(23)
nhl_split <- initial_split(season_2015, prop = 3/4)
nhl_split

nhl_train <- training(nhl_split)
nhl_test  <- testing(nhl_split)

set.seed(234)
nhl_val <- validation_split(nhl_train, prop = 0.80)


nhl_distance_rec <-
  recipe(on_goal ~ ., data = nhl_train) %>%
  step_lencode_mixed(player, outcome = vars(on_goal)) %>%
  step_other(all_nominal_predictors()) %>% # TODO: keep this?
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_mutate(
    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),
    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2)
  )

nhl_distance_wflow <-
  workflow() %>%
  add_recipe(nhl_distance_rec) %>%
  add_model(logistic_reg())

nhl_distance_res <-
  nhl_distance_wflow %>%
  fit_resamples(nhl_val)
```

## Tuning parameters

These are model or preprocessing parameters that are important but cannot be estimated directly from the data.

Some examples:

-   Tree depth in decision trees.
-   Number of neighbors in a K-nearest neighbor model.

Some others...

# Activation function in neural networks?

Sigmoidal functions, ReLu, etc.

::: fragment
Yes, it is a tuning parameter.
:::

# Number of PCA components to retain?

::: fragment
Yes, it is a tuning parameter.
:::

# Bayesian priors for model parameters?

::: fragment
Hmmm probably not. These are based on prior belief.
:::

# Covariance/correlation matrix structure in mixed models?

::: fragment
Yes but it is unlikely to affect performance.
:::

::: fragment
It will impact inference though.
:::

## Optimizing tuning parameters

The main approach is to try different values and measure their performance.

This can lead us to good values for these parameters.

Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.

## Optimizing tuning parameters

The main two classes of optimization models are:

-   *Grid search* where a pre-defined set of candidate values are tested.

-   *Iterative search* methods suggest/estimate new values of candidate parameters to evaluate.

## Choosing tuning parameters `r I(hexes(c("recipes", "workflows", "parsnip", "tune")))`

Let's take our previous recipe and add a few changes:

```{r}
#| code-line-numbers: "12-13"
glm_rec <-
  recipe(on_goal ~ ., data = nhl_train) %>%
  step_lencode_mixed(player, outcome = vars(on_goal)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_mutate(
    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),
    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),
    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)
  ) %>%
  step_rm(coord_x, coord_y) %>%
  step_zv(all_predictors()) %>%
  step_ns(angle, deg_free = tune("angle")) %>%
  step_ns(distance, deg_free = tune("distance")) %>%
  step_normalize(all_numeric_predictors())
glm_wflow <-
  workflow() %>%
  add_model(logistic_reg()) %>%
  add_recipe(glm_rec)
```

## Grid search

This is the most basic (but very effective) way for tuning models.

tidymodels has pre-defined information on tuning parameters, such as their type, range, transformations, etc.

A grid can be created manually or automatically.

The `extract_parameter_set_dials()` function extracts the tuning parameters and the info.

The `grid_*()` functions can make a grid.

## Manual grid - get parameters `r I(hexes(c("dials", "workflows")))`

::: columns
::: {.column width="50%"}
```{r get-param}
#| eval: false
#| tidy: false
glm_wflow %>% 
  extract_parameter_set_dials()
```

This type of object can be updated (e.g. to change the ranges, etc)
:::

::: {.column width="50%"}
```{r ref.label = 'get-param'}
#| echo: false
```
:::
:::

## Manual grid - create grid `r I(hexes(c("dials", "workflows")))`

::: columns
::: {.column width="50%"}
```{r get-grid}
#| eval: false
set.seed(2)
grid <- 
  glm_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

grid
```

This is a type of *space-filling design*.

It tends to do much better than random grids and is (usually) more efficient than regular grids.
:::

::: {.column width="50%"}
```{r ref.label = 'get-grid'}
#| echo: false
```
:::
:::



## Update parameter ranges

::: columns
::: {.column width="50%"}
```{r mod-grid-code}
#| eval: false
#| code-line-numbers: "5-6"
set.seed(2)
grid <- 
  glm_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(angle = spline_degree(c(2L, 20L)),
         distance = spline_degree(c(2L, 20L))) %>% 
  grid_latin_hypercube(size = 25)

grid
```
:::

::: {.column width="50%"}
```{r mod-grid, ref.label = 'mod-grid-code'}
#| echo: false
```
:::
:::


## The results `r I(hexes(c("dials", "workflows")))`

::: columns
::: {.column width="50%"}
```{r show-grid-code}
#| eval: false
grid %>% 
  ggplot(aes(angle, distance)) +
  geom_point(cex = 4)
```
:::

::: {.column width="50%"}
```{r show-grid, ref.label = 'show-grid-code'}
#| echo: false
#| out-width: '90%'
#| fig-width: 5
#| fig-height: 5.1
#| fig-align: 'center'
#| dev: 'svg'
#| dev-args: list(bg = "transparent")
```
:::
:::

## Grid search `r I(hexes(c("tune")))`

The `tune_*()` functions can be used to tune models.

`tune_grid()` is pretty representative of their syntax (and is similar to `fit_resamples()`):

```{r tuning}
#| cache: true
ctrl <- control_grid(save_pred = TRUE, parallel_over = "everything")

set.seed(9)
glm_res <-
  glm_wflow %>%
  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)
glm_res
```

## Grid results `r I(hexes(c("tune")))`

```{r autoplot}
#| out-width: '70%'
#| fig-width: 8
#| fig-height: 4
#| fig-align: 'center'
#| dev: 'svg'
#| dev-args: list(bg = "transparent")
autoplot(glm_res)
```

## Returning results `r I(hexes(c("tune")))`

```{r}
collect_metrics(glm_res)

collect_metrics(glm_res, summarize = FALSE)
```

## Picking a parameter combination `r I(hexes(c("tune")))`

You can create a tibble of your own or use one of the `tune::select_*()` functions:

```{r}
show_best(glm_res, metric = "roc_auc")
select_best(glm_res, metric = "roc_auc")
```

## Boosted Trees

An ensemble method of tree-based models.

A tree-based model creates a series of splits on predictors that partition them into two groups to maximize the purity of the resulting sets.

This forms a series of if/then statements that make up a tree structure.

## Boosted Trees

The creation of the tree has two phases:

-   The *growing* phase where splits are made until we meet some condition
    -   maximum depth,
    -   run out of data
-   Tree *pruning* where the ends of the trees are removed until the "right sized" tree is found.

## Tree-based Models

```{r tree-example}
#| echo: false
#| out-width: '60%'
#| fig-width: 16
#| fig-height: 7
#| fig-align: 'center'
#| dev: 'svg'
#| dev-args: list(bg = "transparent")
tree_mod <- 
    rpart::rpart(
        on_goal ~ . - player - offense_team - defence_team - coord_x - coord_y,
        data = nhl_train,
        control = rpart::rpart.control(maxdepth = 3, cp = 0.001)
    ) %>% 
    partykit::as.party()
plot(tree_mod)
```

## Boosting

Boosting methods fit a sequence of tree-based models.

Each is dependent on the last and tries to compensate to any poor results in the previous models

-   This is akin to gradient-based steepest ascent methods from calculus.


## Boosting

Most modern boosting methods have *a lot* of tuning parameters. 

* For tree growth and pruning (`min_n`, `max_depth`. etc). 

* For boosting (`trees`, `stop_iter`, `learn_rate`)

We'll use *early stopping* where we stop boosting when a few iterations produces consecutively worse results.

## Boosting `r I(hexes(c("recipes", "workflows", "parsnip", "tune")))`

```{r xgboost-specs}
xgb_spec <-
  boost_tree(
    trees = 500, min_n = tune(), stop_iter = tune(), tree_depth = tune(),
    learn_rate = tune(), loss_reduction = tune()
  ) %>%
  set_mode("classification") %>% 
  set_engine("xgboost", validation = 1/10) # <- for better early stopping

xgb_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_lencode_mixed(player, outcome = vars(on_goal)) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

xgb_wflow <- 
  workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(xgb_rec)
```

## Running in parallel

::: columns
::: {.column width="50%"}
Grid search, combined with resampling, ends up fitting a lot of models.

These models don't depend on one another and can be run in parallel.

We can use a *parallel backend* to do this:

```{r, eval= FALSE}
cores <- parallel::detectCores(logical = FALSE)
library(doParallel)
cl <- makePSOCKcluster(cores)
registerDoParallel(cl)
```
:::

::: {.column width="50%"}
```{r resample-times}
#| echo: false
#| out-width: '100%'
#| fig-width: 6
#| fig-height: 6
#| fig-align: 'center'
#| dev: 'svg'
#| dev-args: list(bg = "transparent")
load("resamples_times.RData")
resamples_times %>%
  dplyr::rename(operation = label) %>% 
  ggplot(aes(y = id_alt, x = duration, fill = operation)) +
  geom_bar(stat = "identity", color = "black") +
  labs(y = NULL, x = "Elapsed Time") + 
  scale_fill_brewer(palette = "Paired") +
  theme(legend.position = "top")
```
:::
:::

## Running in parallel

There are various degrees of speed-ups that are fairly linear up until the number of physical cores.

```{r}
#| echo: false
#| out-width: '90%'
#| fig-width: 9
#| fig-height: 4
#| fig-align: 'center'
#| dev: 'svg'
#| dev-args: list(bg = "transparent")
load("xgb_times.RData")
ggplot(times, aes(x = num_cores, y = speed_up, color = parallel_over, shape = parallel_over)) + 
  geom_abline(lty = 1) + 
  geom_point(size = 2) + 
  geom_line() +
  facet_wrap(~ preprocessing) + 
  coord_obs_pred() + 
  scale_color_manual(values = c("#7FC97F", "#386CB0")) +
  labs(x = "Number of Workers", y = "Speed-up")  +
  theme(legend.position = "top")
```

## Tuning `r I(hexes(c("tune")))`

This will take some time to run...

```{r xgboost-tune}
#| cache: true
set.seed(9)
xgb_res <-
  xgb_wflow %>%
  tune_grid(resamples = nhl_val, grid = 20, control = ctrl) # automatic grid now!
xgb_res
```

## Tuning Results `r I(hexes(c("tune")))`

```{r autoplot-xgboost}
#| out-width: '100%'
#| fig-width: 11
#| fig-height: 4
#| fig-align: 'center'
#| dev: 'svg'
#| dev-args: list(bg = "transparent")
autoplot(xgb_res)
```

## Compare models

Best logistic regression results:

```{r logistic-best}
nhl_distance_res %>%
  collect_metrics() %>% 
  filter(.metric == "roc_auc")
```

Best boosting results:

```{r xgboost-best}

show_best(xgb_res, metric = "roc_auc", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```


## Your turn to tune {transition="slide-in"}

Can you get better ROC results by just increasing `tree_depth` beyond the tested range? 


```{r}
#| echo: false
countdown(minutes = 20)
```



## Updating the workflow `r I(hexes(c("workflows", "tune")))`

```{r xgboost-select-best}
#| cache: true

best_auc <- select_best(xgb_res, metric = "roc_auc")
best_auc

xgb_wflow <-
  xgb_wflow %>% 
  finalize_workflow(best_auc)

xgb_wflow
```

## The final fit `r I(hexes(c("workflows", "tune")))`

```{r xgboost-final}
#| cache: true
test_res <- 
  xgb_wflow %>% 
  last_fit(split = nhl_split)
test_res
```

## Estimates of ROC AUC `r I(hexes(c("tune")))`

Validation results from tuning: 

```{r xgb-val-res}
# Resampling results
xgb_res %>% 
  show_best(metric = "roc_auc", n = 1) %>% 
  select(.metric, mean, n, std_err)
```

Test set results: 

```{r xgb-test-res}
test_res %>% collect_metrics()
```

## Final fitted workflow

The final fitted workflow, fit using the training set, can be pulled out:

```{r}
final_xgb_wflow <- 
  test_res %>% 
  extract_workflow()

# use to predict or deploy via vetiver
predict(final_xgb_wflow, nhl_test[1:3,])
```

## Next steps

* We can use explainers to help characterize the model and the predictions. 

* Document the model.

* Create an applicability domain model to help monitor our data over time.

* Deploy the model.



```{r teardown}
#| include: false
parallel::stopCluster(cl)
```

