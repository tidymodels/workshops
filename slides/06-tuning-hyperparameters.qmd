---
title: "6 - Tuning Hyperparameters"
subtitle: "Machine learning with tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```

```{r more-setup}
#| include: false
library(rpart)
library(partykit)

cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)

options(width = 200)

ggplot2::theme_set(ggplot2::theme_bw())
```

```{r previously}
#| include: false
library(tidymodels)
library(embed)
library(ongoal)

tidymodels_prefer()

set.seed(23)
nhl_split <- initial_split(season_2015, prop = 3/4)
nhl_split

nhl_train <- training(nhl_split)
nhl_test  <- testing(nhl_split)

set.seed(234)
nhl_val <- validation_split(nhl_train, prop = 0.80)

nhl_distance_rec <-
  recipe(on_goal ~ ., data = nhl_train) %>%
  step_lencode_mixed(player, outcome = vars(on_goal)) %>%
  step_other(all_nominal_predictors()) %>% # TODO: keep this?
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_mutate(
    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),
    distance = sqrt((89 - abs(coord_x))^2 + coord_y^2),
    distance = log(distance)
  )

nhl_distance_wflow <-
  workflow() %>%
  add_recipe(nhl_distance_rec) %>%
  add_model(logistic_reg())

nhl_distance_res <-
  nhl_distance_wflow %>%
  fit_resamples(nhl_val)
```

## Tuning parameters

Some model or preprocessing parameters cannot be estimated directly from the data.

. . .

Some examples:

-   Tree depth in decision trees
-   Number of neighbors in a K-nearest neighbor model

# Activation function in neural networks?

Sigmoidal functions, ReLu, etc.

::: fragment
Yes, it is a tuning parameter.
âœ…
:::

# Number of PCA components to retain?

::: fragment
Yes, it is a tuning parameter.
âœ…
:::

# Bayesian priors for model parameters?

::: fragment
Hmmmm, probably not.
These are based on prior belief.
âŒ
:::

# Covariance/correlation matrix structure in mixed models?

::: fragment
Yes, but it is unlikely to affect performance.
:::

::: fragment
It will impact inference though.
ğŸ¤”
:::



# Is the random seed a tuning parameter?

::: fragment
Nope. It is not. 
âŒ
:::

## Optimize tuning parameters

-   Try different values and measure their performance.

. . .

-   Find good values for these parameters.

. . .

-   Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.

## Optimize tuning parameters

The main two strategies for optimization are:

. . .

-   **Grid search** ğŸ’  which tests a pre-defined set of candidate values

-   **Iterative search** ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate

## Choosing tuning parameters `r hexes("recipes","parsnip", "workflows", "tune")`

Let's take our previous recipe and add a few changes:

```{r}
#| code-line-numbers: "13-14"
glm_rec <-
  recipe(on_goal ~ ., data = nhl_train) %>%
  step_lencode_mixed(player, outcome = vars(on_goal)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_mutate(
    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),
    distance = sqrt((89 - abs(coord_x))^2 + coord_y^2),
    distance = log(distance),
    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)
  ) %>%
  step_rm(coord_x, coord_y) %>%
  step_zv(all_predictors()) %>%
  step_ns(angle, deg_free = tune("angle")) %>%
  step_ns(distance, deg_free = tune("distance")) %>%
  step_normalize(all_numeric_predictors())
glm_spline_wflow <-
  workflow() %>%
  add_model(logistic_reg()) %>%
  add_recipe(glm_rec)
```

## Grid search

#### Parameters

-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).

-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.

::: fragment
#### Grids

-   Create your grid manually or automatically.

-   The `grid_*()` functions can make a grid.
:::

::: notes
Most basic (but very effective) way to tune models
:::

## Create a grid `r hexes(c("dials", "workflows"))`

```{r get-param}
#| tidy: false
glm_spline_wflow %>% 
  extract_parameter_set_dials()
```

::: fragment
A parameter set can be updated (e.g. to change the ranges).
:::

## Create a grid `r hexes(c("dials", "workflows"))`

::: columns
::: {.column width="50%"}
```{r get-grid}
set.seed(2)
grid <- 
  glm_spline_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

grid
```
:::

::: {.column width="50%"}
::: fragment
-   A *space-filling design* like this tends to perform better than random grids.
-   Space-filling designs are also usually more efficient than regular grids.
:::
:::
:::

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Create a grid for our tunable workflow.*

*Try creating a regular grid.*

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "make-grid")
```

## Create a grid `r hexes(c("dials", "workflows"))`

```{r get-regular-grid}
#| code-line-numbers: "5"
set.seed(2)
grid <- 
  glm_spline_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_regular(levels = 25)

grid
```

:::notes
Note that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the `deg_free` parameters only have a range of `1->15`.
:::

## Update parameter ranges `r hexes(c("dials", "workflows"))` {.annotation}


```{r mod-grid-code}
#| code-line-numbers: "5-6"
set.seed(2)
grid <- 
  glm_spline_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(angle = spline_degree(c(2L, 20L)),
         distance = spline_degree(c(2L, 20L))) %>% 
  grid_latin_hypercube(size = 25)

grid
```

::: notes
Even though `angle` is a `deg_free` parameter in `step_ns()`, we don't use the dials `deg_free()` object here. We have a special `spline_degree()` function that has better defaults for splines.
:::

## The results `r hexes(c("dials", "workflows"))`

```{r show-grid-code}
#| output-location: column
#| fig-width: 5
#| fig-height: 5.1
#| fig-align: 'center'
grid %>% 
  ggplot(aes(angle, distance)) +
  geom_point(size = 4)
```

# Use the `tune_*()` functions to tune models

## Spline grid search `r hexes(c("dials", "workflows", "tune"))` {.annotation}

```{r tuning} 
#| cache: true
set.seed(9)
ctrl <- control_grid(save_pred = TRUE, parallel_over = "everything")

glm_spline_res <-
  glm_spline_wflow %>%
  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)

glm_spline_res
```

::: notes
-   `tune_grid()` is representative of tuning function syntax
-   similar to `fit_resamples()`
:::

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Tune our `glm_wflow`.*

*What happens if you don't supply a `grid` argument to `tune_grid()`?*

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "tune-glm")
```

## Grid results `r hexes(c("tune"))`

```{r autoplot}
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
autoplot(glm_spline_res)
```

## Tuning results `r hexes(c("tune"))`

```{r}
collect_metrics(glm_spline_res)
```

## Tuning results `r hexes(c("tune"))`

```{r}
collect_metrics(glm_spline_res, summarize = FALSE)
```

## Choose a parameter combination `r hexes(c("tune"))`

```{r}
show_best(glm_spline_res, metric = "roc_auc")
```

## Choose a parameter combination `r hexes(c("tune"))`

Create your own tibble for final parameters or use one of the `tune::select_*()` functions:

```{r}
select_best(glm_spline_res, metric = "roc_auc")
```

. . .

This best result has:

-   low-degree spline for `angle` (less "wiggly", less complex)
-   higher-degree spline for `distance` (more "wiggly", more complex)

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Try an alternative selection strategy.*

*Read the docs for `select_by_pct_loss()`.*

*Try choosing a model that has a simpler (less "wiggly") relationship for `distance`.*

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "select-by-pct-loss")
```

## Choose a parameter combination `r hexes(c("tune"))`

```{r}
select_best(glm_spline_res, metric = "roc_auc")
select_by_pct_loss(glm_spline_res, distance, metric = "roc_auc")
```

# Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ´ğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒ³ğŸŒµğŸŒµğŸŒ´ğŸŒ²ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒ´ğŸŒµğŸŒ´ğŸŒ²ğŸŒ´ğŸŒµğŸŒ²ğŸŒµğŸŒ´ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒµğŸŒ³ğŸŒ´ğŸŒ³

## Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ

-   Ensemble many decision tree models

::: fragment
### Review how a decision tree model works:

-   Series of splits or if/then statements based on predictors

-   First the tree *grows* until some condition is met (maximum depth, no more data)

-   Then the tree is *pruned* to reduce its complexity
:::

## Single decision tree

```{r tree-example}
#| echo: false
#| fig.width: 16
#| fig.height: 8
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
tree_mod <- 
    rpart::rpart(
        on_goal ~ . - player - offense_team - defense_team - coord_x - coord_y,
        data = nhl_train,
        control = rpart::rpart.control(maxdepth = 3, cp = 0.001)
    ) %>% 
    partykit::as.party()
plot(tree_mod)
```

## Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ

Boosting methods fit a *sequence* of tree-based models.

. . .

-   Each tree is dependent on the one before and tries to compensate for any poor results in the previous trees.

-   This is like gradient-based steepest ascent methods from calculus.

## Boosted tree tuning parameters  {.annotation}

Most modern boosting methods have *a lot* of tuning parameters!

. . .

-   For tree growth and pruning (`min_n`, `max_depth`, etc)

-   For boosting (`trees`, `stop_iter`, `learn_rate`)

. . .

We'll use *early stopping* to stop boosting when a few iterations produce consecutively worse results.

## Boosted tree code {.annotation}

```{r xgboost-specs}
xgb_spec <-
  boost_tree(
    trees = 500, min_n = tune(), stop_iter = tune(), tree_depth = tune(),
    learn_rate = tune(), loss_reduction = tune()
  ) %>%
  set_mode("classification") %>% 
  set_engine("xgboost", validation = 1/10) # <- for better early stopping

xgb_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_lencode_mixed(player, outcome = vars(on_goal)) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

xgb_wflow <- 
  workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(xgb_rec)
```

:::notes
`validation` is an argument to `parsnip::xgb_train()`, not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to `xgb.train(watchlist = list(validation = data))`.

See `translate(xgb_spec)` to see where it is passed to `parsnip::xgb_train()`.
:::

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Create your boosted tree workflow.*

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "xgb-wflow")
```

## Running in parallel

::: columns
::: {.column width="60%"}
-   Grid search, combined with resampling, requires fitting a lot of models!

-   These models don't depend on one another and can be run in parallel.

We can use a *parallel backend* to do this:

```{r, eval= FALSE}
cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)

# Now call `tune_grid()`!

# Shut it down with:
foreach::registerDoSEQ()
parallel::stopCluster(cl)
```
:::

::: {.column width="40%"}
```{r resample-times}
#| echo: false
#| out-width: '100%'
#| fig-width: 6
#| fig-height: 6
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
load("resamples_times.RData")
resamples_times %>%
  dplyr::rename(operation = label) %>% 
  ggplot(aes(y = id_alt, x = duration, fill = operation)) +
  geom_bar(stat = "identity", color = "black") +
  labs(y = NULL, x = "Elapsed Time") + 
  scale_fill_brewer(palette = "Paired") +
  theme(legend.position = "top")
```
:::
:::

## Running in parallel

Speed-ups are fairly linear up to the number of physical cores (10 here).

```{r}
#| echo: false
#| out-width: '90%'
#| fig-width: 9
#| fig-height: 4
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
load("xgb_times.RData")
ggplot(times, aes(x = num_cores, y = speed_up, color = parallel_over, shape = parallel_over)) + 
  geom_abline(lty = 1) + 
  geom_point(size = 2) + 
  geom_line() +
  facet_wrap(~ preprocessing) + 
  coord_obs_pred() + 
  scale_color_manual(values = c("#7FC97F", "#386CB0")) +
  labs(x = "Number of Workers", y = "Speed-up")  +
  theme(legend.position = "top")
```

:::notes
Faceted on the expensiveness of preprocessing used.
:::

## Tuning `r hexes(c("tune"))`

This will take some time to run â³

```{r xgboost-tune}
#| cache: true
set.seed(9)

xgb_res <-
  xgb_wflow %>%
  tune_grid(resamples = nhl_val, grid = 20, control = ctrl) # automatic grid now!

xgb_res
```

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Start tuning the boosted tree model!*

*We won't wait for everyone's tuning to finish, but take this time to get it started before we move on.*

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "tune-xgboost")
```

## Tuning results `r hexes(c("tune"))`

```{r autoplot-xgboost}
#| out-width: '100%'
#| fig-width: 11
#| fig-height: 4
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
autoplot(xgb_res)
```

## Again with the location features

```{r xgb-coord}
coord_rec <- 
  xgb_rec %>%
  step_mutate(
    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),
    distance = sqrt((89 - abs(coord_x))^2 + coord_y^2),
    distance = log(distance),
    behind_goal_line = ifelse(abs(coord_x) >= 89, 1, 0)
  ) %>% 
  step_rm(coord_x, coord_y)

xgb_coord_wflow <- 
  workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(coord_rec)

set.seed(9)
xgb_coord_res <-
  xgb_coord_wflow %>%
  tune_grid(resamples = nhl_val, grid = 20, control = ctrl)
```

## Did the machine figure it out? 

```{r}
show_best(xgb_res, metric = "roc_auc")

show_best(xgb_coord_res, metric = "roc_auc")
```


## Compare models

Best logistic regression results:

```{r logistic-best}
glm_spline_res %>% 
  show_best(metric = "roc_auc", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```

::: fragment
Best boosting results:

```{r xgboost-best}
xgb_coord_res %>% 
  show_best(metric = "roc_auc", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```
:::

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Can you get better ROC results?*

*Try increasing `tree_depth` beyond the original range.*

```{r}
#| echo: false
countdown::countdown(minutes = 20, id = "improve-xgb")
```

## Updating the workflow `r hexes(c("workflows", "tune"))`

```{r final-select-best}
#| cache: true

best_auc <- select_best(glm_spline_res, metric = "roc_auc")
best_auc

glm_spline_wflow <-
  glm_spline_wflow %>% 
  finalize_workflow(best_auc)

glm_spline_wflow
```

## The final fit to the NHL data `r hexes(c("workflows", "tune"))`  {.annotation}

```{r final-last-fit}
#| cache: true
test_res <- 
  glm_spline_wflow %>% 
  last_fit(split = nhl_split)

test_res
```

. . .

Remember that `last_fit()` **fits** one time with the non-training set and **evaluates** one time with the testing set.

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Finalize your workflow with the best parameters.*

*Create a final fit.*

```{r}
#| echo: false
countdown::countdown(minutes = 20, id = "finalize-xgb")
```

## Estimates of ROC AUC `r hexes(c("tune"))`

Validation results from tuning:

```{r val-res}
glm_spline_res %>% 
  show_best(metric = "roc_auc", n = 1) %>% 
  select(.metric, mean, n, std_err)
```

::: fragment
Test set results:

```{r test-res}
test_res %>% collect_metrics()
```
:::

## Final fitted workflow

Extract the final fitted workflow, fit using the training set:

```{r}
final_glm_spline_wflow <- 
  test_res %>% 
  extract_workflow()

# use this object to predict or deploy
predict(final_glm_spline_wflow, nhl_test[1:3,])
```

## Next steps

-   Use [explainers](https://www.tmwr.org/explain.html) to characterize the model and the predictions.

. . .

-   [Document the model](https://vetiver.rstudio.com/learn-more/model-card.html).

. . .

-   Create an [applicability domain model](https://applicable.tidymodels.org/) to help monitor our data over time.

. . .

-   [Deploy the model](https://vetiver.rstudio.com/get-started/).

```{r teardown}
#| include: false
foreach::registerDoSEQ()
parallel::stopCluster(cl)

# Used in whole game slides in introduction
spline_curves <- 
  glm_spline_res %>% 
  collect_predictions(parameters = select_best(glm_spline_res, metric = "roc_auc")) %>% 
  roc_curve(on_goal, .pred_yes) %>% 
  mutate(wflow_id = "splines") %>% 
  relocate(wflow_id)

xgb_curves <- 
  xgb_coord_res %>% 
  collect_predictions(parameters = select_best(xgb_coord_res, metric = "roc_auc")) %>% 
  roc_curve(on_goal, .pred_yes) %>% 
  mutate(wflow_id = "xgboost") %>% 
  relocate(wflow_id)

xgb_coord_curves <- 
  xgb_coord_res %>% 
  collect_predictions(parameters = select_best(xgb_coord_res, metric = "roc_auc")) %>% 
  roc_curve(on_goal, .pred_yes) %>% 
  mutate(wflow_id = "xgboost-coords") %>% 
  relocate(wflow_id)

roc_curves_part_6 <- bind_rows(spline_curves, xgb_curves, xgb_coord_curves) 

save(roc_curves_part_6, file = "roc_curves_part_6.RData", compress = TRUE)
```
